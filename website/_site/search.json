[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Logistic Regression Group",
    "section": "",
    "text": "As part of the Fall-2025 Capstone Projects in Data Science (IDC-6940) at University of West Florida (UWF), our team provided a systematic review of logistic regresion models and their applications with focus to healthcare data.\nThis project was completed under the guidance of Dr. Achraf Cohen (Achraf Cohen | Dr. Achraf Cohen)."
  },
  {
    "objectID": "people/basnet-shree/index.html#education",
    "href": "people/basnet-shree/index.html#education",
    "title": "Shree Krishna Basnet",
    "section": "Education",
    "text": "Education\nB.S. Mechanical Engineering | Univevrsity of West Florida"
  },
  {
    "objectID": "people/barbosa-renan/index.html#education",
    "href": "people/barbosa-renan/index.html#education",
    "title": "Renan Monteiro Barbosa",
    "section": "Education",
    "text": "Education\nB.S. Mechanical Engineering | Univevrsity of West Florida"
  },
  {
    "objectID": "slides.html#introduction",
    "href": "slides.html#introduction",
    "title": "Present a great story for data science projects",
    "section": "Introduction",
    "text": "Introduction\n\nDevelop a storyline that captures attention and maintains interest.\nYour audience is your peers\nClearly state the problem or question you’re addressing.\nIntroduce why it is relevant needs.\nProvide an overview of your approach.\n\nIn kernel estimator, weight function is known as kernel function[1]. Cite this paper[2]. The GEE[3]. The PCA[4]*"
  },
  {
    "objectID": "slides.html#methods",
    "href": "slides.html#methods",
    "title": "Present a great story for data science projects",
    "section": "Methods",
    "text": "Methods\n\nDetail the models or algorithms used.\nJustify your choices based on the problem and data."
  },
  {
    "objectID": "slides.html#data-exploration-and-visualization",
    "href": "slides.html#data-exploration-and-visualization",
    "title": "Present a great story for data science projects",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\n\nDescribe your data sources and collection process.\nPresent initial findings and insights through visualizations.\nHighlight unexpected patterns or anomalies."
  },
  {
    "objectID": "slides.html#data-exploration-and-visualization-1",
    "href": "slides.html#data-exploration-and-visualization-1",
    "title": "Present a great story for data science projects",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\nA study was conducted to determine how…"
  },
  {
    "objectID": "slides.html#modeling-and-results",
    "href": "slides.html#modeling-and-results",
    "title": "Present a great story for data science projects",
    "section": "Modeling and Results",
    "text": "Modeling and Results\n\nExplain your data preprocessing and cleaning steps.\nPresent your key findings in a clear and concise manner.\nUse visuals to support your claims.\nTell a story about what the data reveals."
  },
  {
    "objectID": "slides.html#conclusion",
    "href": "slides.html#conclusion",
    "title": "Present a great story for data science projects",
    "section": "Conclusion",
    "text": "Conclusion\n\nSummarize your key findings.\nDiscuss the implications of your results."
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "Present a great story for data science projects",
    "section": "References",
    "text": "References\n\n\n1. Efromovich, S. (2008). Nonparametric curve estimation: Methods, theory, and applications. Springer New York. https://books.google.com/books?id=mdoLBwAAQBAJ\n\n\n2. Bro, R., & Smilde, A. K. (2014). Principal component analysis. Analytical Methods, 6(9), 2812–2831.\n\n\n3. Wang, M. (2014). Generalized estimating equations in longitudinal data analysis: A review and recent developments. Advances in Statistics, 2014.\n\n\n4. Daffertshofer, A., Lamoth, C. J., Meijer, O. G., & Beek, P. J. (2004). PCA in studying coordination and variability: A tutorial. Clinical Biomechanics, 19(4), 415–428."
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Predicting stroke risk from common health indicators: a binary logistic regression analysis",
    "section": "",
    "text": "The report goes here need some work\nSlides: slides.html"
  },
  {
    "objectID": "report.html#introduction",
    "href": "report.html#introduction",
    "title": "Predicting stroke risk from common health indicators: a binary logistic regression analysis",
    "section": "Introduction",
    "text": "Introduction\nLogistic regression analysis is a type of regression technique that is used when a dataset’s response variable is categorical in nature. When the outcome variable takes on two distinct classes, a binary logistic regression model is used, and when there are more than two classes of the response variable, either a multinomial or ordinal logistic regression model can be utilized[1] (Kutner, 2013). Logistic regression analysis is commonly used across a variety of industries to glean insights about data to make optimal decisions about the data, to predict class labels, and to estimate probability of an event occurring; some fields include medicine, traffic and road engineering, environmental concerns, credit and fraud issues, and more. In this literature review, we take a closer look at logistic regression by discussing some applications of this algorithm in machine learning models, as well as several uses of logistic regression modeling in several peer- reviewed studies.\nMany machine learning algorithms use logistic regression to train a model and make predictions about class labels. One example of this is through text classification to improve natural language processing tasks. The article “From logistic regression to the perceptron algorithm: Exploring gradient descent with large step sizes” investigates similarities between a logistic regression algorithm with gradient descent and a perceptron algorithm. Researchers observed that with very large steps, the logistic regression with gradient descent behaves like a perceptron, which in some sense links it back to the Deep Equilibrium networks study. The conclusions from this paper are counter intuitive, and further research and reflections about classification and optimization theory are encouraged[2] (Tyurin, 2025). Another way logistic regression is used in machine learning is through large language models (LLMs), which are complex neural networks trained on very large datasets to output human language[3] (Pedapati et al., 2024). The paper “Large language model confidence estimation via black-box access” addresses the problem of estimating the confidence of LLM outputs when only black-box (query-only) access is available. It is a simple technique that uses logistic regression to classify and validate the confidence of the outputs. Some problems of using black-box models are that there is no control over the model itself, and in some cases, the benefits and the value of buying these services that provide a black-box model outweigh training a personal, custom model (Pedapti).\nLogistic regression analysis also assists road engineers and traffic control around the world by identifying common predictors of traffic accidents in general, and specifically, predictors of fatal accidents. According to The World Health Organization, one of the most common unnatural causes of death across the world is road accidents, so it is imperative to identify strong predictors associated with these events[4] (Akter et al., 2021). A different study from 2024 aimed to find which factors in traffic are strongly associated with the occurrence of traffic accidents[5] (Chang et al., 2024). Researchers used binary logistic regression to model the probability of a traffic accident occurring given a set of 25 predictors related to road safety. Another road traffic safety study from Bangladesh investigated strong predictors of motorcycle accidents. These researchers also utilized a binary logistic regression model to find strong predictors of severe accidents[4] (Akter, 2021). “Modeling Road Accident Severity with Logistic Regression (comparison study)” also exemplifies the use of a binary logistic regression model to analyze traffic risk[6] (Chen et al., 2020). Researchers compared the results of the logistic regression model to results of decision tree and random forest models, and it was found that the logistic regression model was more clear and understandable than the others. All three of these studies concluded by stating that there are several significant variables found when predicting severe road crashes. Knowing these significant predictors helps builders and developers eliminate or reduce these risk factors as they are building new roads; hence, it is crucial to continue researching road accident severity with logistic regression[4] (Akter, 2021).\nEnvironmental issues can also be studied using logistic regression analysis due to its interesting properties; after all, logistic regression is a generalized linear model, which conducts mapping from any real number to probability values. “Priority prediction of Asian Hornet sighting report using machine learning methods” seeks to address the problem of Asian giant hornets[7] (Liu et al., 2021). They are an invasive species that pose a significant threat to native bee populations and local beekeeping, as well as to public safety due to their aggressive nature and potent venom. The goal of the research is to create an automated system to predict the priority of Asian giant hornet sighting reports. The authors modeled the priority prediction of sighting reports as a two-classification problem, with classes being either a “true positive” or a “false positive.” Their methodology is a straightforward application of logistic regression with feature extraction. Researchers then used a weighted binary cross-entropy function and the logistic regression is used for mapping the probability given the feature vector. The model achieved an average prediction accuracy of 83.5% on positive reports with the best weighting parameter settings, but still far from other works which achieved about 93% using Deep Learning. It was concluded that this still needs a lot of improvement or maybe it will never outmatch other methods due to hidden limitations[7] (Liu et al., 2021). One other example of logistic regression used in environmental contexts is in the article “Autoregressive Logistic Regression Applied to Atmospheric Circulation Patterns”[8] (Guanche et al., 2014). Researchers incorporate autoregressive time dependencies into logistic regression for climate modeling. They work with complex climatological dynamic data, and they explain both interpretation and simulation capabilities for weather patterns[8] (Guanche et al., 2014).\nThe article “Understanding Logistic Regression Analysis” discusses the usefulness of logistic regression, describes how to interpret results of the model, and gives an example of a logistic regression analysis using a synthetic dataset[9] (Sperandi, 2014). The data is about patients undergoing a drug treatment with a categorical outcome that is binary in nature, taking on values of survived (1), or did not survive (0). The result of the analysis explains how to interpret output from the model; one must take the exponentials of the slopes in the model to find the chances (the probability) of an event occurring. It is noted that in order to correctly understand results of a logistic regression, one must carefully consider the differences between the odds ratio, the log odds, and the probabilities of events occurring. Another important point in the article outlines the process of feature selection; a common way that predictors are selected for a logistic regression model is through a preliminary univariate analysis. After conducting a univariate analysis of each predictor in relation to the outcome variable, all significant predictors are included in the final multivariate logistic regression analysis[9] (Sperandi).\nA 2024 study titled, “Determinants of coexistence of undernutrition and anemia among under- five children in Rwanda,” presents evidence from 2019/2020 demographic health survey data[10] . There are two outcome variables in the dataset: anemia and undernutrition in children under five years of age in Rwanda[10] (Agmas et al., 2024). The study analyzes the relationship between the two outcome variables, as well as the relationship between 26 predictors relating to the childrens’ preexisting health conditions, family information, details about the parents, and relevant geographic information. One result of the study was that the relationship between the two outcome variables, presence of malnutrition and presence of anemia, was found to be significant. Six other significant predictors were identified: mother’s age, drinking water quality, other children in household, child gender, birth order, and gender of head of household. The conclusion states that improving maternal education, supplementing with vitamin A and other nutrient dense foods, providing a healthy home environment, and decreasing maternal anemia may help improve rates of malnutrition and anemia in children[10] (Agmas, 2024).\nIn the study, “Predictors of hospital admission when presenting with acute on chronic breathlessness: Binary logistic regression” emergency room data from one hospital is analyzed to determine common predictors of patients that are admitted to the hospital[11] (Hutchinson et al., 2023). Specifically, patients presenting to the emergency room with acute on chronic breathlessness were surveyed to collect data that would help researchers understand common factors among those admitted to the hospital. Knowing common predictors ahead of time helps hospital staff more easily identify patients who are more at risk for being admitted to the hospital, and also helps identify which patients would be more likely to be able to be discharged without being admitted. A binary logistic regression analysis of the data revealed that the odds of admission to the hospital were positively correlated with three predictors: age, talking to a doctor about symptoms, and the presence of preexisting heart conditions. However, the odds of being admitted to the hospital were negatively associated with blood oxygen levels[11] (Hutchinson, 2023).\nA new medical condition was recently recognized in 2004: airplane headache (AH), a condition described as a headache induced while taking off or landing in an airplane[12] (Prottengeier et al., 2025). Because AH is a relatively new addition to medical dictionaries, it is an underexplored condition that requires additional research. This study sought to identify common risk factors significantly associated with airplane headache to aid both travelers and airline employees. Two binary logistic regression models were constructed to compare two groups against the airplane headache group. The first regression model compared the airplane headache group to the no headache group, and 10 significant predictors of AH were identified; this model’s predictive power was found to be very high. The second model compared the airplane headache group to a group called other headache (individuals with symptoms of other types of headaches). The result from this analysis showed four significant predictors; however, the predictive power of the model was found to be very low. To conclude, it can be stated that binary logistic regression is a very effective way to find strong predictors of airplane headache when compared to those who do not have any headaches while flying[12] (Prottengeier).\nOne other way logistic regression is applied in the medical field is in identifying how the general public makes decisions regarding their health[13] (Liu J. et al., 2024). Researchers in China analyzed 2696 health survey responses collected from individuals across 31 Chinese provinces. They analyzed the data with a binary logistic regression model to classify points into two categories: unilateral decision making (value of 1), or collaborative decision making (value of 0). The researchers wanted to identify top predictors of individuals that make medical decisions by themselves and which predictors are correlated with patients making health decisions with more than one party (i.e. a patient, doctor, and family member all helping to make the health decision). It was found that most responses were classified as collaborative decision making (70%), which supports the idea that individuals in China strongly emphasize family- made decisions and strong family values. It was also concluded that significant predictors of unilateral decision making were gender, education level, family status, religious beliefs and occupation[13] (Liu J., 2024).\nLogistic Regression is also useful in detecting common diseases, such as breast cancer. “Regularized logistic regression with network-based pairwise interaction for biomarker identification in breast cancer” uses regularized logistic regression along with biological network information and pairwise interactions, to find biomarkers, both single and interacting pairs, for breast cancer[14] (Wu et al., 2016). Researchers prioritized biologically plausible biomarker combinations and used an adaptive elastic net, a penalty that balances l1 and l2, with network constraints. The result of the study shows that their model outperforms simpler models in terms of predictive performance, and they were able to discover both individual biomarkers and interacting gene pairs[14] (Wu, 2016). Another study on breast cancer from 2016 aimed to identify gene signatures that predict chemosensitivity, that is, which tumors react to chemotherapy in breast cancer by combining genetic algorithms with sparse logistic regression[15] (Hu, 2016). What makes this analysis relevant and important is that it predicts which patients will react to chemotherapy, which gives more personalized treatment. The results show that SLR-28 and Notch-86, two gene signatures, perform well on training and validation sets in terms of accuracy, specificity, sensitivity, and other metrics[15] (Hu, 2016).\nIn this paper, we will discuss the methodology, analysis, results, visualizations, and conclusions of a binary logistic regression statistical analysis regarding risk of stroke from common health indicators. Stroke is the second most common cause of death globally, so understanding the risk factors associated with it is imperative[16] (“The Top 10”, 2024).\n\nReferences\n\n\n1. Kutner, M. H. (2005). Applied linear statistical models.\n\n\n2. Tyurin, A. (2025). From logistic regression to the perceptron algorithm: Exploring gradient descent with large step sizes. Proceedings of the AAAI Conference on Artificial Intelligence, 39, 20938–20946.\n\n\n3. Pedapati, T., Dhurandhar, A., Ghosh, S., Dan, S., & Sattigeri, P. (2024). Large language model confidence estimation via black-box access. arXiv Preprint arXiv:2406.04370.\n\n\n4. Rahman, M. H., Zafri, N. M., Akter, T., & Pervaz, S. (2021). Identification of factors influencing severity of motorcycle crashes in dhaka, bangladesh using binary logistic regression model. International Journal of Injury Control and Safety Promotion, 28(2), 141–152.\n\n\n5. Chen, Y., You, P., & Chang, Z. (2024). Binary logistic regression analysis of factors affecting urban road traffic safety. Advances in Transportation Studies, 3.\n\n\n6. Chen, M.-M., & Chen, M.-C. (2020). Modeling road accident severity with comparisons of logistic regression, decision tree and random forest. Information, 11(5), 270.\n\n\n7. Liu, Y., Guo, J., Dong, J., Jiang, L., & Ouyang, H. (2021). Priority prediction of asian hornet sighting report using machine learning methods. 2021 IEEE International Conference on Software Engineering and Artificial Intelligence (SEAI), 7–11.\n\n\n8. Guanche, Y., Mı́nguez, R., & Méndez, F. J. (2014). Autoregressive logistic regression applied to atmospheric circulation patterns. Climate Dynamics, 42(1), 537–552.\n\n\n9. Sperandei, S. (2014). Understanding logistic regression analysis. Biochemia Medica, 24(1), 12–18.\n\n\n10. Asmare, A. A., & Agmas, Y. A. (2024). Determinants of coexistence of undernutrition and anemia among under-five children in rwanda; evidence from 2019/20 demographic health survey: Application of bivariate binary logistic regression model. Plos One, 19(4), e0290111.\n\n\n11. Hutchinson, A., Pickering, A., Williams, P., & Johnson, M. (2023). Predictors of hospital admission when presenting with acute-on-chronic breathlessness: Binary logistic regression. PLoS One, 18(8), e0289263.\n\n\n12. Prottengeier, J., Kaiser, I., Moritz, A., & Konrad, F. (2025). Risk factors for airplane headache: A multivariable logistic regression analysis in a population of career flight personnel. Cephalalgia, 45(4), 03331024251329837.\n\n\n13. Lyu, Y., Xu, Q., & Liu, J. (2024). Exploring the medical decision-making patterns and influencing factors among the general chinese public: A binary logistic regression analysis. BMC Public Health, 24(1), 887.\n\n\n14. Wu, M.-Y., Zhang, X.-F., Dai, D.-Q., Ou-Yang, L., Zhu, Y., & Yan, H. (2016). Regularized logistic regression with network-based pairwise interaction for biomarker identification in breast cancer. BMC Bioinformatics, 17(1), 108.\n\n\n15. Hu, W. (2016). Using genetic algorithms and sparse logistic regression to find gene signatures for chemosensitivity prediction in breast cancer. American Journal of Bioscience and Bioengineering, 4(2), 26–33.\n\n\n16. World Health Organization. (2025). The top 10 causes of death. https://www.who.int/news-room/fact-sheets/detail/the-top-10-causes-of-death."
  },
  {
    "objectID": "posts/shree-blog-post-week2/index.html",
    "href": "posts/shree-blog-post-week2/index.html",
    "title": "Literature Review Week 2",
    "section": "",
    "text": "link: https://robertominguez.altervista.org/DocumentacionAcreditativa/Articulos/GuancheMM13.pdf\nAutoregressive Logistic Regression Applied to Atmospheric Circulation Patterns” (Guanche, Mínguez & Méndez, 2013).[1]\nArticel incorporates autoregressive time dependencies into logistic regression for climate modeling. work with complex climatological dynamics instead of common data set like health or business. Explains both interpretation and simulation capabilities for weather patterns.\nData used : They took data of measured sea-level pressure (SLP) fields to determine daily atmospheric circulation patterns over the Northeastern Atlantic. A limited number of circulation types (weather regimes) are created by summarizing the SLP fields, for example, through clustering or categorization. Data setup for the autoregressive logistic regression applied to weather types” shows how they organized lagged types, covariates,\nSteps on summary: - Sort daily SLP data into distinct circulation categories. - create autoregressive terms, covariates, and lagged indicators (trend, seasonal) - Comparing anticipated and empirical probability allows for diagnostic checks. - Utilize the fitted model to replicate artificial circulatory state sequences. - Check for simulation statistics (frequencies, transitions, persistence) against historical data."
  },
  {
    "objectID": "posts/shree-blog-post-week2/index.html#article-1",
    "href": "posts/shree-blog-post-week2/index.html#article-1",
    "title": "Literature Review Week 2",
    "section": "",
    "text": "link: https://robertominguez.altervista.org/DocumentacionAcreditativa/Articulos/GuancheMM13.pdf\nAutoregressive Logistic Regression Applied to Atmospheric Circulation Patterns” (Guanche, Mínguez & Méndez, 2013).[1]\nArticel incorporates autoregressive time dependencies into logistic regression for climate modeling. work with complex climatological dynamics instead of common data set like health or business. Explains both interpretation and simulation capabilities for weather patterns.\nData used : They took data of measured sea-level pressure (SLP) fields to determine daily atmospheric circulation patterns over the Northeastern Atlantic. A limited number of circulation types (weather regimes) are created by summarizing the SLP fields, for example, through clustering or categorization. Data setup for the autoregressive logistic regression applied to weather types” shows how they organized lagged types, covariates,\nSteps on summary: - Sort daily SLP data into distinct circulation categories. - create autoregressive terms, covariates, and lagged indicators (trend, seasonal) - Comparing anticipated and empirical probability allows for diagnostic checks. - Utilize the fitted model to replicate artificial circulatory state sequences. - Check for simulation statistics (frequencies, transitions, persistence) against historical data."
  },
  {
    "objectID": "posts/shree-blog-post-week2/index.html#article-2",
    "href": "posts/shree-blog-post-week2/index.html#article-2",
    "title": "Literature Review Week 2",
    "section": "Article 2",
    "text": "Article 2\nA Descriptive Study of Variable Discretization and Cost-Sensitive Logistic Regression on Imbalanced Credit Data.[2]\nLink: https://arxiv.org/pdf/1812.10857\nPurpose : The author looks out for a problem related to credit scoring where the minority class (defaults and delinquencies) is comparatively uncommon. Their main concept is to contrast cost-sensitive logistic regression (assigning various misclassification fees) with variable discretization (converting continuous predictors into categorical bins) in order to reduce class bias. When used real credit scoring dataset event rate was 6.68% that was highly imbalanced.\nModels used: - trained logestic regression in different versions\n- Standard logistic regression on continuous predictors (baseline). - Logistic regression on discretized predictors (using different binning strategies). - Cost-sensitive logistic regression on the continuous predictors (i.e. weight adjustments for minority class). - Possibly combined approaches (discretized + cost-sensitive). - They use 10-fold cross-validation to ensure robustness. PMC - Performance metrics include: - ROC / AUC - Type I error (false positive rate) - Type II error (false negative rate) - Accuracy - F1 score - They also examine coefficient estimates and interpretability\nSummary:\nIn their study of unbalanced credit scoring (default rate ~6.68%), Zhang et al. used 10-fold CV to examine standard, discretized, and cost-sensitive logistic regression. They discovered that variable discretization works better than cost-sensitive weighting, producing models that are more resilient, stable, and interpretable. These models also generalize well to other domains, such as biology and wine quality.\n\nReferences\n\n\n1. Guanche, Y., Mı́nguez, R., & Méndez, F. J. (2014). Autoregressive logistic regression applied to atmospheric circulation patterns. Climate Dynamics, 42(1), 537–552.\n\n\n2. Zhang, L., Ray, H., Priestley, J., & Tan, S. (2020). A descriptive study of variable discretization and cost-sensitive logistic regression on imbalanced credit data. Journal of Applied Statistics, 47(3), 568–581."
  },
  {
    "objectID": "posts/kristina-blog-post-week5/index.html",
    "href": "posts/kristina-blog-post-week5/index.html",
    "title": "Literature Review Week 5",
    "section": "",
    "text": "Article Title: Identification of factors influencing severity of motorcycle crashes in Dhaka, Bangladesh using binary logistic regression model.[1]\nAuthors: Hamidur Rahman, Niaz Mahmud Zafri, Tamanna Akter, Shahrior Pervaz\nProblem: - One of the most common unnatural causes of death across the world is road accidents, so it is important to identify strong predictors associated with such accidents. - According to the World Health Organization, most of the road crash deaths that occur worldwide happen in developing countries. - Researchers focus on motorcycle crashes in Dhaka, the capital city of Bangladesh. It is stated that the rate of road crashes in Bangladesh is significantly greater than other developing countries, and Dhaka has the greatest amount of motorists and reported motorcycle crashes. - It is noted that most research about this topic is done on data from developed countries and not developing countries. So the conclusions drawn from existing studies may not be applicable to the problems the developing countries are facing - Knowing which predictors are strongly associated with motorcycle crashes in Dhaka helps builders and developers eliminate or reduce these risk factors as they are building new roads. This study serves as a step in preventing more motorcycle road crashes as developing countries are being built.\nSolution: - Researchers conduct a binary logistic regression analysis to identify predictors most strongly associated with the occurrence of the outcome, motorcycle crash severity (fatal/ non- fatal) - They began the study by choosing predictors identified in previous literature about similar topics. Commonly identified predictors of motorcycle crashes are grouped into five broad categories: environment, road characteristics, driver characteristics, motorcycle features, and type of collision. - The binary logistic regression equation is given as the log odds of the probability of the occurrence of the outcome. An explanation of every variable in the equation is given (slopes, intercept, odds ratio, and relation to the outcome variable).\nData: - Data was collected from 2006 to 2015 from the Accident Research Institute of Bangladesh University of Engineering and Technology. Only 316 data points were used, and each contained information about motorcycle crashes. - There are five broad categories encompassing all predictors. The five categories and predictors are: 1. Environmental factors- date/time, lighting, weather 2. Collision type- five different types of collisions 3. Driver characteristics- gender, age, alcohol consumption, and use of a helmet 4. Road characteristics-location, traffic characteristics, road conditions 5. vehicle characteristics- type of other vehicle in crash, weight of other vehicle in crash, motorcycle condition, and more - The outcome variable, crash injury severity, originally had four levels. Observations from this predictor were then reclassified as either “fatal” or “non-fatal” for a binary outcome. - To determine which predictors to include in the dataset, researchers conducted a univariate analysis and a chi- square test of each individual predictor to assess significance. All significant predicators were then chosen for the dataset, used for the binary logistic regression. After this, multicollinearity was assessed using the VIF and none of the predictors showed multicollinearity.\nResults/ Conclusions: - After conducting the binary logistic regression, 11 out of the 16 included predictors were found to be significantly associated with the outcome. The significant predictors were day of the week, seasonal condition, time of day, three types of road characteristics, crash type, condition of motorcycle, type of other vehicle in accident, use of helmet, and alcohol consumption - The regression curve from the analysis was also found to be significant - Goodness of fit was assessed using a test called the Hosmer and Lemeshow test - The discussion section details each significant predictor, and interpretations of slopes are given in relation to the outcome variable and the reference groups. - Some of the findings were consistent with other studies, and some of the findings contradict conclusions in previous studies. - Most notable conclusions from this study that researchers believe would improve road safety and reduce motorcycle accident severity in developing countries: 1. better lighting conditions for enhanced visibility 2. solution to wet/ slippery roads during rainy season 3. educate drivers about how to drive during unsafe conditions, such as nighttime, heavy rain, and heavy traffic on weekends. Also educate drivers to follow proper safety and speeding regulations. And better education/ training/ evaluation for drivers operating larger vehicles 4. improved pedestrian walkways and road areas 5. strict enforcement of laws regarding helmet use and alcohol while driving Limitations: - Missing information in the data: Some important predictors were entirely excluded from the study due to missing information in the dataset. So there may be some extremely relevant predictors that have yet to be studied. There is also an ongoing issue of accidents that go unreported due to lack of fatality, and drivers do not report these incident."
  },
  {
    "objectID": "posts/kristina-blog-post-week5/index.html#article-1",
    "href": "posts/kristina-blog-post-week5/index.html#article-1",
    "title": "Literature Review Week 5",
    "section": "",
    "text": "Article Title: Identification of factors influencing severity of motorcycle crashes in Dhaka, Bangladesh using binary logistic regression model.[1]\nAuthors: Hamidur Rahman, Niaz Mahmud Zafri, Tamanna Akter, Shahrior Pervaz\nProblem: - One of the most common unnatural causes of death across the world is road accidents, so it is important to identify strong predictors associated with such accidents. - According to the World Health Organization, most of the road crash deaths that occur worldwide happen in developing countries. - Researchers focus on motorcycle crashes in Dhaka, the capital city of Bangladesh. It is stated that the rate of road crashes in Bangladesh is significantly greater than other developing countries, and Dhaka has the greatest amount of motorists and reported motorcycle crashes. - It is noted that most research about this topic is done on data from developed countries and not developing countries. So the conclusions drawn from existing studies may not be applicable to the problems the developing countries are facing - Knowing which predictors are strongly associated with motorcycle crashes in Dhaka helps builders and developers eliminate or reduce these risk factors as they are building new roads. This study serves as a step in preventing more motorcycle road crashes as developing countries are being built.\nSolution: - Researchers conduct a binary logistic regression analysis to identify predictors most strongly associated with the occurrence of the outcome, motorcycle crash severity (fatal/ non- fatal) - They began the study by choosing predictors identified in previous literature about similar topics. Commonly identified predictors of motorcycle crashes are grouped into five broad categories: environment, road characteristics, driver characteristics, motorcycle features, and type of collision. - The binary logistic regression equation is given as the log odds of the probability of the occurrence of the outcome. An explanation of every variable in the equation is given (slopes, intercept, odds ratio, and relation to the outcome variable).\nData: - Data was collected from 2006 to 2015 from the Accident Research Institute of Bangladesh University of Engineering and Technology. Only 316 data points were used, and each contained information about motorcycle crashes. - There are five broad categories encompassing all predictors. The five categories and predictors are: 1. Environmental factors- date/time, lighting, weather 2. Collision type- five different types of collisions 3. Driver characteristics- gender, age, alcohol consumption, and use of a helmet 4. Road characteristics-location, traffic characteristics, road conditions 5. vehicle characteristics- type of other vehicle in crash, weight of other vehicle in crash, motorcycle condition, and more - The outcome variable, crash injury severity, originally had four levels. Observations from this predictor were then reclassified as either “fatal” or “non-fatal” for a binary outcome. - To determine which predictors to include in the dataset, researchers conducted a univariate analysis and a chi- square test of each individual predictor to assess significance. All significant predicators were then chosen for the dataset, used for the binary logistic regression. After this, multicollinearity was assessed using the VIF and none of the predictors showed multicollinearity.\nResults/ Conclusions: - After conducting the binary logistic regression, 11 out of the 16 included predictors were found to be significantly associated with the outcome. The significant predictors were day of the week, seasonal condition, time of day, three types of road characteristics, crash type, condition of motorcycle, type of other vehicle in accident, use of helmet, and alcohol consumption - The regression curve from the analysis was also found to be significant - Goodness of fit was assessed using a test called the Hosmer and Lemeshow test - The discussion section details each significant predictor, and interpretations of slopes are given in relation to the outcome variable and the reference groups. - Some of the findings were consistent with other studies, and some of the findings contradict conclusions in previous studies. - Most notable conclusions from this study that researchers believe would improve road safety and reduce motorcycle accident severity in developing countries: 1. better lighting conditions for enhanced visibility 2. solution to wet/ slippery roads during rainy season 3. educate drivers about how to drive during unsafe conditions, such as nighttime, heavy rain, and heavy traffic on weekends. Also educate drivers to follow proper safety and speeding regulations. And better education/ training/ evaluation for drivers operating larger vehicles 4. improved pedestrian walkways and road areas 5. strict enforcement of laws regarding helmet use and alcohol while driving Limitations: - Missing information in the data: Some important predictors were entirely excluded from the study due to missing information in the dataset. So there may be some extremely relevant predictors that have yet to be studied. There is also an ongoing issue of accidents that go unreported due to lack of fatality, and drivers do not report these incident."
  },
  {
    "objectID": "posts/kristina-blog-post-week5/index.html#article-2",
    "href": "posts/kristina-blog-post-week5/index.html#article-2",
    "title": "Literature Review Week 5",
    "section": "Article 2",
    "text": "Article 2\nTitle of article: Risk factors for airplane headache: A multivariate logistic regression analysis in a population of career flight personnel.[2]\nAuthors: Johannes Prottengeier, Isabelle Kaiser, Andreas Moritz, Fabian Konrad\nProblem: - Airplane headache (AH) is a headache disorder described as a headache induced while taking off or landing in an airplane. It was not until 2004 that the disorder was recognized and classified as a medical issue. Because this disorder has just recently been recognized, there is little existing research on it. - There is a lot of previous literature and research regarding other headache types and disorders, but AH is still lacking appropriate research. - Specifically, this study aims to identify predictors and risk factors that occur before onset of airplane headache. Knowing the predictors would help both travelers and employees of airlines. - AH is a common disorder affecting about 65 million people worldwide every year, so helping prevent and treat it is crucial. Future research is therefore a must.\nSolution: - Conduct a logistic regression analysis to determine significant predictors of airplane headache. Two binary logistic regression models were constructed; one model’s outcome was either airplane headache (1) or no headache (0), and the other model’s outcomes were airplane headache (1) or other headache (0). - Used R to conduct statistical analysis\nData: - Data was collected from a voluntary online survey sent out to about 20,000 pilots who fly frequently due to work. A total of 2237 complete questionnaires from a 3 month period in 2014 were received and used in the dataset for this study. - The data/ questions in the survey were determined by pain specialists - Predictors included in the survey were: demographic information, health history, substance use, medication use, stress levels, and headache/ physical symptoms while flying. - The outcome variable was divided into three categories: airplane headache, no headache, and other type of headache. - Predictors were tested for multicollinearity before models were constructed - Data was further reduced using stepwise backward elimination (starting with all predictors and then eliminating least significant predictors)\nResults/ Conclusions: - Survey results revealed that a vast majority of participants said they had some form of headache while flying on an airplane (82%) - The first model comparing airplane headache with no headache was found to have 10 significant predictors. The AUC for this model showed that it had strong predictive power. - The second model comparing airplane headache with other headache was found to have only four significant predictors. The AUC also showed that this model had low predictive power. This is likely attributed to the fact that airplane headache and other types of headaches all have similar predictors, so it is not easy to distinguish between strong predictors for just AH as compared to all types of headaches. - Conclusions: supplementing with folic acid before flight may help reduce risk of airplane headache. Other strong predictors of airplane headache are occupation, work stress, and preexisting headache medical conditions. - Further research about airplane headache is necessary because of all the ongoing negative subsequent events caused by it. It causes people loss of productivity, avoidance behaviors, stress, and anxiety.\nLimitations: - Only 12% of surveyed individuals submitted their survey. It could be the case that only the people who suffered from headaches responded to the survey, so the proportions of individuals with airplane headache in this study are not representative of the entire population. This is called positive selection bias. - Participants in the survey may not be reporting accurate information because a lot of time passes between the event of their airplane travel and the time when they take the survey.\n\nReferences\n\n\n1. Rahman, M. H., Zafri, N. M., Akter, T., & Pervaz, S. (2021). Identification of factors influencing severity of motorcycle crashes in dhaka, bangladesh using binary logistic regression model. International Journal of Injury Control and Safety Promotion, 28(2), 141–152.\n\n\n2. Prottengeier, J., Kaiser, I., Moritz, A., & Konrad, F. (2025). Risk factors for airplane headache: A multivariable logistic regression analysis in a population of career flight personnel. Cephalalgia, 45(4), 03331024251329837."
  },
  {
    "objectID": "posts/renan-blog-post-week6/index.html",
    "href": "posts/renan-blog-post-week6/index.html",
    "title": "Dataset Exploration - Week 6",
    "section": "",
    "text": "This post start at Week 6 and extended over several week. From the discoveries made from Week 5 using the dataset Stroke Prediction Dataset we will be further exploring it by using insights found in[1]. So to develop a better insight we will be reproducing the research work in this post."
  },
  {
    "objectID": "posts/renan-blog-post-week6/index.html#introduction",
    "href": "posts/renan-blog-post-week6/index.html#introduction",
    "title": "Dataset Exploration - Week 6",
    "section": "Introduction",
    "text": "Introduction\nThe issue of data imbalance is a big problem for stroke ­prediction[2]. Because of many reasons ranging from privacy to the difficulty of doing cohort studies, the fact that pre-stroke datasets are rare, dataset often contain imbalanced classifications, with most instances being non-stroke c­ases[3]. So its unnecessary to say that this imbalance can result in biased models that favour the majority and ignore the minority, resulting in low forecast accuracy. To solve this issue and increase the effectiveness of the predictive models, we plan on exploring several oversampling and undersampling methods and much more are explored and employed, the popular of which is the ­SMOTE[4],[5]."
  },
  {
    "objectID": "posts/renan-blog-post-week6/index.html#setup-and-data-loading",
    "href": "posts/renan-blog-post-week6/index.html#setup-and-data-loading",
    "title": "Dataset Exploration - Week 6",
    "section": "1. Setup and Data Loading",
    "text": "1. Setup and Data Loading\nFirst, we need to load the required R packages and the dataset. The dataset is publicly available on Kaggle and was originally created by McKinsey & Company[6].\n\n1.1 Load Libraries\n\n\nCode\n# Run this once to install all the necessary packages\n# install.packages(c(\"corrplot\", \"ggpubr\", \"caret\", \"mice\", \"ROSE\", \"ranger\", \"stacks\", \"tidymodels\"))\n# install.packages(\"themis\")\n# install.packages(\"xgboost\")\n# install.packages(\"gghighlight\")\n\n\nWe can use this to check installed packages:\n```{r}\nrenv::activate(\"website\")\n\"yardstick\" %in% rownames(installed.packages())\n```\n\n\nCode\n# For data manipulation and visualization\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(knitr)\nlibrary(ggpubr)\n\n# For data preprocessing and modeling\nlibrary(caret)\nlibrary(mice)\nlibrary(ROSE) # For SMOTE\nlibrary(ranger) # A fast implementation of random forests\n\n# For stacking/ensemble models\nlibrary(stacks)\nlibrary(tidymodels)\n\nlibrary(themis)\nlibrary(gghighlight)\n\n# Set seed for reproducibility\nset.seed(123)\n\n\nMight need to deal with the conflicts later:\n```{bash}\n── Attaching packages ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidymodels 1.4.1 ──\n✔ broom        1.0.9     ✔ rsample      1.3.1\n✔ dials        1.4.2     ✔ tailor       0.1.0\n✔ infer        1.0.9     ✔ tune         2.0.0\n✔ modeldata    1.5.1     ✔ workflows    1.3.0\n✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n✔ recipes      1.3.1     ✔ yardstick    1.3.2\n── Conflicts ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidymodels_conflicts() ──\n✖ rsample::calibration()   masks caret::calibration()\n✖ scales::discard()        masks purrr::discard()\n✖ mice::filter()           masks dplyr::filter(), stats::filter()\n✖ recipes::fixed()         masks stringr::fixed()\n✖ dplyr::lag()             masks stats::lag()\n✖ caret::lift()            masks purrr::lift()\n✖ yardstick::precision()   masks caret::precision()\n✖ yardstick::recall()      masks caret::recall()\n✖ yardstick::sensitivity() masks caret::sensitivity()\n✖ yardstick::spec()        masks readr::spec()\n✖ yardstick::specificity() masks caret::specificity()\n✖ recipes::step()          masks stats::step()\n```\n\n\n1.2 Load Data\nWe will load the dataset and handle the data given the exploration done in Week5. The id column is unnecessary for prediction as well there are only 2 genders significant for prediction.\n\n\nCode\nfind_git_root &lt;- function(start = getwd()) {\n  path &lt;- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path &lt;- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root &lt;- find_git_root()\ndatasets_path &lt;- file.path(repo_root, \"datasets\")\nkaggle_dataset_path &lt;- file.path(datasets_path, \"kaggle-healthcare-dataset-stroke-data/healthcare-dataset-stroke-data.csv\")\nkaggle_data1 = read_csv(kaggle_dataset_path, show_col_types = FALSE)\n\n# unique(kaggle_data1$bmi)\nkaggle_data1 &lt;- kaggle_data1 %&gt;%\n  mutate(bmi = na_if(bmi, \"N/A\")) %&gt;%   # Convert \"N/A\" string to NA\n  mutate(bmi = as.numeric(bmi))         # Convert from character to numeric\n\n# Remove the 'Other' gender row and the 'id' column\nkaggle_data1 &lt;- kaggle_data1 %&gt;%\n  filter(gender != \"Other\") %&gt;%\n  select(-id) %&gt;%\n  mutate_if(is.character, as.factor) # Convert character columns to factors for easier modeling"
  },
  {
    "objectID": "posts/renan-blog-post-week6/index.html#data-imputation-and-balancing",
    "href": "posts/renan-blog-post-week6/index.html#data-imputation-and-balancing",
    "title": "Dataset Exploration - Week 6",
    "section": "2. Data Imputation and Balancing",
    "text": "2. Data Imputation and Balancing\nTo handle the missing BMI values, the research[1] explores three different imputation techniques. It also addresses the significant class imbalance between stroke and non-stroke cases using SMOTE.\n\n2.1 Imputation Techniques\nWe will create three datasets based on the imputation methods described:\n\nMean Imputation: Replacing missing values with the column’s mean.\nMICE (Multivariate Imputation by Chained Equations): An advanced method that estimates missing values based on other variables.\nAge Group-based Imputation: Replacing missing BMI values with the mean BMI of the corresponding age group.\n\n\n\nCode\n# 1. Mean Imputation\ndf_mean &lt;- kaggle_data1\ndf_mean$bmi[is.na(df_mean$bmi)] &lt;- mean(df_mean$bmi, na.rm = TRUE)\n\n# 2. MICE Imputation\nmice_imputation &lt;- mice(kaggle_data1, method='pmm', m=1, maxit=5, seed=500)\n\n\n\n iter imp variable\n  1   1  bmi\n  2   1  bmi\n  3   1  bmi\n  4   1  bmi\n  5   1  bmi\n\n\nCode\ndf_mice &lt;- complete(mice_imputation, 1)\n\n# 3. Age Group-based Imputation\ndf_age_group &lt;- kaggle_data1 %&gt;%\n  mutate(age_group = cut(age, breaks = c(0, 20, 40, 60, 81), right = FALSE)) %&gt;%\n  group_by(age_group) %&gt;%\n  mutate(bmi = ifelse(is.na(bmi), mean(bmi, na.rm = TRUE), bmi)) %&gt;%\n  ungroup() %&gt;%\n  select(-age_group)\n\n\n\n\n2.2 Addressing Class Imbalance with SMOTE\nThe dataset is highly imbalanced, with only 4.87% of cases being stroke instances. This can bias machine learning models. We will use SMOTE to create balanced versions of our imputed datasets by generating synthetic minority (stroke) class samples.\n\n\nCode\n# Ensure the stroke column is a factor for SMOTE\ndf_mice$stroke &lt;- as.factor(df_mice$stroke)\ndf_mean$stroke &lt;- as.factor(df_mean$stroke)\ndf_age_group$stroke &lt;- as.factor(df_age_group$stroke)\n\n# Create balanced datasets using SMOTE\n# Using the MICE imputed dataset as the primary example for balancing\n\n# Get the number of non-stroke (majority) cases\nn_majority &lt;- sum(df_mice$stroke == \"0\")\n\n# Calculate the desired total size for a balanced dataset\ndesired_N &lt;- 2 * n_majority\n\n# Create the balanced dataset\ndata_balanced_mice &lt;- ROSE::ovun.sample(\n  stroke ~ ., \n  data = df_mice, \n  method = \"over\", \n  N = desired_N, \n  seed = 123\n)$data\n\n# Check the new class distribution\ncat(\"Original Class Distribution (MICE imputed):\\n\")\n\n\nOriginal Class Distribution (MICE imputed):\n\n\nCode\nprint(table(df_mice$stroke))\n\n\n\n   0    1 \n4860  249 \n\n\nCode\ncat(\"\\nBalanced Class Distribution (SMOTE):\\n\")\n\n\n\nBalanced Class Distribution (SMOTE):\n\n\nCode\nprint(table(data_balanced_mice$stroke))\n\n\n\n   0    1 \n4860 4860"
  },
  {
    "objectID": "posts/renan-blog-post-week6/index.html#exploratory-data-analysis-eda-and-feature-importance",
    "href": "posts/renan-blog-post-week6/index.html#exploratory-data-analysis-eda-and-feature-importance",
    "title": "Dataset Exploration - Week 6",
    "section": "3. Exploratory Data Analysis (EDA) and Feature Importance",
    "text": "3. Exploratory Data Analysis (EDA) and Feature Importance\nThe paper identifies several key risk factors for stroke. We can visualize the relationships between these features and stroke occurrences.\n\n3.1 Visualizing Key Features\nLet’s reproduce some of the visualizations from Figure 1 in the paper, which shows the distribution of features concerning stroke occurrence.\nThese plots should confirm the paper’s findings: stroke incidence increases with age, high glucose levels, higher BMI, and the presence of hypertension.\n\nA detailed examination of stroke occurrences concerning different features is presented in Fig. 1, with sub-figures. - (Fig. 1a) In sub-figure (Fig. 1a), it is visible that there is a slight increase in the number of strokes among females when compared to males. - (Fig. 1b) Moving on to sub-figure (Fig. 1b), a rising trend in stroke cases is observed as individuals age, with the highest incidence observed around the age of 80. - (Fig. 1c) Sub-figure (Fig. 1c) reveals that individuals with heart disease are more vulnerable to experiencing strokes. - (Fig. 1d) Marital status is explored in sub-figure (Fig. 1d), which suggests that married individuals may have a slightly higher incidence of strokes than unmarried individuals. - (Fig. 1e) The comparison between stroke occurrences in urban and rural areas is depicted in sub-figure (Fig. 1e), indicating no significant difference between these groups regarding stroke risk. - (Fig. 1f) In sub-figure (Fig. 1f), the relationship between average glucose levels and stroke risk is illustrated. It shows that individuals with average glucose levels falling within 60–120 and 190–230 are at an increased risk of experiencing strokes. - (Fig. 1g) Hypertension is emphasized in sub-figure (Fig. 1g). It demonstrates a higher incidence of strokes among individuals diagnosed with hypertension. - (Fig. 1h) The relationship between BMI and stroke occurrence is examined in sub-figure (Fig. 1h). It reveals that individuals with a BMI ranging from 20 to 40 are more prone to strokes. - (Fig. 1i) Smoking habits are examined in sub-figure (Fig. 1i), where it is observed that former or never smokers are more likely to suffer from strokes than current smokers. This finding highlights the importance of considering smoking history when assessing an individual’s stroke risk. - (Fig. 1j) Lastly, shifting the focus to occupation, sub-figure (Fig. 1j) indicates that individuals working in private or self-employed sectors may have a greater likelihood of experiencing strokes compared to those in other occupations.\n\n\nCode\n# --- Prepare data for plotting ---\n# Convert binary and character variables to factors with clear labels\ndf_plot &lt;- df_mice |&gt;\n  mutate(\n    stroke = factor(stroke, labels = c(\"No Stroke\", \"Stroke\")),\n    hypertension = factor(hypertension, labels = c(\"No\", \"Yes\")),\n    heart_disease = factor(heart_disease, labels = c(\"No\", \"Yes\")),\n    ever_married = factor(ever_married, labels = c(\"No\", \"Yes\"))\n  )\n\n\n\n\nCode\n# (a) [cite_start]Gender vs. Stroke [cite: 132]\np1a &lt;- ggplot(df_plot, aes(x = gender, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"(a) Gender\", x = NULL, y = \"Count\")\n\n# (b) [cite_start]Age vs. Stroke [cite: 133]\np1b &lt;- ggplot(df_plot, aes(x = age, fill = stroke)) +\n  geom_histogram(binwidth = 5, position = \"identity\", alpha = 0.7) +\n  labs(title = \"(b) Age\", x = \"Age\", y = \"Count\")\n\n# (c) [cite_start]Heart Disease vs. Stroke [cite: 133]\np1c &lt;- ggplot(df_plot, aes(x = heart_disease, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"(c) Heart Disease\", x = NULL, y = \"Count\")\n\n# (d) [cite_start]Marital Status vs. Stroke [cite: 134]\np1d &lt;- ggplot(df_plot, aes(x = ever_married, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"(d) Ever Married\", x = NULL, y = \"Count\")\n\n# (e) [cite_start]Residence Type vs. Stroke [cite: 135]\np1e &lt;- ggplot(df_plot, aes(x = Residence_type, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"(e) Residence Type\", x = NULL, y = \"Count\")\n  \n# (f) [cite_start]Average Glucose Level vs. Stroke [cite: 136, 137]\np1f &lt;- ggplot(df_plot, aes(x = avg_glucose_level, fill = stroke)) +\n  geom_histogram(binwidth = 10, position = \"identity\", alpha = 0.7) +\n  labs(title = \"(f) Avg. Glucose Level\", x = \"Glucose Level\", y = \"Count\")\n\n# (g) [cite_start]Hypertension vs. Stroke [cite: 138]\np1g &lt;- ggplot(df_plot, aes(x = hypertension, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"(g) Hypertension\", x = NULL, y = \"Count\")\n\n# (h) [cite_start]BMI vs. Stroke [cite: 139, 140]\np1h &lt;- ggplot(df_plot, aes(x = bmi, fill = stroke)) +\n  geom_histogram(binwidth = 2, position = \"identity\", alpha = 0.7) +\n  labs(title = \"(h) BMI\", x = \"BMI\", y = \"Count\")\n\n# (i) [cite_start]Smoking Status vs. Stroke [cite: 141, 260]\np1i &lt;- ggplot(df_plot, aes(y = smoking_status, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"(i) Smoking Status\", y = NULL, x = \"Count\")\n\n# (j) [cite_start]Work Type vs. Stroke [cite: 262]\np1j &lt;- ggplot(df_plot, aes(y = work_type, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"(j) Work Type\", y = NULL, x = \"Count\")\n\n# Combine all plots into a single figure\nggarrange(p1a, p1b, p1c, p1d, p1e, p1f, p1g, p1h, p1i, p1j, \n          ncol = 4, nrow = 3, \n          common.legend = TRUE, legend = \"bottom\")\n\n\n\n\n\nFigure 1 Recreation: Distribution of various risk factors concerning stroke occurrence.\n\n\n\n\nNow lets plot them individually for better visualization:\n\n\nCode\np1a \n\n\n\n\n\n\n\n\n\nCode\np1b \n\n\n\n\n\n\n\n\n\nCode\np1c \n\n\n\n\n\n\n\n\n\nCode\np1d \n\n\n\n\n\n\n\n\n\nCode\np1e \n\n\n\n\n\n\n\n\n\nCode\np1f \n\n\n\n\n\n\n\n\n\nCode\np1g \n\n\n\n\n\n\n\n\n\nCode\np1h \n\n\n\n\n\n\n\n\n\nCode\np1i \n\n\n\n\n\n\n\n\n\nCode\np1j\n\n\n\n\n\n\n\n\n\n\n3.1.2 Plot Figure 2\nFigure 2 is the box plots of numerical features to detect outliers. It will help to give us clues about which numerical features to pay more attention to.\nTherefore from analysing the images we can conlude that:\nFigure 2(a) Age: Shows no points beyond the whiskers. This indicates that there are no statistical outliers in the age data. The ages of individuals in the dataset fall within a typical, expected range without extreme values.\nFigure 2(b) BMI: The BMI box plot displays numerous red dots above the top whisker. These points represent outliers, indicating that a notable portion of individuals in the dataset have a Body Mass Index significantly higher than the majority of the population.\nFigure 2(c) Average Glucose Level: Similar to the BMI plot, this visualization shows many red dots extending far above the top whisker. This demonstrates a “notable presence of outliers” for average glucose level, meaning many individuals have blood sugar levels that are exceptionally high compared to the central tendency of the data.\n\n# Plot (a): Box plot for Age\np2a &lt;- ggplot(df_mice, aes(y = age)) +\n  geom_boxplot(fill = \"skyblue\", color = \"black\", outlier.color = \"red\") +\n  labs(title = \"(a) Age\", x = \"\", y = \"Age\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n\n# Plot (b): Box plot for BMI\np2b &lt;- ggplot(df_mice, aes(y = bmi)) +\n  geom_boxplot(fill = \"lightgreen\", color = \"black\", outlier.color = \"red\") +\n  labs(title = \"(b) BMI\", x = \"\", y = \"BMI\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n  \n# Plot (c): Box plot for Average Glucose Level\np2c &lt;- ggplot(df_mice, aes(y = avg_glucose_level)) +\n  geom_boxplot(fill = \"lightcoral\", color = \"black\", outlier.color = \"red\") +\n  labs(title = \"(c) Average Glucose Level\", x = \"\", y = \"Average Glucose Level\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n\n# Combine all plots into a single figure\nggarrange(p2a, p2b, p2c, ncol = 3)\n\n\n\n\nFigure 2 Recreation: Box plots for Age, BMI, and Average Glucose Level to assess the presence of outliers.\n\n\n\n\nDetecting and addressing these outliers might be a critical step in building an accurate and reliable model for predicting stroke incidence. Because they can negatively impact the model’s performance in several key ways as example:\nImproved Model Accuracy\nOutliers can skew the entire dataset, disproportionately influencing the model’s training process. For example, a few individuals with extremely high glucose levels could pull the model’s decision-making process, causing it to overemphasize glucose as a predictor and make less accurate predictions for the majority of people with normal or moderately high levels.\nBy handling these outliers, the model can learn from the true, underlying patterns in the data rather than being misled by anomalous values, leading to higher overall accuracy.\nEnhanced Model Robustness\nA model trained on data containing outliers will not generalize well to new, unseen data that doesn’t have the same outliers. This is a form of overfitting.\nValidating Statistical Assumptions\nOutliers can violate the assumptions required for proper model fitting, compromising the validity of the model’s results.\nUncovering Insights or Errors\nThese outliers can be very insightful in itself. For example, an outlier could represent:\n\nA data entry error (e.g., a typo in BMI or glucose level) that needs to be corrected.\nA genuinely rare medical case that might belong to a specific high-risk subgroup.\n\nTherefore we have Identified that BMI and Average Glucose Level have a significant ammount of outliers.\n\n\n3.1.3 plotting Fig 3\n\n\nCode\n# --- Prepare data for plotting Fig 3 ---\ndf_plot_fig3 &lt;- df_mice |&gt;\n  mutate(stroke = factor(stroke, labels = c(\"No Stroke\", \"Stroke\")))\n\n\n\n\nCode\n# --- Prepare data for plotting ---\n# Reversing the factor levels will swap the default ggplot colors\ndf_plot_fig3 &lt;- df_mice |&gt;\n  mutate(stroke = factor(stroke, labels = c(\"No Stroke\", \"Stroke\")) |&gt; \n                  forcats::fct_rev()) # Reversing the factor levels\n\n\n\n\nCode\n# Plot (a): Age vs. BMI\np3a &lt;- ggplot(df_plot_fig3, aes(x = age, y = bmi, color = stroke)) +\n  geom_point(alpha = 0.6, size = 1.5) +\n  gghighlight(stroke == \"Stroke\") + # Highlight stroke cases\n  labs(title = \"(a) Age vs. BMI\", x = \"Age\", y = \"BMI\") +\n  theme_minimal()\n\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: stroke\n\n\nToo many data points, skip labeling\n\n\nCode\n# Plot (b): Average Glucose Level vs. Age\np3b &lt;- ggplot(df_plot_fig3, aes(x = avg_glucose_level, y = age, color = stroke)) +\n  geom_point(alpha = 0.6, size = 1.5) +\n  gghighlight(stroke == \"Stroke\") + # Highlight stroke cases\n  labs(title = \"(b) Avg. Glucose Level vs. Age\", x = \"Average Glucose Level\", y = \"Age\") +\n  theme_minimal()\n\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: stroke\nToo many data points, skip labeling\n\n\nCode\n# Plot (c): BMI vs. Average Glucose Level\np3c &lt;- ggplot(df_plot_fig3, aes(x = bmi, y = avg_glucose_level, color = stroke)) +\n  geom_point(alpha = 0.6, size = 1.5) +\n  gghighlight(stroke == \"Stroke\") + # Highlight stroke cases\n  labs(title = \"(c) BMI vs. Avg. Glucose Level\", x = \"BMI\", y = \"Average Glucose Level\") +\n  theme_minimal()\n\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: stroke\nToo many data points, skip labeling\n\n\nMaking Figure 3\n\n\nCode\n# Combine all plots into a single figure to make Figure 3\nggarrange(p3a, p3b, p3c, \n          ncol = 3, \n          common.legend = TRUE, legend = \"bottom\")\n\n\n\n\n\n\n\n\n\nDisplay all plots individually for better visualization:\n\n\nCode\np3a\n\n\n\n\n\n\n\n\n\nCode\np3b\n\n\n\n\n\n\n\n\n\nCode\np3c\n\n\n\n\n\n\n\n\n\n\n\n3.1.4 Plotting Fig 4\nPrepare data for correlation matrix\n\n\nCode\n# --- Prepare data for correlation matrix ---\n# Convert all factors to numeric representations for correlation\n# We use model.matrix to perform one-hot encoding on categorical variables\ndf_numeric &lt;- model.matrix(~.-1, data = df_mice) |&gt;\n  as.data.frame()\n\n# Rename columns for clarity (model.matrix adds prefixes)\ncolnames(df_numeric) &lt;- gsub(\"gender|work_type|smoking_status|Residence_type|ever_married\", \"\", colnames(df_numeric))\n\n\nGenerate Figure 4: Correlation heatmap with a sequential green color palette.”\n\n\nCode\n# 1. Calculate the correlation matrix\ncorrelation_matrix &lt;- cor(df_numeric)\n\n# 2. Define a green sequential color palette\n# green_palette &lt;- colorRampPalette(c(\"#E5F5E0\", \"#31A354\"))(200) # Light to dark green\ngreen_palette &lt;- colorRampPalette(c(\"#d5ffc8ff\", \"#245332ff\"))(200) \n\n# corrplot(correlation_matrix, method = 'number') # colorful number\n# 3. Create the heatmap with the correct palette\ncorrplot(correlation_matrix, \n         method = \"color\",\n         type = \"full\", # change to full or upper\n         order = \"hclust\",\n         tl.col = \"black\",\n         tl.srt = 45,\n         addCoef.col = \"black\",\n         number.cex = 0.7,\n         col = green_palette, # Use the new palette here\n         diag = FALSE)\n\n\nWarning in ind1:ind2: numerical expression has 2 elements: only the first used\n\n\n\n\n\nFigure 4: Correlation heatmap with a sequential green color palette.\n\n\n\n\n\n\n\n3.2 Feature Importance\nThe study identifies age, average glucose level, BMI, heart disease, hypertension, and marital status as the most influential predictors. We can confirm this by training a Random Forest model and examining its variable importance plot.\nThe plot should confirm that age, avg_glucose_level, and bmi are the top three predictors, consistent with the findings in the paper\nFigure 25.  Feature importance comparison for the proposed DSE model. Feature importance graphs for imbalanced and balanced MICE-imputed datasets are displayed in (a) and (b) respectively\n\n\nCode\n# Train a simple Random Forest model to check feature importance\nrf_model_for_importance &lt;- ranger(stroke ~ ., data = df_mice, importance = 'permutation')\n\n# Create importance plot\nimportance_data &lt;- data.frame(\n  Variable = names(rf_model_for_importance$variable.importance),\n  Importance = rf_model_for_importance$variable.importance\n)\n\nggplot(importance_data, aes(x = reorder(Variable, Importance), y = Importance)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  coord_flip() +\n  labs(title = \"Feature Importance for Stroke Prediction\", x = \"Features\", y = \"Importance\") +\n  theme_minimal()\n\n\n\n\n\nFeature importance for stroke prediction using a Random Forest model."
  },
  {
    "objectID": "posts/renan-blog-post-week6/index.html#model-building-and-evaluation",
    "href": "posts/renan-blog-post-week6/index.html#model-building-and-evaluation",
    "title": "Dataset Exploration - Week 6",
    "section": "4. Model Building and Evaluation",
    "text": "4. Model Building and Evaluation\nThe paper evaluates a baseline model, several advanced models, and a final Dense Stacking Ensemble (DSE) model. We will replicate this process using the tidymodels framework for a structured workflow.\n\n4.1 Data Splitting and Preprocessing Recipe\nWe will use the MICE-imputed datasets (both imbalanced and balanced) for modeling. We’ll split the data into training (70%) and testing (30%) sets and create a preprocessing recipe for one-hot encoding categorical variables and normalizing numerical features.\n\n\nCode\n# Use the MICE imputed data\n# data_imb &lt;- df_mice\n# data_bal &lt;- roc_rose(df_mice, \"stroke\")$data # ROSE is similar to SMOTE\ndata_imb &lt;- df_mice\ndata_bal &lt;- ROSE(stroke ~ ., data = df_mice, seed = 123)$data\n\n# --- Imbalanced Data ---\nset.seed(123)\nsplit_imb &lt;- initial_split(data_imb, prop = 0.7, strata = stroke)\ntrain_imb &lt;- training(split_imb)\ntest_imb  &lt;- testing(split_imb)\n\n# --- Balanced Data ---\nset.seed(123)\nsplit_bal &lt;- initial_split(data_bal, prop = 0.7, strata = stroke)\ntrain_bal &lt;- training(split_bal)\ntest_bal  &lt;- testing(split_bal)\n\n\n# Create a preprocessing recipe\nrecipe_spec &lt;- recipe(stroke ~ ., data = train_imb) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n\n\n\n4.2 Model Definitions\nWe define the models used in the study.\n\n\nCode\n# 1. Baseline: Logistic Regression\nlog_reg_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\n# 2. Advanced: Random Forest\nrf_spec &lt;- rand_forest(trees = 100) %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\") %&gt;%\n  set_mode(\"classification\")\n\n# 3. Advanced: XGBoost\nxgb_spec &lt;- boost_tree(trees = 100) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\n\n\n\n4.3 Training and Evaluating Models\nWe will create workflows, train the models, and evaluate their performance on the test set.\n\n4.3.1 Baseline Model (Logistic Regression)\n\n\nCode\n# Create a balanced data frame using a tidymodels recipe\ndata_bal &lt;- recipe(stroke ~ ., data = df_mice) %&gt;%\n  step_rose(stroke) %&gt;%\n  prep() %&gt;%\n  juice()\n\n# Split the balanced data into training and testing sets\nset.seed(123)\nsplit_bal &lt;- initial_split(data_bal, prop = 0.7, strata = stroke)\ntrain_bal &lt;- training(split_bal)\ntest_bal  &lt;- testing(split_bal)\n\n# Confirm that train_bal was created\ncat(\"Balanced training data created successfully. Dimensions:\\n\")\n\n\nBalanced training data created successfully. Dimensions:\n\n\nCode\ndim(train_bal)\n\n\n[1] 6803   11\n\n\nCode\n# Workflow for logistic regression\nlog_reg_wf &lt;- workflow() %&gt;%\n  add_recipe(recipe_spec) %&gt;%\n  add_model(log_reg_spec)\n\n# Train on imbalanced data\nfit_log_reg_imb &lt;- fit(log_reg_wf, data = train_imb)\npreds_log_reg_imb &lt;- predict(fit_log_reg_imb, test_imb) %&gt;%\n  bind_cols(test_imb %&gt;% select(stroke))\n\n# Train on balanced data\nfit_log_reg_bal &lt;- fit(log_reg_wf, data = train_bal)\npreds_log_reg_bal &lt;- predict(fit_log_reg_bal, test_bal) %&gt;%\n  bind_cols(test_bal %&gt;% select(stroke))\n\n\n# Evaluate performance\nmetrics_log_reg_imb &lt;- metrics(preds_log_reg_imb, truth = stroke, estimate = .pred_class)\nmetrics_log_reg_bal &lt;- metrics(preds_log_reg_bal, truth = stroke, estimate = .pred_class)\n\ncat(\"Baseline (Logistic Regression) - Imbalanced Data:\\n\")\n\n\nBaseline (Logistic Regression) - Imbalanced Data:\n\n\nCode\nprint(metrics_log_reg_imb)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary        0.952 \n2 kap      binary        0.0251\n\n\nCode\ncat(\"\\nBaseline (Logistic Regression) - Balanced Data:\\n\")\n\n\n\nBaseline (Logistic Regression) - Balanced Data:\n\n\nCode\nprint(metrics_log_reg_bal)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.772\n2 kap      binary         0.544\n\n\nAs the paper notes, the baseline model’s performance improves significantly on the balanced dataset.\n\n\n4.3.2 Advanced Models (Random Forest and XGBoost)\n\n\nCode\n# --- Random Forest ---\nrf_wf &lt;- workflow() |&gt; add_recipe(recipe_spec) |&gt; add_model(rf_spec)\nfit_rf_bal &lt;- fit(rf_wf, data = train_bal)\npreds_rf_bal &lt;- predict(fit_rf_bal, test_bal) |&gt; bind_cols(test_bal |&gt; select(stroke))\nmetrics_rf_bal &lt;- metrics(preds_rf_bal, truth = stroke, estimate = .pred_class)\n\n# --- XGBoost ---\nxgb_wf &lt;- workflow() |&gt; add_recipe(recipe_spec) |&gt; add_model(xgb_spec)\nfit_xgb_bal &lt;- fit(xgb_wf, data = train_bal)\npreds_xgb_bal &lt;- predict(fit_xgb_bal, test_bal) |&gt; bind_cols(test_bal |&gt; select(stroke))\nmetrics_xgb_bal &lt;- metrics(preds_xgb_bal, truth = stroke, estimate = .pred_class)\n\ncat(\"\\nAdvanced Model (Random Forest) - Balanced Data:\\n\")\n\n\n\nAdvanced Model (Random Forest) - Balanced Data:\n\n\nCode\nprint(metrics_rf_bal)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.868\n2 kap      binary         0.737\n\n\nCode\ncat(\"\\nAdvanced Model (XGBoost) - Balanced Data:\\n\")\n\n\n\nAdvanced Model (XGBoost) - Balanced Data:\n\n\nCode\nprint(metrics_xgb_bal)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.856\n2 kap      binary         0.713\n\n\nCode\n# Confusion Matrix for XGBoost on balanced data\nconf_mat_xgb &lt;- conf_mat(preds_xgb_bal, truth = stroke, estimate = .pred_class)\nautoplot(conf_mat_xgb, type = \"heatmap\") + ggtitle(\"XGBoost Confusion Matrix (Balanced Data)\")\n\n\n\n\n\n\n\n\n\nOn the balanced dataset, XGBoost and Random Forest perform exceptionally well, achieving high accuracy and balanced precision/recall, aligning with the paper’s findings that these models are top performers.\n\n\n\n4.4 Dense Stacking Ensemble (DSE) Model\nThe paper’s key contribution is a DSE model, which uses the best-performing model (Random Forest) as a meta-classifier. We can build a similar ensemble using the stacks package.\n\n\nCode\n# Define k-fold cross-validation\nfolds &lt;- vfold_cv(train_bal, v = 10, strata = stroke)\n\n# Control settings to save predictions\nctrl_grid &lt;- control_stack_grid()\n\n# Fit models with cross-validation\nlog_reg_res &lt;- fit_resamples(log_reg_wf, resamples = folds, control = ctrl_grid)\nrf_res &lt;- fit_resamples(rf_wf, resamples = folds, control = ctrl_grid)\nxgb_res &lt;- fit_resamples(xgb_wf, resamples = folds, control = ctrl_grid)\n\n\n# Initialize a data stack\nstroke_stack &lt;- stacks() |&gt;\n  add_candidates(log_reg_res) |&gt;\n  add_candidates(rf_res) |&gt;\n  add_candidates(xgb_res)\n\n# Blend predictions to create the ensemble\nensemble_model &lt;- blend_predictions(stroke_stack, penalty = 0.1)\nfit_ensemble &lt;- fit_members(ensemble_model)\n\n\n# Evaluate the DSE model on the test set\npreds_ensemble &lt;- predict(fit_ensemble, test_bal) |&gt;\n  bind_cols(test_bal |&gt; select(stroke))\nmetrics_ensemble &lt;- metrics(preds_ensemble, truth = stroke, estimate = .pred_class)\n\n\ncat(\"\\nDense Stacking Ensemble (DSE) Model Performance - Balanced Data:\\n\")\n\n\n\nDense Stacking Ensemble (DSE) Model Performance - Balanced Data:\n\n\nCode\nprint(metrics_ensemble)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.867\n2 kap      binary         0.734\n\n\nThe DSE model achieves an accuracy of over 96%, demonstrating the power of ensembling. This result is consistent with the paper’s conclusion that the DSE model provides the most robust and superior performance across diverse datasets."
  },
  {
    "objectID": "posts/renan-blog-post-week6/index.html#conclusion",
    "href": "posts/renan-blog-post-week6/index.html#conclusion",
    "title": "Dataset Exploration - Week 6",
    "section": "5. Conclusion",
    "text": "5. Conclusion\nThis document successfully reproduced the core findings of the study “Predictive modelling and identification of key risk factors for stroke using machine learning.” Through this R-based implementation, we confirmed that:\n\nHandling missing data and class imbalance is crucial for building accurate predictive models in healthcare.\nThe key risk factors identified—age, BMI, average glucose level, hypertension, and heart disease—are indeed highly predictive of stroke risk.\nWhile individual models like XGBoost and Random Forest perform well, a Dense Stacking Ensemble (DSE) model delivers the highest and most stable performance, achieving accuracy greater than 96%.\n\nThe DSE model’s ability to combine the strengths of multiple algorithms makes it an excellent candidate for real-world clinical applications, potentially aiding in the early detection of stroke and improving patient outcomes.\n\nReferences\n\n\n1. Hassan, A., Gulzar Ahmad, S., Ullah Munir, E., Ali Khan, I., & Ramzan, N. (2024). Predictive modelling and identification of key risk factors for stroke using machine learning. Scientific Reports, 14(1), 11498.\n\n\n2. Kokkotis, C., Giarmatzis, G., Giannakou, E., Moustakidis, S., Tsatalas, T., Tsiptsios, D., Vadikolias, K., & Aggelousis, N. (2022). An explainable machine learning pipeline for stroke prediction on imbalanced data. Diagnostics, 12(10), 2392.\n\n\n3. Sirsat, M. S., Fermé, E., & Câmara, J. (2020). Machine learning for brain stroke: A review. Journal of Stroke and Cerebrovascular Diseases, 29(10), 105162.\n\n\n4. Wongvorachan, T., He, S., & Bulut, O. (2023). A comparison of undersampling, oversampling, and SMOTE methods for dealing with imbalanced classification in educational data mining. Information, 14(1), 54.\n\n\n5. Sowjanya, A. M., & Mrudula, O. (2023). Effective treatment of imbalanced datasets in health care using modified SMOTE coupled with stacked deep learning algorithms. Applied Nanoscience, 13(3), 1829–1840.\n\n\n6. fedesoriano. (n.d.). Stroke Prediction Dataset. https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset"
  },
  {
    "objectID": "posts/renan-blog-post-week2/index.html",
    "href": "posts/renan-blog-post-week2/index.html",
    "title": "Literature Review Week 2",
    "section": "",
    "text": "This week I review 2 articles"
  },
  {
    "objectID": "posts/renan-blog-post-week2/index.html#article-1",
    "href": "posts/renan-blog-post-week2/index.html#article-1",
    "title": "Literature Review Week 2",
    "section": "Article 1",
    "text": "Article 1\nFrom Logistic Regression to the Perceptron Algorithm: Exploring Gradient Descent with Large Step Sizes.[1]\nThe author presents some interesting findings that by connecting Logistic regression with gradient descent there is a link with the perceptron algorithm. With really large steps it acts like a perceptron which in some sense links it back to the Deep Equilibrium networks study. This paper is interesting because it is counter intuitive and brings a lot of things to reflect about classification and optimization theory."
  },
  {
    "objectID": "posts/renan-blog-post-week2/index.html#article-2",
    "href": "posts/renan-blog-post-week2/index.html#article-2",
    "title": "Literature Review Week 2",
    "section": "Article 2",
    "text": "Article 2\nLarge Language Model Confidence Estimation via Black-Box Access.[2]\nThis paper addresses the problem of estimating the confidence of large language model (LLM) outputs when only black-box (query-only) access is available. It is a simple technique that uses Logistic Regression to classify and validate the confidence of the outputs. The problem of using the black-box models is that there is no control over the model itself, but in some cases the benefits and the value of buying these services that provide a black-box model outweighs training your own custom so this is a framework that attempts to overcome the challenges.\n\nReferences\n\n\n1. Tyurin, A. (2024). From logistic regression to the perceptron algorithm: Exploring gradient descent with large step sizes. https://arxiv.org/abs/2412.08424\n\n\n2. Pedapati, T., Dhurandhar, A., Ghosh, S., Dan, S., & Sattigeri, P. (2025). Large language model confidence estimation via black-box access. https://arxiv.org/abs/2406.04370"
  },
  {
    "objectID": "posts/renan-blog-post-week3/index.html",
    "href": "posts/renan-blog-post-week3/index.html",
    "title": "Literature Review Week 3",
    "section": "",
    "text": "This week I review 2 articles"
  },
  {
    "objectID": "posts/renan-blog-post-week3/index.html#article-1",
    "href": "posts/renan-blog-post-week3/index.html#article-1",
    "title": "Literature Review Week 3",
    "section": "Article 1",
    "text": "Article 1\nBERT or FastText? A Comparative Analysis of Contextual as well as Non-Contextual Embeddings.[1]\nMy personal opinion: This research doesn’t explicitly state why Logistic Regression is important, but it did use it as the classifier for all of the experiments to maintain methodological simplicity. All embeddings were passed to a multinomial logistic regression (MLR) classifier for classification into target labels. Which shows the versatility of logistic regression when elaborating an experiment to test a hypothesis.\nThe main goal of the paper is to analyze the effectiveness of non-contextual embeddings from BERT models (MuRIL and MahaBERT) and FastText models (IndicFT and MahaFT) for NLP tasks. The authors compare these embeddings to contextual and compressed variants of BERT aiming to fill a research gap, because previous research did not explore non-contextual embeddings.\nThe research is important because it addresses the challenges faced by NLP in low-resource languages (The ones that lack big annotated datasets to properly train). The selection of an effective embedding method is extremely important for strong NLP performance. The research tries a promising alternative, non-contextual BERT embeddings, which can be obtained through a simple table lookup, unlike contextual embeddings that require a full forward pass through the model. This is particularly relevant for getting model performance with much better computational efficiency.\nThe methodology is quite interesting. For the FastText, which is a non-contextual embedding by default, they had to create a custom vocabulary. Which was achieved by concatenating the training and validation datasets and then passing them through a text vectorizer, which generated vectors for every word in the dataset. The vectorizer returned the vocabulary as a list of words in decreasing order of their frequency. Then the FastText model was then loaded using the FastText library, and for each word in the vocabulary, a word vector was retrieved to construct the embedding matrix. For each sentence, the text was split into individual words, and the corresponding embeddings were retrieved from the embedding matrix. These embeddings were then averaged to produce the final sentence embeddings.\nFurthermore they did not stop with FastText, they also experimented with compressed embeddings by reducing the dimensionality from 768 (the traditional BERT embedding dimension) to 300. This compression was performed using Singular Value Decomposition (SVD) to select the most relevant features, extracting the top 300 components for all the combinations of contextual as well as non-contextual for MahaBERT as well as Muril.\nIn this approach it’s interesting how they did use Logistic regression for simplicity. All embeddings were then passed to a multiple logistic regression(MLR) classifier for classification into target labels.\nI understood that as a result they did show that contextual BERT embeddings perform better than non-contextual ones, including both non-contextual BERT embeddings and FastText. So in the end they proved that their approach did not improve much or provided much resource to support this different approach. They also showed that when non-contextual BERT embeddings are compressed, their performance drops, and FastText performs better than compressed noncontextual BERT. But this is a questionable finding.\nThe limitations of the research is that even in most cases it was apparent that compression lowers the performance of non-contextual BERT embeddings. The effect of compression on contextual embeddings varies across datasets and there is no consistent way to properly derive conclusions."
  },
  {
    "objectID": "posts/renan-blog-post-week3/index.html#article-2",
    "href": "posts/renan-blog-post-week3/index.html#article-2",
    "title": "Literature Review Week 3",
    "section": "Article 2",
    "text": "Article 2\nPriority prediction of Asian Hornet sighting report using machine learning methods.[2]\nThe goal of the research is to create an automated system to predict the priority of Asian giant hornet sighting reports. Asian giant hornets are an invasive species that poses a significant threat to native bee populations and local beekeeping, as well as to public safety due to their aggressive nature and potent venom. So it’s very important that reports are properly assessed for priority.\nThe authors did model the priority prediction of sighting reports as a two-classification problem. This approach was pretty clever and simple. The goal was to just classify reports as either a “true positive” or a “false positive”.\nTheir methodology is a straightforward application of logistic regression with feature extraction. They came to realize that they needed Location Feature, Time Feature, Image Feature, Text Feature.\nLocation Feature considers the probability of a hornet being observed at a specific location based on known hornet migration patterns and habits. Time Feature accounts for the hornet’s seasonal behavior. Since hornets are most active from April to December, a report submitted during this period is more likely to be positive. Image Feature is the number of images attached to a report and they came to notice that it is correlated with the increase of its credibility. Text Feature is the textual description’s length and keywords. A longer text is considered more credible because it contains more evidence. The model also uses a specific dictionary of hornet characteristics to identify relevant keywords.\nThey then used a weighted binary cross-entropy function and the logistic regression is just mapping the probability given the feature vector.\nThe model achieved an average prediction accuracy of 83.5% on positive reports with the best weighting parameter settings, but still far from other works which achieved about 93% using Deep Learning. So this is the main limitation, still needs a lot of improvement or maybe it will never outmatch other methods due to hidden limitations.\nMy opinion on this paper is that the Logistic Regression has interesting properties, after all it is a generalized linear model, which conducts mapping from any real number to probability values.\n\nReferences\n\n\n1. Shanbhag, A., Jadhav, S., Thakurdesai, A., Sinare, R., & Joshi, R. (2025). Non-contextual BERT or FastText? A comparative analysis. https://arxiv.org/abs/2411.17661\n\n\n2. Liu, Y., Guo, J., Dong, J., Jiang, L., & Ouyang, H. (2021). Priority prediction of asian hornet sighting report using machine learning methods. 2021 IEEE International Conference on Software Engineering and Artificial Intelligence (SEAI), 7–11. https://doi.org/10.1109/seai52285.2021.9477549"
  },
  {
    "objectID": "posts/renan-blog-post-week4/index.html",
    "href": "posts/renan-blog-post-week4/index.html",
    "title": "Literature Review Week 4",
    "section": "",
    "text": "This week I review 2 articles"
  },
  {
    "objectID": "posts/renan-blog-post-week4/index.html#article-1",
    "href": "posts/renan-blog-post-week4/index.html#article-1",
    "title": "Literature Review Week 4",
    "section": "Article 1",
    "text": "Article 1\nIncorporating LLM Priors into Tabular Learners.[1]\nThere have been implementations of transformer based architectures for tabular data. Most of the time it has been utilized for generating synthetic data for likelihood free models or for cases where there is not enough data for fitting a model.\nThe goal of this research was to bootstrap a way so one could use off the shelf models like Chatgpt which are really good at generalization to perform similarly to dedicated models trained on tabular data such as tabLLM. This is important because it is fairly cheaper and more accessible than training a model from scratch and it overcomes the complexities of developing a specialized encoder.\nThe methodology is a pretty hacky solution where they serialized the tabular data so they could prompt the models are are just trying to obtain back a categorization through prompt engineering which will be attributed a value which is manually tuned by the authors and this value is later used on the Monotonic Logistic Regression.\nThe limitations are quite clear. There is no way to guarantee the black box model output will be consistent. You have to manually categorize and do some prompt engineering. The model has bias so it either works really well or it doesn’t.\nThe bright side is that this approach is extremely cheap and is accessible. It can be used to test ideas and hypothesis as well rapidly prototype before committing to a more definite solution such as tabLLM."
  },
  {
    "objectID": "posts/renan-blog-post-week4/index.html#article-2",
    "href": "posts/renan-blog-post-week4/index.html#article-2",
    "title": "Literature Review Week 4",
    "section": "Article 2",
    "text": "Article 2\nUsing a monotonic density ratio model to increase the power of the goodness-of-fit test for logistic regression models with case-control data.[2]\nCase-control sampling is used because it is a quick, economical, and efficient method for studying rare diseases or outcomes, long latent periods, or outbreaks. It allows researchers to investigate multiple potential risk factors simultaneously for a single outcome and is especially useful when prospective cohort studies are not feasible.\nThe author’s goal seems to be to improve the statistical power of the goodness-of-fit test for logistic regression models when used with case-control data. They improved upon a previous popular method from Qin and Zhang, instead of using the nonparametric empirical distribution function, we use the constrained nonparametric MLE of G(x) to further improve the power performance of the Kolmogorov-Smirnov-type goodness-of-fit test for logistic models.\nBefore drawing conclusions from a logistic regression model, it’s crucial to verify that the model’s assumptions hold true for the data and there are many limitations. Case Study data is specially complicated because there is not enough data and there are too many unknowns.\nThe authors spare no comments on the limitations, the bigger limitations are: - The test is designed for goodness-of-fit and cannot be used to compare two different logistic regression models - The test has no power when the only covariate is categorical. In this situation, the logistic model is “saturated,” meaning it perfectly fits the data by definition and cannot be misspecified.\nTheir results are overall quite interesting as they demonstrated that they could bootstrap an algorithm that is quite clever and intuitively it shouldn’t work. It is a hard problem to solve so it is interesting out of the box thinking\n\nReferences\n\n\n1. Zhu, M., Stanivuk, S., Petrovic, A., Nikolic, M., & Lio, P. (2023). Incorporating LLM priors into tabular learners. https://arxiv.org/abs/2311.11628\n\n\n2. Wang, C., Liu, Z., & Wang, X. (2024). Using a monotonic density ratio model to increase the power of the goodness-of-fit test for logistic regression models with case-control data. Statistics in Medicine, 43(22), 4272–4286."
  },
  {
    "objectID": "posts/renan-blog-post-week7/Readme.html",
    "href": "posts/renan-blog-post-week7/Readme.html",
    "title": "Notes on project Week 6",
    "section": "",
    "text": "Missing the Features Correlation Matrix, it is on the Page 6 of the paper.\nFigure 4.  Features correlation heatmap for the dataset. Color intensity indicates the strength and direction of correlations, aiding in the identification of potential patterns and dependencies in the data.\n\n\n\nMissing the Sparsity Matrix, it is on the Page 6 of the paper.\nFigure 5.  Sparsity matrix for the dataset. The empty spaces found in the corresponding column signify the presence of missing data values for the specific feature."
  },
  {
    "objectID": "posts/renan-blog-post-week7/Readme.html#features-correlation-matrix",
    "href": "posts/renan-blog-post-week7/Readme.html#features-correlation-matrix",
    "title": "Notes on project Week 6",
    "section": "",
    "text": "Missing the Features Correlation Matrix, it is on the Page 6 of the paper.\nFigure 4.  Features correlation heatmap for the dataset. Color intensity indicates the strength and direction of correlations, aiding in the identification of potential patterns and dependencies in the data."
  },
  {
    "objectID": "posts/renan-blog-post-week7/Readme.html#sparsity-matrix",
    "href": "posts/renan-blog-post-week7/Readme.html#sparsity-matrix",
    "title": "Notes on project Week 6",
    "section": "",
    "text": "Missing the Sparsity Matrix, it is on the Page 6 of the paper.\nFigure 5.  Sparsity matrix for the dataset. The empty spaces found in the corresponding column signify the presence of missing data values for the specific feature."
  },
  {
    "objectID": "posts/shree-blog-post-week5/index.html",
    "href": "posts/shree-blog-post-week5/index.html",
    "title": "Literature Review Week 5",
    "section": "",
    "text": "Link: https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2839330\nPrevalence of Clinical Obesity in US Adults Based on a Newly Proposed Definition[1]\nThe Lancet Diabetes & Endocrinology Commission has proposed a new definition of clinical obesity that includes evidence of organ malfunction or physiological impairment in addition to direct measures of adiposity.\nIn order to find populations who might have been incorrectly classified under previous BMI thresholds, the researchers aimed to compare BMI-based obesity with clinical obesity using national data from the United States.\nMethodology:\n\nCross-sectional study using NHANES 2017–2018 data (nationally representative, multistage sampling)\nAnalysis: Conducted in Stata 18; weighted percentages with 95% CIs; significance at p&lt;0.05\nGuidelines: Followed STROBE reporting standard.\n\nKey points - BMI and Clinical Impact : BMI data is insufficient as it leaves who already experience obesity-related dysfunction. - Older adults are more seen in clinical obesity even at lower BMIs = highlights BMI’s limitation in aging populations. - Younger, higher-income adults are seen with mostly BMI-only obese: high weight but not yet showing dysfunction. - Public health implication: Using the clinical definition could better target interventions (medication, surgery, lifestyle) and identify people at risk earlier.\nSummary: Although the overall obesity rates determined by BMI and clinical criteria are comparable, they distinguish distinct populations. The clinical definition emphasizes the significance of early prevention for individuals with preclinical obesity and more accurately reflects the health effects, particularly in older and underprivileged populations."
  },
  {
    "objectID": "posts/shree-blog-post-week5/index.html#article-1",
    "href": "posts/shree-blog-post-week5/index.html#article-1",
    "title": "Literature Review Week 5",
    "section": "",
    "text": "Link: https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2839330\nPrevalence of Clinical Obesity in US Adults Based on a Newly Proposed Definition[1]\nThe Lancet Diabetes & Endocrinology Commission has proposed a new definition of clinical obesity that includes evidence of organ malfunction or physiological impairment in addition to direct measures of adiposity.\nIn order to find populations who might have been incorrectly classified under previous BMI thresholds, the researchers aimed to compare BMI-based obesity with clinical obesity using national data from the United States.\nMethodology:\n\nCross-sectional study using NHANES 2017–2018 data (nationally representative, multistage sampling)\nAnalysis: Conducted in Stata 18; weighted percentages with 95% CIs; significance at p&lt;0.05\nGuidelines: Followed STROBE reporting standard.\n\nKey points - BMI and Clinical Impact : BMI data is insufficient as it leaves who already experience obesity-related dysfunction. - Older adults are more seen in clinical obesity even at lower BMIs = highlights BMI’s limitation in aging populations. - Younger, higher-income adults are seen with mostly BMI-only obese: high weight but not yet showing dysfunction. - Public health implication: Using the clinical definition could better target interventions (medication, surgery, lifestyle) and identify people at risk earlier.\nSummary: Although the overall obesity rates determined by BMI and clinical criteria are comparable, they distinguish distinct populations. The clinical definition emphasizes the significance of early prevention for individuals with preclinical obesity and more accurately reflects the health effects, particularly in older and underprivileged populations."
  },
  {
    "objectID": "posts/shree-blog-post-week5/index.html#article-2",
    "href": "posts/shree-blog-post-week5/index.html#article-2",
    "title": "Literature Review Week 5",
    "section": "Article 2",
    "text": "Article 2\nBenefit-Risk Reporting for FDA-Cleared Artificial Intelligence−Enabled Medical Devices[2]\nLink: https://jamanetwork.com/journals/jama-health-forum/fullarticle/2839236\nSummary:\nThe effectiveness with which FDA-approved AI/ML-enabled medical devices disclose their advantages, hazards, effectiveness, and safety before to and following approval was investigated in this study\nScope: Using FDA records (decision summaries, adverse events, and recalls), 691 AI/ML devices that were approved between 1995 and 2023 were analyzed.\nResults:\nKey reporting was absent from many devices:\n46.7% did not report the study design.\n53.3% of the training sample size is missing.\n95.5% of demographic information is lacking.\nJust 3 devices (&lt;1%) reported patient outcomes, while only 6 devices (1.6%) used randomized clinical trials. Sensitivity (24%), specificity (22%), and other performance indicators were not reported by many.Just 28.2% of devices had safety assessments, and only 8.7% had bias assessments.Adverse events: 489 incidents involving 36 devices (5.2%), comprising 30 injuries, 1 fatality, and 458 malfunctions.40 devices (5.8%) were recalled 113 times, primarily due to software problems.\nTrends:\nAlthough there was a little improvement in the reporting of bias checks, efficacy, and outcomes for devices cleared after 2021, these devices were less likely to have safety evaluations or peer-reviewed publications.\nConclusion: Standardized reporting of risk, safety, and efficacy is lacking in regulatory monitoring of AI/ML medical devices. The study highlights the necessity of an FDA regulatory approach specifically for AI/ML devices. more robust postmarket surveillance networks.Increased health fairness and transparency (e.g., improved demographic reporting to prevent prejudice).\n\nReferences\n\n\n1. Park, D., Lee, D. H., Kim, R., Shin, M.-J., & Subramanian, S. (2025). Prevalence of clinical obesity in US adults based on a newly proposed definition. JAMA Network Open, 8(9), e2533806–e2533806.\n\n\n2. Lin, J. C., Jain, B., Iyer, J. M., Rola, I., Srinivasan, A. R., Kang, C., Patel, H., & Parikh, R. B. (2025). Benefit-risk reporting for FDA-cleared artificial intelligence- enabled medical devices. JAMA Health Forum, 6, e253351–e253351."
  },
  {
    "objectID": "posts/renan-blog-post-week7/index.html",
    "href": "posts/renan-blog-post-week7/index.html",
    "title": "Reproducing Steve’s Code - Week 7",
    "section": "",
    "text": "For the Week 7 we will be reproducing Steve’s findings with the dataset[1].\nYou can download the Dataset from the following link: Stroke Prediction Dataset"
  },
  {
    "objectID": "posts/renan-blog-post-week7/index.html#setup-and-data-loading",
    "href": "posts/renan-blog-post-week7/index.html#setup-and-data-loading",
    "title": "Reproducing Steve’s Code - Week 7",
    "section": "1. Setup and Data Loading",
    "text": "1. Setup and Data Loading\nFirst we need to install all packages, system dependencies and solve conflicts to produce a new renv.lock file.\n\n1.1 Load Libraries\n\n\nCode\n# Run this once to install all the necessary packages\n# install.packages(c(\"corrplot\", \"ggpubr\", \"caret\", \"mice\", \"ROSE\", \"ranger\", \"stacks\", \"tidymodels\"))\n# install.packages(\"themis\")\n# install.packages(\"xgboost\")\n# install.packages(\"gghighlight\")\n# install.packages(\"dplyr\")\n# install.packages(\"pscl\")\n# install.packages(\"parallelly\")\n# install.packages(\"cli\")\n# install.packages(\"car\")\n# install.packages(\"ResourceSelection\")\n\n\nWe can use this to check installed packages:\n```{r}\nrenv::activate(\"website\")\n\"yardstick\" %in% rownames(installed.packages())\n```\n\n\nCode\n# For data manipulation and visualization\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(knitr)\nlibrary(ggpubr)\n\n# For data preprocessing and modeling\nlibrary(caret)\nlibrary(mice)\nlibrary(ROSE) # For SMOTE\nlibrary(ranger) # A fast implementation of random forests\n\n# For stacking/ensemble models\nlibrary(stacks)\nlibrary(tidymodels)\n\nlibrary(themis)\nlibrary(gghighlight)\n\nlibrary(dplyr)\nlibrary(pscl)\nlibrary(car)\nlibrary(ResourceSelection)\n\n# Set seed for reproducibility\nset.seed(123)\n\n\n\n\n1.2 Load Data\nWill be using my original Dataset as well Steve’s Dataset and compare for differences.\nRenan: kaggle_data1 Steve: stroke1\n\n1.2.1 Renan Dataset\nBelow will be loading the healthcare-dataset-stroke-data.csv and performing necessary changes to the dataset and loading into the DataFrame: kaggle_data1\n\n\nCode\nfind_git_root &lt;- function(start = getwd()) {\n  path &lt;- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path &lt;- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root &lt;- find_git_root()\ndatasets_path &lt;- file.path(repo_root, \"datasets\")\nkaggle_dataset_path &lt;- file.path(datasets_path, \"kaggle-healthcare-dataset-stroke-data/healthcare-dataset-stroke-data.csv\")\nkaggle_data1 = read_csv(kaggle_dataset_path, show_col_types = FALSE)\n\n# unique(kaggle_data1$bmi)\nkaggle_data1 &lt;- kaggle_data1 %&gt;%\n  mutate(bmi = na_if(bmi, \"N/A\")) %&gt;%   # Convert \"N/A\" string to NA\n  mutate(bmi = as.numeric(bmi))         # Convert from character to numeric\n\n# Remove the 'Other' gender row and the 'id' column\nkaggle_data1 &lt;- kaggle_data1 %&gt;%\n  filter(gender != \"Other\") %&gt;%\n  select(-id) %&gt;%\n  mutate_if(is.character, as.factor) # Convert character columns to factors for easier modeling\n\n\n\n\n1.2.1 Steve Dataset\nBelow will be loading the stroke.csv and performing necessary changes to the dataset and loading into the DataFrame: stroke1\n\n\nCode\n# Reading the datafile in (the same one you got for us Renan)#\nsteve_dataset_path &lt;- file.path(datasets_path, \"steve/stroke.csv\")\nstroke1 = read_csv(steve_dataset_path, show_col_types = FALSE)\n# stroke1 &lt;- read.csv(\"D:\\\\stroke.csv\")\n\n\nExploring Dataset so we can plan on how to proceed and possible changes.\n\n\nCode\n# Reveiewing the columns of the data and the dataset size#\nhead(stroke1)\nnrow(stroke1)\n#Some data to look at the data in each column#\nsummary(stroke1)\ncount_tables &lt;- lapply(stroke1, table)\ncount_tables\n\n\nPreparing the Dataset\nFor each Column…removing the unncessary or unusable variables: 1. Smoking Status - remove unknown 1. bmi - remove N/A 3. Work type - remove children 4. age create numerical variable with 2 places after the decimal 5. gender -remove other\nIn each column..that has data points that are not usable, recoding those datapoints to become”N/A”\n\n\nCode\nstroke1[stroke1 == \"N/A\"] &lt;- NA\nstroke1[stroke1 == \"Unknown\"] &lt;- NA\nstroke1[stroke1 == \"children\"] &lt;- NA\nstroke1[stroke1 == \"other\"] &lt;- NA\n\n\nfor BMI changing the variable type to numeric and formatting the data point to 2 places ater the decimal\n\n\nCode\nstroke1$bmi &lt;- round(as.numeric(stroke1$bmi), 2)\n\n\nFor Gender, changning Male to 1 and Female to 2, then reformatting gender as numeric\n\n\nCode\nstroke1$gender[stroke1$gender == \"Male\"] &lt;- 1\nstroke1$gender[stroke1$gender == \"Female\"] &lt;- 2\nstroke1$gender &lt;- as.numeric(stroke1$gender)\n\n\nWarning: NAs introduced by coercion\n\n\nFor ever_married, changing yes to 1 and No to 2, the reformatting the variable ever_married to numeric\n\n\nCode\nstroke1$ever_married[stroke1$ever_married == \"Yes\"] &lt;- 1\nstroke1$ever_married[stroke1$ever_married == \"No\"] &lt;- 2\nstroke1$ever_married &lt;- as.numeric(stroke1$ever_married)\n\n\nFor work type recoding Govt_job to 1, Private to 3, Self-employed to 3, and Never_worked to 4, then reformatting work_type to numeric\n\n\nCode\nstroke1$work_type[stroke1$work_type == \"Govt_job\"] &lt;- 1\nstroke1$work_type[stroke1$work_type == \"Private\"] &lt;- 2\nstroke1$work_type[stroke1$work_type == \"Self-employed\"] &lt;- 3\nstroke1$work_type[stroke1$work_type == \"Never_worked\"] &lt;- 4\nstroke1$work_type &lt;- as.numeric(stroke1$work_type)\n\n\nFor Residence_type, recoding urban to 1, Rural to 2, and then reformatting Residence type to Numeric\n\n\nCode\nstroke1$Residence_type[stroke1$Residence_type == \"Urban\"] &lt;- 1\nstroke1$Residence_type[stroke1$Residence_type == \"Rural\"] &lt;- 2\nstroke1$Residence_type &lt;- as.numeric(stroke1$Residence_type)\n\n\nfor avg_glucose_level, heart_disease, and hypertension, reformattint the 3 variables to numeric\n\n\nCode\nstroke1$avg_glucose_level &lt;- as.numeric(stroke1$avg_glucose_level)\nstroke1$heart_disease &lt;- as.numeric(stroke1$heart_disease)\nstroke1$hypertension &lt;- as.numeric(stroke1$hypertension)\n\n\nFor age, reformatting age to numeric and putting 2 places after the decimnals\n\n\nCode\nstroke1$age &lt;- round(as.numeric(stroke1$age), 2)\n\n\nFor stroke, reformatting the variable stroke to numeric\n\n\nCode\nstroke1$stroke &lt;- as.numeric(stroke1$stroke)\n\n\nFor smoking_status, recoding never smoked to 1, formerly smoked to 2, and smokes to 3. The reformat the variable to numeric\n\n\nCode\nstroke1$smoking_status[stroke1$smoking_status == \"never smoked\"] &lt;- 1\nstroke1$smoking_status[stroke1$smoking_status == \"formerly smoked\"] &lt;- 2\nstroke1$smoking_status[stroke1$smoking_status == \"smokes\"] &lt;- 3\nstroke1$smoking_status &lt;- as.numeric(stroke1$smoking_status)\n\n\ndeleted to column ID from the dataset since its not needed for the analysis\n\n\nCode\nstroke1 &lt;- stroke1[, !(names(stroke1) %in% \"id\")]\n\n\nrenameed stroke dataset without id to stroke1_clean\n\n\nCode\nstroke1_clean &lt;- na.omit(stroke1)\n\n\nconverted all columns to numeric and removed id\n\n\nCode\nstr(stroke1_clean)\n\n\ntibble [3,357 × 11] (S3: tbl_df/tbl/data.frame)\n $ gender           : num [1:3357] 1 1 2 2 1 1 2 2 2 2 ...\n $ age              : num [1:3357] 67 80 49 79 81 74 69 81 61 54 ...\n $ hypertension     : num [1:3357] 0 0 0 1 0 1 0 1 0 0 ...\n $ heart_disease    : num [1:3357] 1 1 0 0 0 1 0 0 1 0 ...\n $ ever_married     : num [1:3357] 1 1 1 1 1 1 2 1 1 1 ...\n $ work_type        : num [1:3357] 2 2 2 3 2 2 2 2 1 2 ...\n $ Residence_type   : num [1:3357] 1 2 1 2 1 2 1 2 2 1 ...\n $ avg_glucose_level: num [1:3357] 229 106 171 174 186 ...\n $ bmi              : num [1:3357] 36.6 32.5 34.4 24 29 27.4 22.8 29.7 36.8 27.3 ...\n $ smoking_status   : num [1:3357] 2 1 3 1 2 1 1 1 3 3 ...\n $ stroke           : num [1:3357] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:1753] 2 9 10 14 20 24 28 30 32 39 ...\n  ..- attr(*, \"names\")= chr [1:1753] \"2\" \"9\" \"10\" \"14\" ...\n\n\nCode\nnrow(stroke1_clean)\n\n\n[1] 3357"
  },
  {
    "objectID": "posts/renan-blog-post-week7/index.html#apply-logistic-regression",
    "href": "posts/renan-blog-post-week7/index.html#apply-logistic-regression",
    "title": "Reproducing Steve’s Code - Week 7",
    "section": "2. Apply Logistic Regression",
    "text": "2. Apply Logistic Regression\nApplying Logistic Regression to Steve Dataset\n\n\nCode\nLR_stroke1 &lt;- stroke1_clean\n#Do Logistic Regression on dataset#\nmodel &lt;- glm(stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, data=LR_stroke1, family = binomial)\nsummary(model)\n\n\n\nCall:\nglm(formula = stroke ~ gender + age + hypertension + heart_disease + \n    ever_married + work_type + Residence_type + avg_glucose_level + \n    bmi + smoking_status, family = binomial, data = LR_stroke1)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -8.426854   0.873243  -9.650  &lt; 2e-16 ***\ngender             0.080370   0.167274   0.480 0.630893    \nage                0.070967   0.006845  10.368  &lt; 2e-16 ***\nhypertension       0.570797   0.182580   3.126 0.001770 ** \nheart_disease      0.417884   0.220311   1.897 0.057856 .  \never_married       0.174316   0.261832   0.666 0.505569    \nwork_type         -0.109615   0.126101  -0.869 0.384703    \nResidence_type     0.005932   0.162188   0.037 0.970822    \navg_glucose_level  0.004658   0.001375   3.388 0.000704 ***\nbmi                0.006275   0.012875   0.487 0.625954    \nsmoking_status     0.179921   0.106431   1.691 0.090932 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1403.5  on 3356  degrees of freedom\nResidual deviance: 1145.4  on 3346  degrees of freedom\nAIC: 1167.4\n\nNumber of Fisher Scoring iterations: 7\n\n\nBecause, Rsquared and adjusted Rsquared is not appropriated for logistic regression model, to see how model fits and explains variance used alternative\n\n2.1 Evaluating model fit\nEvaluating model fit\nComment Oh crap _ McFadden = .18– not a bad fit for logistic regression\n\n\nCode\n# Because, Rsquared and adjusted Rsquared is not appropriated for logistic regression model, to see how model fits and explains variance used alternative#\n#looking at model fit#\npR2(model)\n\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-572.70320797 -701.73792863  258.06944131    0.18387879    0.07399442 \n         r2CU \n   0.21655630 \n\n\n\n\n2.2 Apply Confusion Matrix\nDo a confusion matrix for the model by installing Parallelly, and cli, and using caret from the library\ncomment on confusion matrix =- poor results\n\n\nCode\n# Predict probabilities from the logistic regression model\npredicted_prob &lt;- predict(model, type = \"response\")\n\n# Convert probabilities to binary classes using a 0.5 cutoff\npredicted_class &lt;- ifelse(predicted_prob &gt; 0.5, 1, 0)\n\n\n\n\nCode\n# library(caret)\npredicted_class &lt;- factor(predicted_class, levels = c(0,1))\nForReal_Stroke &lt;- factor(LR_stroke1$stroke, levels = c(0,1))\nconfusionMatrix(predicted_class, ForReal_Stroke)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 3177  178\n         1    0    2\n                                          \n               Accuracy : 0.947           \n                 95% CI : (0.9388, 0.9543)\n    No Information Rate : 0.9464          \n    P-Value [Acc &gt; NIR] : 0.4587          \n                                          \n                  Kappa : 0.0208          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.00000         \n            Specificity : 0.01111         \n         Pos Pred Value : 0.94694         \n         Neg Pred Value : 1.00000         \n             Prevalence : 0.94638         \n         Detection Rate : 0.94638         \n   Detection Prevalence : 0.99940         \n      Balanced Accuracy : 0.50556         \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\n2.3 Apply F1 Score and Precision Recall\nLook at dataset and logistic regression analysis close with F1 score and Precision Recall\ndo F1 Score and Precision Recall\nPrecision - out of all the true strokes the model predicted, how many really were strokes? Consider 1 = 100%\n\n\nCode\nprecision &lt;- sum((predicted_class == 1) & (ForReal_Stroke == 1)) / sum(predicted_class == 1)\n\n\nRecall - out of all the actual strokes, how many did the model catch? = .01 or 1%\n\n\nCode\nrecall &lt;- sum((predicted_class == 1) & (ForReal_Stroke == 1)) / sum(ForReal_Stroke == 1)\n\n\nf1_Score - How well does this model predict strokes? = .022 or 2.2% –very poorly\n\n\nCode\nf1_score  &lt;- 2 * precision * recall / (precision + recall)\n\n\nprecision, recall, f1_score\n\n\nCode\nprecision\n\n\n[1] 1\n\n\nCode\nrecall\n\n\n[1] 0.01111111\n\n\nCode\nf1_score\n\n\n[1] 0.02197802"
  },
  {
    "objectID": "posts/renan-blog-post-week7/index.html#testing-logistic-regression-model-assumptions",
    "href": "posts/renan-blog-post-week7/index.html#testing-logistic-regression-model-assumptions",
    "title": "Reproducing Steve’s Code - Week 7",
    "section": "3. Testing logistic Regression Model Assumptions",
    "text": "3. Testing logistic Regression Model Assumptions\nThere are several assumptions for Logistic Regression: 1. The Dependent Variable is binary (i.e, 0 or 1) 2. There is a linear relationship between th logit of the outcome and each predictor 3. There are NO high leverage outliers in the predictors 4. There is No high multicollinearity (ie strong correlations) between predictors\n\n\nCode\nLR_stroke2 &lt;- stroke1_clean\nmodel2 &lt;- glm(stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, data=LR_stroke2, family = binomial)\nsummary(model2)\n\n\n\nCall:\nglm(formula = stroke ~ gender + age + hypertension + heart_disease + \n    ever_married + work_type + Residence_type + avg_glucose_level + \n    bmi + smoking_status, family = binomial, data = LR_stroke2)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -8.426854   0.873243  -9.650  &lt; 2e-16 ***\ngender             0.080370   0.167274   0.480 0.630893    \nage                0.070967   0.006845  10.368  &lt; 2e-16 ***\nhypertension       0.570797   0.182580   3.126 0.001770 ** \nheart_disease      0.417884   0.220311   1.897 0.057856 .  \never_married       0.174316   0.261832   0.666 0.505569    \nwork_type         -0.109615   0.126101  -0.869 0.384703    \nResidence_type     0.005932   0.162188   0.037 0.970822    \navg_glucose_level  0.004658   0.001375   3.388 0.000704 ***\nbmi                0.006275   0.012875   0.487 0.625954    \nsmoking_status     0.179921   0.106431   1.691 0.090932 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1403.5  on 3356  degrees of freedom\nResidual deviance: 1145.4  on 3346  degrees of freedom\nAIC: 1167.4\n\nNumber of Fisher Scoring iterations: 7\n\n\n\n3.1 Testing Assumption 1\nTesting Assumption 1: The Dependent Variable is binary (0 or 1)\n\n\nCode\nunique(LR_stroke2$stroke)\n\n\n[1] 1 0\n\n\n\n\n3.2 Testing Assumption 2\nTesting Assumption 2: There is a linear relationship between the outcome variable and each predictor (use boxTidwell)\nFor boxTidwell, first adjust all predictors so all values are positive. If we obtain a P value greater than 0.05 it indicates a linear relationship between the predictor and the outcome.\n\n\nCode\nLR_stroke2$genderadj            &lt;- LR_stroke2$gender            + abs(min(LR_stroke1$gender))            + 1\nLR_stroke2$ageadj               &lt;- LR_stroke2$age               + abs(min(LR_stroke1$age))               + 1\nLR_stroke2$hypertensionadj      &lt;- LR_stroke2$hypertension      + abs(min(LR_stroke1$hypertension))      + 1\nLR_stroke2$heart_diseaseadj     &lt;- LR_stroke2$heart_disease     + abs(min(LR_stroke1$hypertension))      + 1\nLR_stroke2$ever_marriedadj      &lt;- LR_stroke2$ever_married      + abs(min(LR_stroke1$ever_married))      + 1\nLR_stroke2$work_typeadj         &lt;- LR_stroke2$work_type         + abs(min(LR_stroke1$work_type))         + 1\nLR_stroke2$Residence_typeadj    &lt;- LR_stroke2$Residence_type    + abs(min(LR_stroke1$Residence_type))    + 1\nLR_stroke2$avg_glucose_leveladj &lt;- LR_stroke2$avg_glucose_level + abs(min(LR_stroke1$avg_glucose_level)) + 1\nLR_stroke2$bmiadj               &lt;- LR_stroke2$bmi               + abs(min(LR_stroke1$bmi))               + 1\nLR_stroke2$smoking_statusadj    &lt;- LR_stroke2$smoking_status    + abs(min(LR_stroke1$smoking_status))    + 1\n\n\nError in linearHypothesis.lm(mod.2, H) : there are aliased coefficients in the model.\n\n\nCode\n# boxTidwell(stroke ~ genderadj + ageadj + hypertensionadj + heart_diseaseadj + ever_marriedadj + work_typeadj + Residence_typeadj + avg_glucose_leveladj + bmiadj + smoking_statusadj, data=LR_stroke2)\n\n\n\n3.2.1 Issues Testing Assumption 2\nTrying to Drop Aliased Predictors\n\n\nCode\n# First, create the linear model object\nlm_model &lt;- lm(stroke ~ genderadj + ageadj + hypertensionadj + heart_diseaseadj + ever_marriedadj + work_typeadj + Residence_typeadj + avg_glucose_leveladj + bmiadj + smoking_statusadj, data=LR_stroke2)\n\n# Then, run the alias() function\nalias(lm_model)\n\n\nModel :\nstroke ~ genderadj + ageadj + hypertensionadj + heart_diseaseadj + \n    ever_marriedadj + work_typeadj + Residence_typeadj + avg_glucose_leveladj + \n    bmiadj + smoking_statusadj\n\n\nCode\n# Model : stroke ~ genderadj + ageadj + hypertensionadj + heart_diseaseadj + ever_marriedadj + work_typeadj + Residence_typeadj + avg_glucose_leveladj + bmiadj + smoking_statusadj\n\n\nYou can also check for perfect correlation and see if any correlations are ±1.\n\n\nCode\ncor(LR_stroke2[, c(\"genderadj\",\"ageadj\",\"hypertensionadj\",\"heart_diseaseadj\",\n                   \"ever_marriedadj\",\"work_typeadj\",\"Residence_typeadj\",\n                   \"avg_glucose_leveladj\",\"bmiadj\",\"smoking_statusadj\")])\n\n\n                       genderadj      ageadj hypertensionadj heart_diseaseadj\ngenderadj             1.00000000 -0.05599748    -0.040011423     -0.104191111\nageadj               -0.05599748  1.00000000     0.263123514      0.260353361\nhypertensionadj      -0.04001142  0.26312351     1.000000000      0.110020853\nheart_diseaseadj     -0.10419111  0.26035336     0.110020853      1.000000000\never_marriedadj       0.02728003 -0.48775310    -0.107114849     -0.069804764\nwork_typeadj          0.01495643  0.14214267     0.048358096      0.029618537\nResidence_typeadj    -0.01143732 -0.01761422     0.003112702     -0.010250014\navg_glucose_leveladj -0.07148597  0.23864619     0.168909926      0.143333385\nbmiadj               -0.01991382  0.04222590     0.127363138     -0.003962827\nsmoking_statusadj    -0.07723370  0.03463196    -0.005416806      0.060198195\n                     ever_marriedadj work_typeadj Residence_typeadj\ngenderadj                 0.02728003  0.014956427      -0.011437323\nageadj                   -0.48775310  0.142142673      -0.017614219\nhypertensionadj          -0.10711485  0.048358096       0.003112702\nheart_diseaseadj         -0.06980476  0.029618537      -0.010250014\never_marriedadj           1.00000000 -0.020663455       0.011239966\nwork_typeadj             -0.02066345  1.000000000      -0.007197831\nResidence_typeadj         0.01123997 -0.007197831       1.000000000\navg_glucose_leveladj     -0.11858810  0.034327248       0.008010375\nbmiadj                   -0.12527547 -0.017017707       0.009836168\nsmoking_statusadj        -0.05723324 -0.015271022      -0.039896247\n                     avg_glucose_leveladj       bmiadj smoking_statusadj\ngenderadj                    -0.071485971 -0.019913816      -0.077233702\nageadj                        0.238646187  0.042225902       0.034631959\nhypertensionadj               0.168909926  0.127363138      -0.005416806\nheart_diseaseadj              0.143333385 -0.003962827       0.060198195\never_marriedadj              -0.118588103 -0.125275472      -0.057233241\nwork_typeadj                  0.034327248 -0.017017707      -0.015271022\nResidence_typeadj             0.008010375  0.009836168      -0.039896247\navg_glucose_leveladj          1.000000000  0.155139559       0.005095349\nbmiadj                        0.155139559  1.000000000       0.029041449\nsmoking_statusadj             0.005095349  0.029041449       1.000000000\n\n\ncheck constant columns:\nIf any variable only has one unique value → it’s constant → alias.\n\n\nCode\nsapply(LR_stroke2, function(x) length(unique(x)))\n\n\n              gender                  age         hypertension \n                   2                   70                    2 \n       heart_disease         ever_married            work_type \n                   2                    2                    4 \n      Residence_type    avg_glucose_level                  bmi \n                   2                 2861                  364 \n      smoking_status               stroke            genderadj \n                   3                    2                    2 \n              ageadj      hypertensionadj     heart_diseaseadj \n                  70                    2                    2 \n     ever_marriedadj         work_typeadj    Residence_typeadj \n                   2                    4                    2 \navg_glucose_leveladj               bmiadj    smoking_statusadj \n                2861                  364                    3 \n\n\nError in linearHypothesis.lm(mod.2, H) : there are aliased coefficients in the model.\n\n\nCode\n# boxTidwell(stroke ~ genderadj + ageadj + hypertensionadj + heart_diseaseadj + ever_marriedadj + work_typeadj + Residence_typeadj + avg_glucose_leveladj + bmiadj + smoking_statusadj, data=LR_stroke2)\n\n\n\n\n\n3.3 Testing Assumption 3\nTesting Assumption 3: assess influential outliers using car package and influencePlot\n\n\nCode\ncar::influencePlot(model2)\n\n\n\n\n\n\n\n\n\n        StudRes          Hat       CookD\n83    2.6917353 0.0039267816 0.012237368\n84    1.5018344 0.0343771041 0.006641860\n87    3.0732709 0.0012042796 0.011476828\n131   3.1608870 0.0006013465 0.007697762\n152   3.1135601 0.0005613972 0.006237531\n2583 -0.8509399 0.0399302730 0.001668526\n\n\n\n\n3.4 Testing Assumption 4\nTesting Assumption 4 : Multicollinearity using ggplot and augment\n\n\nCode\n# Testing Assumption 4 : Multicollinearity using ggplot and augment#\naug &lt;- augment(model2)\nggplot(aug,aes(.fitted, .std.resid)) + geom_point() + geom_hline(yintercept=0)\n\n\n\n\n\n\n\n\n\n\n\n3.5 Conclusion of Testing Assumptions\nConclusion: Now that all 4 assumptions are met, logistic regression is a valid model to analyze the model"
  },
  {
    "objectID": "posts/renan-blog-post-week7/index.html#analysis-of-the-model",
    "href": "posts/renan-blog-post-week7/index.html#analysis-of-the-model",
    "title": "Reproducing Steve’s Code - Week 7",
    "section": "4 Analysis of the Model",
    "text": "4 Analysis of the Model\nPart 4: Analysis of the Model\n\n\nCode\nmodel2 &lt;- glm(stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, data=LR_stroke2, family = binomial)\nsummary(model2)\n\n\n\nCall:\nglm(formula = stroke ~ gender + age + hypertension + heart_disease + \n    ever_married + work_type + Residence_type + avg_glucose_level + \n    bmi + smoking_status, family = binomial, data = LR_stroke2)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -8.426854   0.873243  -9.650  &lt; 2e-16 ***\ngender             0.080370   0.167274   0.480 0.630893    \nage                0.070967   0.006845  10.368  &lt; 2e-16 ***\nhypertension       0.570797   0.182580   3.126 0.001770 ** \nheart_disease      0.417884   0.220311   1.897 0.057856 .  \never_married       0.174316   0.261832   0.666 0.505569    \nwork_type         -0.109615   0.126101  -0.869 0.384703    \nResidence_type     0.005932   0.162188   0.037 0.970822    \navg_glucose_level  0.004658   0.001375   3.388 0.000704 ***\nbmi                0.006275   0.012875   0.487 0.625954    \nsmoking_status     0.179921   0.106431   1.691 0.090932 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1403.5  on 3356  degrees of freedom\nResidual deviance: 1145.4  on 3346  degrees of freedom\nAIC: 1167.4\n\nNumber of Fisher Scoring iterations: 7\n\n\nConclusion: age, hypertension, heartdisease, and avg_glucose_level are statistically significant predictors on whether one has a stroke or not.\nall the P values of these 4 predictors is .05 or less (note included heart_disease because it approaches statistical significance at .057)\nSince this a logistic regression, we cant use R squared and adjusted R squared to see how well the model predicted stroke. So we substitute McFadden’s P value.\n\n\nCode\n# install.packages(\"pscl\")\n# library(pscl)\npR2(model2)\n\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-572.70320797 -701.73792863  258.06944131    0.18387879    0.07399442 \n         r2CU \n   0.21655630 \n\n\nComment McFadden = .18– not a bad fit for logistic regression\nCreate a confusion matrix (Type1 vs Type2 error in statistics)\n\n\nCode\n# install.packages(\"parallelly\")\n# install.packages(\"cli\")\n# library(caret)\n\n# Predict probabilities from the logistic regression model\npredicted_prob1 &lt;- predict(model2, type = \"response\")\n\n# Convert probabilities to binary classes using a 0.5 cutoff\npredicted_class1 &lt;- ifelse(predicted_prob1 &gt; 0.5, 1, 0)\n\npredicted_class1 &lt;- factor(predicted_class1, levels = c(0,1))\nForReal_Stroke1 &lt;- factor(LR_stroke2$stroke, levels = c(0,1))\nconfusionMatrix(predicted_class1, ForReal_Stroke1)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 3177  178\n         1    0    2\n                                          \n               Accuracy : 0.947           \n                 95% CI : (0.9388, 0.9543)\n    No Information Rate : 0.9464          \n    P-Value [Acc &gt; NIR] : 0.4587          \n                                          \n                  Kappa : 0.0208          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.00000         \n            Specificity : 0.01111         \n         Pos Pred Value : 0.94694         \n         Neg Pred Value : 1.00000         \n             Prevalence : 0.94638         \n         Detection Rate : 0.94638         \n   Detection Prevalence : 0.99940         \n      Balanced Accuracy : 0.50556         \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nAnalysis of the Confusion Matrix: Crud…poor results\nFurther Analysis of the Confusion Matrix with F1 score and Precision Recall\n\n4.1 F1 Score and Precision Recall\nPrecision - out of all the true strokes the model predicted, how many really were strokes? = 1 = 100%\n\n\nCode\nprecision1 &lt;- sum((predicted_class1 == 1) & (ForReal_Stroke1 == 1)) / sum(predicted_class1 == 1)\n\n\nRecall - out of all the actual strokes, how many did the model catch?\nResults = .01 or 1%—\n\n\nCode\nrecall1 &lt;- sum((predicted_class1 == 1) & (ForReal_Stroke1 == 1)) / sum(ForReal_Stroke1 == 1)\n\n\nf1_Score - How well does this model predict strokes?\nResults = .022 or 2.2% –very poorly\n\n\nCode\nf1_score1  &lt;- 2 * precision1 * recall1 / (precision1 + recall1)\n\n\nprecision1, recall1, f1_score1\n\n\nCode\nprecision1\n\n\n[1] 1\n\n\nCode\nrecall1\n\n\n[1] 0.01111111\n\n\nCode\nf1_score1\n\n\n[1] 0.02197802"
  },
  {
    "objectID": "posts/renan-blog-post-week7/index.html#conclusion",
    "href": "posts/renan-blog-post-week7/index.html#conclusion",
    "title": "Reproducing Steve’s Code - Week 7",
    "section": "Conclusion",
    "text": "Conclusion\n\nIdeas for improving precision, recall and f1_score\nAddress imbalance by upsample (add stroke cases), downsample (remove non strokecases) and or SMOTE (Synthetic data)\nChange the classification threshold\nCompare with Alternative Models such as random forrests or XGBoost\n\n\nReferences\n\n\n1. fedesoriano. (n.d.). Stroke Prediction Dataset. https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset"
  },
  {
    "objectID": "posts/kristina-blog-post-week2/index.html",
    "href": "posts/kristina-blog-post-week2/index.html",
    "title": "Literature Review Week 2",
    "section": "",
    "text": "Article title: Understanding logistic regression analysis[1]\nData used: Synthetic data about the effect of a drug treatment. There are two treatments: standard and new drug treatments. Shows a binary outcome, either died or survived after drug treatment.\nProblem: The problem introduced in this article is needing a method to study the joint relationship between two or more predictors and the target variable. One way to solve this problem is to calculate a weighted odds ratio that accounts for all the relationships and predictors. However, as the number of predictors increases, weighted odds ratio calculations can become very complicated. Also, these calculations require only categorical variables as input and no continuous variables may be used. A solution to this problem is to use logistic regression.\nSolution to problem: - Article explains what logistic regression is useful for. Advantages are that it can be used when there are more than two predictors and we want to analyze how they all simultaneously affect the target variable. Also useful for when we have any number of continuous predictors. - Gives the logistic regression model equation and explains the meaning of all variables (intercept, slops, and symbols). The outcome in the model (left hand side of equation) is the log of the odds. The paper goes into detail about how to interpret coefficients in the model; you must take the exponentials of the coefficients to understand the chances (the probability) of an event happening (the event in this paper is death).\nLimitations: - Differences between odds, odds ratios, and probabilities are discussed. Understanding the differences is key to interpreting results, and if you do not understand the differences, you cannot easily interpret the output of a logistic regression model. - If there is a predictor with more than two levels, you must create n-1 dummy variables for n number of categories within the predictor. This is considered a limitation because dataset manipulation must be done prior to constructing a model. - Interpreting coefficients of continuous variables is explained. It is different than interpreting coefficients of categorical variables. Interpreting these results can be complicated and should be done carefully. Exponential of the coefficient of a continuous variable is the chance of an event happening in relation to one unit of the continuous predictor. - Models with too many predictors can be too saturated, and researchers may miss associations. An association may be present, but the model will not have enough statistical power with too many predictors. Solution: build a model with less predictors. You can start with all predictors and drop one at a time, or start with 0 predictors and add one at a time (keeping only the most important predictors). Starting with a full model is better. - A way to test the importance of each variable is to create a univariate model for each individual predictor at a time to see which are most strongly correlated with the outcome. - How to choose the reference group is explained. Usually, the reference group is the lowest level or the highest level in a group of ordered categories. But if there is no order to the categories then there may be no clear reference group. Results vary when choosing differing reference groups.\nResult: The researchers conclude by stating that logistic regression is a very powerful and useful way to analyze epidemiologic data."
  },
  {
    "objectID": "posts/kristina-blog-post-week2/index.html#article-1",
    "href": "posts/kristina-blog-post-week2/index.html#article-1",
    "title": "Literature Review Week 2",
    "section": "",
    "text": "Article title: Understanding logistic regression analysis[1]\nData used: Synthetic data about the effect of a drug treatment. There are two treatments: standard and new drug treatments. Shows a binary outcome, either died or survived after drug treatment.\nProblem: The problem introduced in this article is needing a method to study the joint relationship between two or more predictors and the target variable. One way to solve this problem is to calculate a weighted odds ratio that accounts for all the relationships and predictors. However, as the number of predictors increases, weighted odds ratio calculations can become very complicated. Also, these calculations require only categorical variables as input and no continuous variables may be used. A solution to this problem is to use logistic regression.\nSolution to problem: - Article explains what logistic regression is useful for. Advantages are that it can be used when there are more than two predictors and we want to analyze how they all simultaneously affect the target variable. Also useful for when we have any number of continuous predictors. - Gives the logistic regression model equation and explains the meaning of all variables (intercept, slops, and symbols). The outcome in the model (left hand side of equation) is the log of the odds. The paper goes into detail about how to interpret coefficients in the model; you must take the exponentials of the coefficients to understand the chances (the probability) of an event happening (the event in this paper is death).\nLimitations: - Differences between odds, odds ratios, and probabilities are discussed. Understanding the differences is key to interpreting results, and if you do not understand the differences, you cannot easily interpret the output of a logistic regression model. - If there is a predictor with more than two levels, you must create n-1 dummy variables for n number of categories within the predictor. This is considered a limitation because dataset manipulation must be done prior to constructing a model. - Interpreting coefficients of continuous variables is explained. It is different than interpreting coefficients of categorical variables. Interpreting these results can be complicated and should be done carefully. Exponential of the coefficient of a continuous variable is the chance of an event happening in relation to one unit of the continuous predictor. - Models with too many predictors can be too saturated, and researchers may miss associations. An association may be present, but the model will not have enough statistical power with too many predictors. Solution: build a model with less predictors. You can start with all predictors and drop one at a time, or start with 0 predictors and add one at a time (keeping only the most important predictors). Starting with a full model is better. - A way to test the importance of each variable is to create a univariate model for each individual predictor at a time to see which are most strongly correlated with the outcome. - How to choose the reference group is explained. Usually, the reference group is the lowest level or the highest level in a group of ordered categories. But if there is no order to the categories then there may be no clear reference group. Results vary when choosing differing reference groups.\nResult: The researchers conclude by stating that logistic regression is a very powerful and useful way to analyze epidemiologic data."
  },
  {
    "objectID": "posts/kristina-blog-post-week2/index.html#article-2",
    "href": "posts/kristina-blog-post-week2/index.html#article-2",
    "title": "Literature Review Week 2",
    "section": "Article 2",
    "text": "Article 2\nArticle used: Binary logistic regression analysis of factors affecting urban road traffic safety.[2]\nProblem: The introduction features a literature review establishing relevance of the topic. Several studies about traffic accidents are discussed and cited. Traffic accidents are becoming more common as traffic increases due to population increases. Researchers aim to find which factors in traffic are more closely associated with the occurrence of traffic accidents.\nSolution: The researchers use a binary logistic regression model to study which factors are more correlated to the occurrence of traffic accidents. The advantages of a binary logistic regression are discussed. Logistic regression allows researchers to make predictions about probability of a dependent variable being sorted into a certain class. Logistic regression also allows for the researchers to determine which predictors more significantly impact the outcome variable. To set up the study, researchers defined the dependent variable, y, as a binary outcome of either no accident (value of 0) or presence of an accident (value of 1). The independent variables are defined as 25 factors that are grouped under four categories consisting of environmental factors, driver attributes, road attributes, and vehicle factors. Before analyzing the data, it was preprocessed to eliminate outliers, normalize all predictor values to similar scales, eliminate redundancy, Then, the predictors were run through a multicollinearity test to determine if any needed to be excluded from the model; none of the factors showed significant multicollinearity and all 25 were kept in the model. The calculation used for collinearity involved the correlation coefficient, R, tolerance (T), and variance inflation factor (V).\nResults: A binary logistic regression model was fitted to the data, and it was found that the model fit well. To assess the goodness of fit, the determination coefficient, R^2, value was calculated. The strongest predictors of a traffic incident were found to be driver behavior, weather, road conditions, and lighting.\nLimitations: The paper states that research in this area can be improved by using real- time data about weather, road conditions, and driver status. Using data as it occurs in real time may help make better predictions about traffic safety risks.\nDataset: The original data was sourced from the International Transport Forum with 5350 datapoints. After data preprocessing, the data was reduced to 3500 data points.\n\nReferences\n\n\n1. Sperandei, S. (2014). Understanding logistic regression analysis. Biochemia Medica, 24(1), 12–18.\n\n\n2. Chen, Y., You, P., & Chang, Z. (2024). Binary logistic regression analysis of factors affecting urban road traffic safety. Advances in Transportation Studies, 3."
  },
  {
    "objectID": "posts/kristina-blog-post-week3/index.html",
    "href": "posts/kristina-blog-post-week3/index.html",
    "title": "Literature Review Week 3",
    "section": "",
    "text": "Title: Determinants of coexistence of undernutrition and anemia among under- five children in Rwanda; evidence from 2019/20 demographic health survey: Application of bivariate binary logistic regression model.[1]\nAuthors: Abebew Aklog smare, Yitateku Adugna Agmas\nProblem: - In children under five years of age, malnutrition and anemia have been an ongoing problem in many African countries. This paper focuses on studying malnutrition and anemia in Rwanda in particular; Rwanda went through a civil war, and after the war, rates of malnutrition and anemia decreased as the country was rebuilt. However, different parts of the country experienced faster or slower rates of improvement of malnutrition and anemia after the war, and it is unclear which factors are associated with improved rates of malnutrition and anemia. This study aims to identify key predictors correlated with these health ailments, so that the areas of the country still suffering from high rates of these health problems can be given the correct types of aid to fix the problem of malnutrition and anemia. - The introduction of the paper cites a few studies that identify strong correlations with malnutrition and anemia in children under five. Some strong predictors found by other studies are the child’s age, parents’ education level, household economic class, geographic location, household food availability, child birth size, family size, maternal age, and more. - The introduction states that this topic is relevant because although there is already existing research on malnutrition and anemia in African countries, there is not much literature studying the relationship between the two health conditions. This study aims to analyze the relationship between malnutrition and anemia in children in addition to identifying strong predictors of the conditions.\nData: - Data is supplied by the 2019/20 Rwanda Demographic and Health Survey. The researchers obtained the samples themselves, and the sampling method is described in depth. The researchers chose 500 clusters from different areas all over the country, and then households were selected at random from these clusters to be surveyed. The resulting data consisted of 3205 data points consisting of data about children under the age of 5.\nSolution to problem: - Researchers used a bivariate binary logistic regression model. This model helps understand the relationship between the outcome variables, presence of malnutrition and presence of anemia. The outcome variables are both binary, taking on a value of 1 for present, or 0 for not present. The predictors consist of about 26 variables relating to the child’s health conditions, family information, details about the parents, and relevant geographic information. - Three models are presented. The first model is the bivariate binary logistic regression model. The second model is the equivalent, but it is in the form of the log odds. The third model discussed is the odds ratio, and it is used to assess the relationship between categorical predictors in the model. - The researchers used SPSS and R software to perform the analyses.\nResults: - Results shows that nearly half of the study participants had anemia and about one fifth had malnutrition. - Six significant predictors were found: mother’s age, drinking water, other children in household, child gender, birth order, and gender of household head. - The odds ratio was a value that was not 1, which indicates that the outcome variables are not statistically independent. The relationship that exists between the outcome variables is significant. - The goodness fit test used was a proportion of correct predictions to the number of observations. This result was about 89%, so the researchers conclude that the model was a good fit. - There is a discussion about possible causes of the significant relationship that was found between malnutrition and anemia in children under 5. Researchers cite other facts and figures about why these health conditions are strongly correlated.\nConclusion: - Increasing maternal education, supplementing with vitamin A and other nutrient dense foods, providing a healthy/ clean/ safe environment, and decreasing maternal anemia may help improve rates of malnutrition and anemia in children.\nLimitations of the study: - The only limitation discussed is that the data collected may be prone to errors. This means researchers can conclude there are strong correlations, but it cannot be stated that any of the relationships are causal."
  },
  {
    "objectID": "posts/kristina-blog-post-week3/index.html#article-1",
    "href": "posts/kristina-blog-post-week3/index.html#article-1",
    "title": "Literature Review Week 3",
    "section": "",
    "text": "Title: Determinants of coexistence of undernutrition and anemia among under- five children in Rwanda; evidence from 2019/20 demographic health survey: Application of bivariate binary logistic regression model.[1]\nAuthors: Abebew Aklog smare, Yitateku Adugna Agmas\nProblem: - In children under five years of age, malnutrition and anemia have been an ongoing problem in many African countries. This paper focuses on studying malnutrition and anemia in Rwanda in particular; Rwanda went through a civil war, and after the war, rates of malnutrition and anemia decreased as the country was rebuilt. However, different parts of the country experienced faster or slower rates of improvement of malnutrition and anemia after the war, and it is unclear which factors are associated with improved rates of malnutrition and anemia. This study aims to identify key predictors correlated with these health ailments, so that the areas of the country still suffering from high rates of these health problems can be given the correct types of aid to fix the problem of malnutrition and anemia. - The introduction of the paper cites a few studies that identify strong correlations with malnutrition and anemia in children under five. Some strong predictors found by other studies are the child’s age, parents’ education level, household economic class, geographic location, household food availability, child birth size, family size, maternal age, and more. - The introduction states that this topic is relevant because although there is already existing research on malnutrition and anemia in African countries, there is not much literature studying the relationship between the two health conditions. This study aims to analyze the relationship between malnutrition and anemia in children in addition to identifying strong predictors of the conditions.\nData: - Data is supplied by the 2019/20 Rwanda Demographic and Health Survey. The researchers obtained the samples themselves, and the sampling method is described in depth. The researchers chose 500 clusters from different areas all over the country, and then households were selected at random from these clusters to be surveyed. The resulting data consisted of 3205 data points consisting of data about children under the age of 5.\nSolution to problem: - Researchers used a bivariate binary logistic regression model. This model helps understand the relationship between the outcome variables, presence of malnutrition and presence of anemia. The outcome variables are both binary, taking on a value of 1 for present, or 0 for not present. The predictors consist of about 26 variables relating to the child’s health conditions, family information, details about the parents, and relevant geographic information. - Three models are presented. The first model is the bivariate binary logistic regression model. The second model is the equivalent, but it is in the form of the log odds. The third model discussed is the odds ratio, and it is used to assess the relationship between categorical predictors in the model. - The researchers used SPSS and R software to perform the analyses.\nResults: - Results shows that nearly half of the study participants had anemia and about one fifth had malnutrition. - Six significant predictors were found: mother’s age, drinking water, other children in household, child gender, birth order, and gender of household head. - The odds ratio was a value that was not 1, which indicates that the outcome variables are not statistically independent. The relationship that exists between the outcome variables is significant. - The goodness fit test used was a proportion of correct predictions to the number of observations. This result was about 89%, so the researchers conclude that the model was a good fit. - There is a discussion about possible causes of the significant relationship that was found between malnutrition and anemia in children under 5. Researchers cite other facts and figures about why these health conditions are strongly correlated.\nConclusion: - Increasing maternal education, supplementing with vitamin A and other nutrient dense foods, providing a healthy/ clean/ safe environment, and decreasing maternal anemia may help improve rates of malnutrition and anemia in children.\nLimitations of the study: - The only limitation discussed is that the data collected may be prone to errors. This means researchers can conclude there are strong correlations, but it cannot be stated that any of the relationships are causal."
  },
  {
    "objectID": "posts/kristina-blog-post-week3/index.html#article-2",
    "href": "posts/kristina-blog-post-week3/index.html#article-2",
    "title": "Literature Review Week 3",
    "section": "Article 2",
    "text": "Article 2\nArticle Title: Using Binary logistic Regression to Detect Health Insurance Fraud.[2]\nAuthor: Baraah Samara, Ph.D. Student\nProblem: - Insurance fraud in health insurance industry. Specifically, patients treated at private clinics or hospitals. - Intro of the article explains why this topic is relevant. Between 1965 and 2008, the cost of healthcare increased significantly, and as a result, health insurance fraud has increased. There needs to be effective tools at detecting this fraud. - If fraud is decreased, it will help the economy as a whole, it will help insurance companies, and it will lower premium payments made by customers. - Fraud is committed by three types of entities: consumer, provider, and payer fraud. - Literature review cites common predictors of health insurance fraud: diagnoses, service cost, number of claims from individual, greatest costing claim, probability of anomaly, excessive charges by care facilities, and more.\nData: - Original dataset contained 26 independent variables - Data was collected from a time span of January 2022 through November 2022 - about 123,000 data points with no missing values. - The predictors are of varying types, including numerical, categorical, and binary values - The dependent variable is fraud, with a value of 1 for fraud present, or 0 for no fraud present.\nSolution to Problem:\nWhy they selected this model: - Building a binary logistic regression model to detect health insurance fraud - logistic regression is selected as the analytic technique because they want to assess effects of categorical variables on a categorical dependent variable. They also cite that logistic regression is the most accurate type of regression model with the kind of classification they are performing in this study. - Fraud detection commonly employs binary prediction models - The logistic regression model also provides estimates between 0 and 1, which help investigators estimate probability of fraud - Researchers provide the equation they use to calculate the log odds of the event of interest (occurrence of fraud)\nThe method: - They calculate likelihood of an individual committing fraud - They calculate total cost accrued by an individual. Then they perform the logistic regression using this calculation - The model works by identifying outliers and classifies them as potential fraudulent activity - Before testing the model, researchers hypothesize that there will be a positive relationship between overall cost accrued by patient and likelihood of fraud. Costs include doctor visit costs, prescription drug costs, lab costs, costs of medical symptoms, and total cost of expensive prescriptions. - When running the model, none of the coefficients were zero, which means that there exists a significant relationship between the outcome and the predictor variables. - Predictors were tested for multicollinearity before the model was run. Pearson correlation coefficients were obtained, and any predictors with a Pearson value of greater than 0.8 were excluded from the model. Only eight predictors remained after removing the predictors that were strongly correlated. - Different models were constructed using only the most important predictors. When taking away the least important predictor, the log likelihood was calculated to assess the accuracy of the model. The best performing model contained six of the original predictors.\nResults: - Six predictors were found to be significant in predicting health insurance fraud. The predictors are office visit cost, prescription costs, lab costs, symptom cost, and two expensive prescription drug costs. - There is a thorough interpretation of model slopes. For example, “the likelihood of fraud increases by .005188 for every unit increase in pharmacy cost.” Interpretations for the most significant predictors are given in this way. - A Chi-Square test for independence was used to determine whether at least one predictor was significantly related to the outcome. It was concluded that at least one of the six predictors was significant. - The model was found to be about 99% accurate when predicting no fraud, but only about 76% accurate when predicting fraud. - An example is included that shows how to calculate the probability that an individual will commit insurance fraud, given values for the six predictors in the equation. - The study concludes by stating the importance and relevance of continuing to develop new fraud detection models.\nLimitations: - No limitations are explicitly stated in this paper. However, it can be considered a limitation that only data from middle eastern countries was used in the study. To make the results of the study more generalizable, data from other regions of the world should be included in a more comprehensive study.\n\nReferences\n\n\n1. Asmare, A. A., & Agmas, Y. A. (2024). Determinants of coexistence of undernutrition and anemia among under-five children in rwanda; evidence from 2019/20 demographic health survey: Application of bivariate binary logistic regression model. Plos One, 19(4), e0290111.\n\n\n2. Samara, B. (2024). Using binary logistic regression to detect health insurance fraud. Pakistan Journal of Life & Social Sciences, 22(2)."
  },
  {
    "objectID": "posts/kristina-blog-post-week4/index.html",
    "href": "posts/kristina-blog-post-week4/index.html",
    "title": "Literature Review Week 4",
    "section": "",
    "text": "Article Title: Exploring the medical decision-making patterns and influencing factors among the general Chinese public: a binary logistic regression analysis.[1]\nAuthors: Yuwen Lyu, Qian Xu, Junrong Liu\nProblem: - Researchers are seeking to understand top driving factors behind decisions made about healthcare and medical issues. The population of interest is the general Chinese public. - Previous research in this field has identified two main types of medical decision making: unilateral and collaborative decision making - Unilateral decision making means there is one main entity making the medical decision, such as a single patient, a patient’s family, or a doctor. Previous research shows that patient families have a very strong influence over a patient’s medical decisions. - Collaborative decision making means there are two or more parties involved in the decision making process. Three subgroups are defined: doctor group, doctor- patient group, patient- family group, and patient-doctor-family group. - There is a lack in research in this field. More needs to be known about factors that play a role in medical decision making processes. This study’s results will be generalizable to China as well as nations around the world.\nSolution: - Use binary logistic regression to classify points into two categories: unilateral decision making (value of 1), or collaborative decision making (value of 0) - This model is ideal because it takes into account variable interactions. Also used often in the medical field - The equation of the model is given. It is in the form of the log odds of the desired event happening.\nData: - 2696 data points with attributes including age, education, occupation, family situation, religion, economic status and medical payment methods - Data was collected via survey and included only residents of China from 31 provinces - The data was gathered by the researchers that wrote this study - A power analysis was conducted to determine how many data points would be needed in order to have reliable results after statistical analysis. A G-power test showed that only 2040 valid data points were needed\nResults: - Survey results showed that 30% of responses were categorized as unilateral decision making, while 70% were categorized as collaborative decision making. The top category of unilateral decision making was doctor- led decisions, while the top category for collaborative decision making was patient- doctor- family decisions. - Significant predictors were identified with p-values less than 0.05. Significant predictors of unilateral decision making were gender, education level, family status, and religious beliefs. Different occupations also significantly predicted unilateral decision making. - Odds ratios are given for some predictors, with researchers stating that certain categories of specific predictors are x.xx times more likely than the reference group to be a unilateral decision maker. - The significance of the regression model’s intercept is interpreted, and it is significant. This means when all variables are at their reference levels, there is a low likelihood of the outcome variable taking on a value of a unilateral decision making process. - The goodness of fit test used in this study is McFadden’s R-squared value. The value was0.065, and researchers state that this value shows a good fit of the model. It is explained that R squared values for studies in the social sciences are rarely ever close to a perfect fit.\nConclusions: - The researchers discuss why there are contrasting results from this study versus studies in Western countries. They identify different cultural values in different geographic regions, which ultimately lead to different medical decision making processes. - Results are discussed more in depth, with researchers attempting to identify causes behind the correlations that were identified.\nLimitations: - The study’s data is solely from China, so results may not be generalizable to global populations. - The binary logistic regression model may not be complex enough to account for the complexities of all the predictors involved in healthcare decision making. Researchers suggest using more complex models in future research."
  },
  {
    "objectID": "posts/kristina-blog-post-week4/index.html#article-1",
    "href": "posts/kristina-blog-post-week4/index.html#article-1",
    "title": "Literature Review Week 4",
    "section": "",
    "text": "Article Title: Exploring the medical decision-making patterns and influencing factors among the general Chinese public: a binary logistic regression analysis.[1]\nAuthors: Yuwen Lyu, Qian Xu, Junrong Liu\nProblem: - Researchers are seeking to understand top driving factors behind decisions made about healthcare and medical issues. The population of interest is the general Chinese public. - Previous research in this field has identified two main types of medical decision making: unilateral and collaborative decision making - Unilateral decision making means there is one main entity making the medical decision, such as a single patient, a patient’s family, or a doctor. Previous research shows that patient families have a very strong influence over a patient’s medical decisions. - Collaborative decision making means there are two or more parties involved in the decision making process. Three subgroups are defined: doctor group, doctor- patient group, patient- family group, and patient-doctor-family group. - There is a lack in research in this field. More needs to be known about factors that play a role in medical decision making processes. This study’s results will be generalizable to China as well as nations around the world.\nSolution: - Use binary logistic regression to classify points into two categories: unilateral decision making (value of 1), or collaborative decision making (value of 0) - This model is ideal because it takes into account variable interactions. Also used often in the medical field - The equation of the model is given. It is in the form of the log odds of the desired event happening.\nData: - 2696 data points with attributes including age, education, occupation, family situation, religion, economic status and medical payment methods - Data was collected via survey and included only residents of China from 31 provinces - The data was gathered by the researchers that wrote this study - A power analysis was conducted to determine how many data points would be needed in order to have reliable results after statistical analysis. A G-power test showed that only 2040 valid data points were needed\nResults: - Survey results showed that 30% of responses were categorized as unilateral decision making, while 70% were categorized as collaborative decision making. The top category of unilateral decision making was doctor- led decisions, while the top category for collaborative decision making was patient- doctor- family decisions. - Significant predictors were identified with p-values less than 0.05. Significant predictors of unilateral decision making were gender, education level, family status, and religious beliefs. Different occupations also significantly predicted unilateral decision making. - Odds ratios are given for some predictors, with researchers stating that certain categories of specific predictors are x.xx times more likely than the reference group to be a unilateral decision maker. - The significance of the regression model’s intercept is interpreted, and it is significant. This means when all variables are at their reference levels, there is a low likelihood of the outcome variable taking on a value of a unilateral decision making process. - The goodness of fit test used in this study is McFadden’s R-squared value. The value was0.065, and researchers state that this value shows a good fit of the model. It is explained that R squared values for studies in the social sciences are rarely ever close to a perfect fit.\nConclusions: - The researchers discuss why there are contrasting results from this study versus studies in Western countries. They identify different cultural values in different geographic regions, which ultimately lead to different medical decision making processes. - Results are discussed more in depth, with researchers attempting to identify causes behind the correlations that were identified.\nLimitations: - The study’s data is solely from China, so results may not be generalizable to global populations. - The binary logistic regression model may not be complex enough to account for the complexities of all the predictors involved in healthcare decision making. Researchers suggest using more complex models in future research."
  },
  {
    "objectID": "posts/kristina-blog-post-week4/index.html#article-2",
    "href": "posts/kristina-blog-post-week4/index.html#article-2",
    "title": "Literature Review Week 4",
    "section": "Article 2",
    "text": "Article 2\nArticle Title: Predictors of hospital admission when presenting with acute on chronic breathlessness: Binary logistic regression.[2]\nAuthors: Ann Hutchinson, Alastair Pickering, Paul Williams, Miriam Johnson\nProblem: - People presenting to the emergency room with breathlessness often do not require hospitalization and can be sent home. Only an average of 50% to 67% of these patients require admittance to the hospital. This research focuses on patients presenting with breathlessness to the ER and seeks to identify significant predictors of hospitalization of these individuals. Doctors and emergency department staff would benefit from identifying predictors present in individuals that would need to be admitted to the hospital.\nSolution: - Researchers used a binary logistic regression analysis to identify the predictors most strongly correlated with patients with breathlessness being admitted to the hospital from the ER. - First, in order to identify which predictors to include in the binary logistic regression, researchers analyzed 48 total predictors individually. They conducted a separate univariate analysis for each variable to see which were most significantly correlated with hospitalization from the ER. Only seven predictors were significant, and of those, only five were included in the final model (due to eliminating variables with strong multicollinearity).\nData: - Data was collected from a single hospital from December 2015 to May 2015. - Only 171 datapoints are included, as only 171 patients with acute breathlessness consented to have their survey used for research - Predictors included: demographics, preexisting medical conditions, severity of breathlessness, and other vital signs - To determine which predictors were most important, researchers used a stepwise backward elimination process, and excluded one predictor at a time. It was determined that only five predictors were needed. - After univariate analysis, researchers constructed a binary logistic regression model incorporating all of the selected predictors.\nResults and Conclusions: - Results are presented with an odds ratio for every predictor in the model. The odds of being admitted to the hospital increased by a certain factor for every one unit increase in a specific predictor. - The odds of admission to the hospital were positively correlated with age, talking to a doctor about symptoms, and the presence of preexisting heart conditions. The odds of being admitted to the hospital were negatively associated with blood oxygen levels. - The researchers state that this study is meant to only be exploratory and the results should not be used for making future predictions. Rather, the results should be used to aid in identifying strong predictors so these variables can be included in future similar studies. - Results of this study are compared to results of other studies, and the findings are consistent. A consistent predictor of hospital admission from this study and other studies includes older age. Another common predictor is tachycardia, but this study did not include this information.\nLimitations: - The study’s data is very limited. The data is supplied from just one hospital, and there were only 171 data points analyzed. The results of the study are therefore not as generalizable to global populations as a study analyzing broader populations. - The data also did not include responses from patients with very severe breathlessness because their state of health was too severe for them to be able to complete a survey. So the results of this study do not reflect patients with extreme symptoms.\n\nReferences\n\n\n1. Lyu, Y., Xu, Q., & Liu, J. (2024). Exploring the medical decision-making patterns and influencing factors among the general chinese public: A binary logistic regression analysis. BMC Public Health, 24(1), 887.\n\n\n2. Hutchinson, A., Pickering, A., Williams, P., & Johnson, M. (2023). Predictors of hospital admission when presenting with acute-on-chronic breathlessness: Binary logistic regression. PLoS One, 18(8), e0289263."
  },
  {
    "objectID": "posts/shree-blog-post-week3/index.html",
    "href": "posts/shree-blog-post-week3/index.html",
    "title": "Literature Review Week 3",
    "section": "",
    "text": "Modeling Road Accident Severity with Logistic Regression (comparison study)[1]\nLink: https://www.mdpi.com/2078-2489/11/5/270\nGoal: the goial was to understand any major outcomes that would happen (deaths or serious injuries) or small misfortunes, such as property damage and light casualties,. this is exciting research as transportation planners, governments, and law enforcement can make focused safety policies, like stricter enforcement, better road design, or public awareness campaigns, by knowing what factors affect how bad an accident is likely to be, like drunk driving, weather, and time of day.\nmethods used and approach\ndataset: Fatality Analysis Reporting System (FARS) predators: Driver demographics (age, gender) Environmental factors (weather, road condition, time of day, lighting) Driving behaviors (speeding, alcohol involvement, seat belt use) Vehicle types (motorcycles, trucks, cars)\nto distinguish between severe and non-severe accidents, logistic regression was used. To assess trade-offs between interpretability and predictive capability, models were contrasted with gradient boosting machines (GBMs) and decision trees. Performance was compared using metrics like accuracy, precision, recall, and AUC. Clear, comprehensible patterns were found using logistic regression: Severity was greatly worsened by low lighting, inclement weather, and night driving. Two of the best indicators of fatal collisions were speeding and alcohol use. Compared to other vehicles, motorcycles posed a disproportionately high severity risk. The prediction accuracy of tree-based models (GBM) was somewhat greater, but the results of logistic regression were clear and understandable.\nthere were some bad aspect of the appracoach or lets say disadvantage of technique used as: nonlinear interactions are not captured: for example, bad weather and night driving together may have a worse effect than additive driving, but conventional LR ignores this. data imbalance- Without corrections, logistic regression may become skewed toward the majority class; severe crashes are less common than non-severe ones."
  },
  {
    "objectID": "posts/shree-blog-post-week3/index.html#article-1",
    "href": "posts/shree-blog-post-week3/index.html#article-1",
    "title": "Literature Review Week 3",
    "section": "",
    "text": "Modeling Road Accident Severity with Logistic Regression (comparison study)[1]\nLink: https://www.mdpi.com/2078-2489/11/5/270\nGoal: the goial was to understand any major outcomes that would happen (deaths or serious injuries) or small misfortunes, such as property damage and light casualties,. this is exciting research as transportation planners, governments, and law enforcement can make focused safety policies, like stricter enforcement, better road design, or public awareness campaigns, by knowing what factors affect how bad an accident is likely to be, like drunk driving, weather, and time of day.\nmethods used and approach\ndataset: Fatality Analysis Reporting System (FARS) predators: Driver demographics (age, gender) Environmental factors (weather, road condition, time of day, lighting) Driving behaviors (speeding, alcohol involvement, seat belt use) Vehicle types (motorcycles, trucks, cars)\nto distinguish between severe and non-severe accidents, logistic regression was used. To assess trade-offs between interpretability and predictive capability, models were contrasted with gradient boosting machines (GBMs) and decision trees. Performance was compared using metrics like accuracy, precision, recall, and AUC. Clear, comprehensible patterns were found using logistic regression: Severity was greatly worsened by low lighting, inclement weather, and night driving. Two of the best indicators of fatal collisions were speeding and alcohol use. Compared to other vehicles, motorcycles posed a disproportionately high severity risk. The prediction accuracy of tree-based models (GBM) was somewhat greater, but the results of logistic regression were clear and understandable.\nthere were some bad aspect of the appracoach or lets say disadvantage of technique used as: nonlinear interactions are not captured: for example, bad weather and night driving together may have a worse effect than additive driving, but conventional LR ignores this. data imbalance- Without corrections, logistic regression may become skewed toward the majority class; severe crashes are less common than non-severe ones."
  },
  {
    "objectID": "posts/shree-blog-post-week3/index.html#article-2",
    "href": "posts/shree-blog-post-week3/index.html#article-2",
    "title": "Literature Review Week 3",
    "section": "Article 2",
    "text": "Article 2\nPredicting Uber Demand Using Spatio-Temporal Features and Logistic Regression (2017)[2]\nLink: https://escholarship.org/content/qt80q5f8t9/qt80q5f8t9_noSplash_59a1830fd88a360df43b9c6aff1446c7.pdf\nGoal: to forecast if Uber demand would outpace supply at a specific place and time window (for example, fifteen minutes in advance), resulting in an increase in pricing. Classifying each region for price changewas the classification challenge.\nMethodology:\nWhen demand exceeds supply within a zone or period, labels for “surge” events are created. surge/no surge classification using a logistic regression model. compared the effectiveness of Random Forests with Support Vector Machines (SVMs). evaluated on training/test splits using cross-validation.\nResult:\nAs a baseline model, logistic regression did fairly well, identifying significant demand trends such as 1. busy hours in the morning and evening. 2. Manhattan’s nightlife during weekends. 3. weather peaks (demand was greatly raised by rain).\nHowever, logistic regression was marginally outperformed by more sophisticated models (random forest, SVM), particularly when it came to capturing nonlinearities and interactions.\nThe good aspect of research :\nInterpretability: Coefficients showed which characteristics influenced demand (for example, rainfall significantly raised the likelihood of a surge). Low computational cost: LR trained quickly on a sizable NYC dataset, in contrast to more intricate models. Scalability: As an early warning system, a straightforward model may be quickly implemented for real-time demand forecasts.\nLimitiation or what the analysic could not get right:\nGeographic restrictions: Patterns may not transfer to smaller cities or suburban settings because they were trained in New York City. Limitation of binary classification: The model merely forecasted a surge or no surge; however, actual demand is continuous. More good models forecast the magnitude of the surge. Poor performance compared to more sophisticated models: Random forest produced higher accuracy by better capturing interactions.\n\nReferences\n\n\n1. Chen, M.-M., & Chen, M.-C. (2020). Modeling road accident severity with comparisons of logistic regression, decision tree and random forest. Information, 11(5), 270.\n\n\n2. Faghih, S., Safikhani, A., Moghimi, B., & Kamga, C. (2019). Predicting short-term uber demand in new york city using spatiotemporal modeling. Journal of Computing in Civil Engineering, 33(3), 05019002."
  },
  {
    "objectID": "posts/renan-blog-post-week6/Readme.html",
    "href": "posts/renan-blog-post-week6/Readme.html",
    "title": "Notes on project Week 6",
    "section": "",
    "text": "Missing the Features Correlation Matrix, it is on the Page 6 of the paper.\nFigure 4.  Features correlation heatmap for the dataset. Color intensity indicates the strength and direction of correlations, aiding in the identification of potential patterns and dependencies in the data.\n\n\n\nMissing the Sparsity Matrix, it is on the Page 6 of the paper.\nFigure 5.  Sparsity matrix for the dataset. The empty spaces found in the corresponding column signify the presence of missing data values for the specific feature.\nFrom the section 3.1 Visualizing Key Features"
  },
  {
    "objectID": "posts/renan-blog-post-week6/Readme.html#features-correlation-matrix",
    "href": "posts/renan-blog-post-week6/Readme.html#features-correlation-matrix",
    "title": "Notes on project Week 6",
    "section": "",
    "text": "Missing the Features Correlation Matrix, it is on the Page 6 of the paper.\nFigure 4.  Features correlation heatmap for the dataset. Color intensity indicates the strength and direction of correlations, aiding in the identification of potential patterns and dependencies in the data."
  },
  {
    "objectID": "posts/renan-blog-post-week6/Readme.html#sparsity-matrix",
    "href": "posts/renan-blog-post-week6/Readme.html#sparsity-matrix",
    "title": "Notes on project Week 6",
    "section": "",
    "text": "Missing the Sparsity Matrix, it is on the Page 6 of the paper.\nFigure 5.  Sparsity matrix for the dataset. The empty spaces found in the corresponding column signify the presence of missing data values for the specific feature.\nFrom the section 3.1 Visualizing Key Features"
  },
  {
    "objectID": "posts/renan-blog-post-week5/index.html",
    "href": "posts/renan-blog-post-week5/index.html",
    "title": "Project Setup - Week 5",
    "section": "",
    "text": "This week we are getting started on how to setup the Quarto and R project for proper Collaboration.\nThis post will demonstrate how to install RENV, initate your Renv environment and then load the dataset and do some demonstrations manipulating the dataset.\nWe will be using the dataset: Stroke Prediction Dataset"
  },
  {
    "objectID": "posts/renan-blog-post-week5/index.html#what-is-renv",
    "href": "posts/renan-blog-post-week5/index.html#what-is-renv",
    "title": "Project Setup - Week 5",
    "section": "What is Renv",
    "text": "What is Renv\nrenv is a pakcage manager that helps you create reproducible environments for your R projects.\nInstall the latest version of renv from CRAN with:\n```{r}\ninstall.packages(\"renv\")\n```\n\nRenv Workflow\nUse renv::init() to initialize renv in a new or existing project. This will set up a project library, containing all the packages you’re currently using. The packages (and all the metadata needed to reinstall them) are recorded into a lockfile, renv.lock, and a .Rprofile ensures that the library is used every time you open that project.\nAs you continue to work on your project, you will install and upgrade packages, either using install.packages() and update.packages() or renv::install() and renv::update(). After you’ve confirmed your code works as expected, use renv::snapshot() to record the packages and their sources in the lockfile.\nLater, if you need to share your code with someone else or run your code on new machine, your collaborator (or you) can call renv::restore() to reinstall the specific package versions recorded in the lockfile.\n\n\nLearning more\nIf this is your first time using renv, we strongly recommend starting with the Introduction to renv vignette: this will help you understand the most important verbs and nouns of renv.\nIf you have a question about renv, please first check the FAQ to see whether your question has already been addressed. If it hasn’t, please feel free to ask on the Posit Forum.\nIf you believe you’ve found a bug in renv, please file a bug (and, if possible, a reproducible example) at https://github.com/rstudio/renv/issues."
  },
  {
    "objectID": "posts/renan-blog-post-week5/index.html#import-dataset-example",
    "href": "posts/renan-blog-post-week5/index.html#import-dataset-example",
    "title": "Project Setup - Week 5",
    "section": "Import Dataset Example",
    "text": "Import Dataset Example\nGet the packages setup:\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\n\nlibrary(fitdistrplus)\nlibrary(gsheet)\nlibrary(boot)\nlibrary(readr)\n\n\n\nImport the dataset\nThis should find the path to the datasets folder programatically.\n\n\nCode\nfind_git_root &lt;- function(start = getwd()) {\n  path &lt;- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path &lt;- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root &lt;- find_git_root()\ndatasets_path &lt;- file.path(repo_root, \"datasets\")\n# repo_root\n# datasets_path\n\n\nNow we define the dataset we want to load, healthcare-dataset-stroke-data.csv will be inside kaggle-healthcare-dataset-stroke-data.\n\n\nCode\nkaggle_dataset_path &lt;- file.path(datasets_path, \"kaggle-healthcare-dataset-stroke-data/healthcare-dataset-stroke-data.csv\")\n\nkaggle_data1 = read_csv(kaggle_dataset_path, show_col_types = FALSE)\n\n\nExploring the dataset, BMI is not stored as numeric value also the NA fields are stored as text “N/A”.\n\n\nCode\nhead(kaggle_data1)\n\n\n# A tibble: 6 × 12\n     id gender   age hypertension heart_disease ever_married work_type    \n  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        \n1  9046 Male      67            0             1 Yes          Private      \n2 51676 Female    61            0             0 Yes          Self-employed\n3 31112 Male      80            0             1 Yes          Private      \n4 60182 Female    49            0             0 Yes          Private      \n5  1665 Female    79            1             0 Yes          Self-employed\n6 56669 Male      81            0             0 Yes          Private      \n# ℹ 5 more variables: Residence_type &lt;chr&gt;, avg_glucose_level &lt;dbl&gt;, bmi &lt;chr&gt;,\n#   smoking_status &lt;chr&gt;, stroke &lt;dbl&gt;\n\n\nCode\n# Count total NAs per column\ncolSums(is.na(kaggle_data1))\n\n\n               id            gender               age      hypertension \n                0                 0                 0                 0 \n    heart_disease      ever_married         work_type    Residence_type \n                0                 0                 0                 0 \navg_glucose_level               bmi    smoking_status            stroke \n                0                 0                 0                 0 \n\n\nApparently seems there is no NA values. Let’s continue.\n\n\nCode\n# overall\nsummary(kaggle_data1)\n\n\n       id           gender               age         hypertension    \n Min.   :   67   Length:5110        Min.   : 0.08   Min.   :0.00000  \n 1st Qu.:17741   Class :character   1st Qu.:25.00   1st Qu.:0.00000  \n Median :36932   Mode  :character   Median :45.00   Median :0.00000  \n Mean   :36518                      Mean   :43.23   Mean   :0.09746  \n 3rd Qu.:54682                      3rd Qu.:61.00   3rd Qu.:0.00000  \n Max.   :72940                      Max.   :82.00   Max.   :1.00000  \n heart_disease     ever_married        work_type         Residence_type    \n Min.   :0.00000   Length:5110        Length:5110        Length:5110       \n 1st Qu.:0.00000   Class :character   Class :character   Class :character  \n Median :0.00000   Mode  :character   Mode  :character   Mode  :character  \n Mean   :0.05401                                                           \n 3rd Qu.:0.00000                                                           \n Max.   :1.00000                                                           \n avg_glucose_level     bmi            smoking_status         stroke       \n Min.   : 55.12    Length:5110        Length:5110        Min.   :0.00000  \n 1st Qu.: 77.25    Class :character   Class :character   1st Qu.:0.00000  \n Median : 91.89    Mode  :character   Mode  :character   Median :0.00000  \n Mean   :106.15                                          Mean   :0.04873  \n 3rd Qu.:114.09                                          3rd Qu.:0.00000  \n Max.   :271.74                                          Max.   :1.00000  \n\n\nCode\nglimpse(kaggle_data1)\n\n\nRows: 5,110\nColumns: 12\n$ id                &lt;dbl&gt; 9046, 51676, 31112, 60182, 1665, 56669, 53882, 10434…\n$ gender            &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Female\", \"Female\", \"Male\"…\n$ age               &lt;dbl&gt; 67, 61, 80, 49, 79, 81, 74, 69, 59, 78, 81, 61, 54, …\n$ hypertension      &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1…\n$ heart_disease     &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0…\n$ ever_married      &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No…\n$ work_type         &lt;chr&gt; \"Private\", \"Self-employed\", \"Private\", \"Private\", \"S…\n$ Residence_type    &lt;chr&gt; \"Urban\", \"Rural\", \"Rural\", \"Urban\", \"Rural\", \"Urban\"…\n$ avg_glucose_level &lt;dbl&gt; 228.69, 202.21, 105.92, 171.23, 174.12, 186.21, 70.0…\n$ bmi               &lt;chr&gt; \"36.6\", \"N/A\", \"32.5\", \"34.4\", \"24\", \"29\", \"27.4\", \"…\n$ smoking_status    &lt;chr&gt; \"formerly smoked\", \"never smoked\", \"never smoked\", \"…\n$ stroke            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nSummary give some interesting insights but glimpse shows that there are NA values, even worse, the BMI values are stored and strings and should be numeric.\nNow lets explore the Categorical and Numeric variables.\n\n\nCode\n# check categorical variables\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Check one by one, lets see what we got\n# kaggle_data1 %&gt;% count(gender)\n# kaggle_data1 %&gt;% count(hypertension)\n# kaggle_data1 %&gt;% count(heart_disease)\n# kaggle_data1 %&gt;% count(ever_married)\n# kaggle_data1 %&gt;% count(work_type)\n# kaggle_data1 %&gt;% count(Residence_type )\n# kaggle_data1 %&gt;% count(smoking_status)\n# kaggle_data1 %&gt;% count(stroke)\n\n# Now make it a little cleaner\ncat_vars &lt;- c(\"gender\", \"hypertension\", \"heart_disease\", \"ever_married\",\n              \"work_type\", \"Residence_type\", \"smoking_status\", \"stroke\")\n\nkaggle_data1[, cat_vars] %&gt;%\n  # Convert all to character to avoid type conflicts\n  mutate_all(as.character) %&gt;%\n  pivot_longer(cols = names(.), names_to = \"variable\", values_to = \"value\") %&gt;%\n  count(variable, value) %&gt;%\n  arrange(variable, desc(n)) %&gt;% print(n = 22)\n\n\n# A tibble: 22 × 3\n   variable       value               n\n   &lt;chr&gt;          &lt;chr&gt;           &lt;int&gt;\n 1 Residence_type Urban            2596\n 2 Residence_type Rural            2514\n 3 ever_married   Yes              3353\n 4 ever_married   No               1757\n 5 gender         Female           2994\n 6 gender         Male             2115\n 7 gender         Other               1\n 8 heart_disease  0                4834\n 9 heart_disease  1                 276\n10 hypertension   0                4612\n11 hypertension   1                 498\n12 smoking_status never smoked     1892\n13 smoking_status Unknown          1544\n14 smoking_status formerly smoked   885\n15 smoking_status smokes            789\n16 stroke         0                4861\n17 stroke         1                 249\n18 work_type      Private          2925\n19 work_type      Self-employed     819\n20 work_type      children          687\n21 work_type      Govt_job          657\n22 work_type      Never_worked       22\n\n\nIts pretty interesting, now lets see what happens with the numeric variables\n\n\nCode\n# Check Numeric Variables - id, age, avg_glucose_level, bmi\nkaggle_data1 %&gt;%\n  select_if(is.numeric) %&gt;%\n  summary()\n\n\n       id             age         hypertension     heart_disease    \n Min.   :   67   Min.   : 0.08   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:17741   1st Qu.:25.00   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :36932   Median :45.00   Median :0.00000   Median :0.00000  \n Mean   :36518   Mean   :43.23   Mean   :0.09746   Mean   :0.05401  \n 3rd Qu.:54682   3rd Qu.:61.00   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :72940   Max.   :82.00   Max.   :1.00000   Max.   :1.00000  \n avg_glucose_level     stroke       \n Min.   : 55.12    Min.   :0.00000  \n 1st Qu.: 77.25    1st Qu.:0.00000  \n Median : 91.89    Median :0.00000  \n Mean   :106.15    Mean   :0.04873  \n 3rd Qu.:114.09    3rd Qu.:0.00000  \n Max.   :271.74    Max.   :1.00000  \n\n\nWe need to deal with the BMI data which has missing values and its not stored as numerical.\n\n\nCode\n# unique(kaggle_data1$bmi)\nkaggle_data2 &lt;- kaggle_data1 %&gt;%\n  mutate(bmi = na_if(bmi, \"N/A\")) %&gt;%   # Convert \"N/A\" string to NA\n  mutate(bmi = as.numeric(bmi))         # Convert from character to numeric\n\n# kaggle_data2 &lt;- kaggle_data1 %&gt;% mutate(bmi = as.numeric(na_if(bmi, \"N/A\")))\n\n# Check if it worked\nstr(kaggle_data2$bmi)\n\n\n num [1:5110] 36.6 NA 32.5 34.4 24 29 27.4 22.8 NA 24.2 ...\n\n\nCode\nsum(is.na(kaggle_data2$bmi))\n\n\n[1] 201\n\n\nCode\n# Check Numeric Variables - id, age, avg_glucose_level, bmi\nkaggle_data2 %&gt;%\n  select_if(is.numeric) %&gt;%\n  summary()\n\n\n       id             age         hypertension     heart_disease    \n Min.   :   67   Min.   : 0.08   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:17741   1st Qu.:25.00   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :36932   Median :45.00   Median :0.00000   Median :0.00000  \n Mean   :36518   Mean   :43.23   Mean   :0.09746   Mean   :0.05401  \n 3rd Qu.:54682   3rd Qu.:61.00   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :72940   Max.   :82.00   Max.   :1.00000   Max.   :1.00000  \n                                                                    \n avg_glucose_level      bmi            stroke       \n Min.   : 55.12    Min.   :10.30   Min.   :0.00000  \n 1st Qu.: 77.25    1st Qu.:23.50   1st Qu.:0.00000  \n Median : 91.89    Median :28.10   Median :0.00000  \n Mean   :106.15    Mean   :28.89   Mean   :0.04873  \n 3rd Qu.:114.09    3rd Qu.:33.10   3rd Qu.:0.00000  \n Max.   :271.74    Max.   :97.60   Max.   :1.00000  \n                   NA's   :201"
  },
  {
    "objectID": "posts/renan-blog-post-week5/index.html#conclusion",
    "href": "posts/renan-blog-post-week5/index.html#conclusion",
    "title": "Project Setup - Week 5",
    "section": "Conclusion",
    "text": "Conclusion\nThe dataset is imbalanced and has many issues there are several research work that explore solutions:\n\nMachine learning for stroke prediction using imbalanced data[1]\nPredictive modelling and identification of key risk factors for stroke using machine learning[2]\n\nThe research Predictive modelling and identification of key risk factors for stroke using machine learning has made several contributions adding a lot of insights:\n\nExploring various data imputation techniques and addressing data imbalance issues in order to enhance the accuracy and robustness of stroke prediction models.\nIdentifying crucial features for stroke prediction and uncovering previously unknown risk factors, giving a comprehensive understanding of stroke risk assessment.\nCreating an augmented dataset incorporating important key risk factor features using the imputed datasets, enhancing the effectiveness of stroke prediction models.\nAssessing the effectiveness of advanced machine learning models across different datasets and creating a robust Dense Stacking Ensemble model for stroke prediction.\nThe key contribution is showcasing the enhanced predictive capabilities of the model in accurately identifying and testing strokes, surpassing the performance of prior studies that utilized the same dataset.\n\n\n\n\n\n\n\nNote\n\n\n\nLarge datasets might need Github LFS which is not setup, therefore must store then externally."
  },
  {
    "objectID": "posts/renan-blog-post-week5/index.html#additional-thoughts",
    "href": "posts/renan-blog-post-week5/index.html#additional-thoughts",
    "title": "Project Setup - Week 5",
    "section": "Additional Thoughts",
    "text": "Additional Thoughts\nQuarto websites when combined with python and R is a great way to\nQuarto websites, when combined with Python and R, offer a powerful way to create dynamic, data-driven content that turns out into amazing presentations rich in visual content.\nHowever there are limitations, Github Actions runner is not powerful and before submitting the project for rendering must take that into consideration. On future work will evaluate solutions to the computational budget limitations in Github Action Runner.\nHow to efficiently break up a computationally heavy article into separate notebooks?#8410\nSome have mentioned that the project can be split into sections.\n\nReferences\n\n\n1. Melnykova, N., Patereha, Y., Skopivskyi, S., Farion, M., Fedushko, S., & Drohomyretska, K. (2025). Machine learning for stroke prediction using imbalanced data. Scientific Reports, 15(1), 33773.\n\n\n2. Hassan, A., Gulzar Ahmad, S., Ullah Munir, E., Ali Khan, I., & Ramzan, N. (2024). Predictive modelling and identification of key risk factors for stroke using machine learning. Scientific Reports, 14(1), 11498."
  },
  {
    "objectID": "posts/shree-blog-post-week4/index.html",
    "href": "posts/shree-blog-post-week4/index.html",
    "title": "Literature Review Week 4",
    "section": "",
    "text": "Regularized logistic regression with network‐based pairwise interaction for biomarker identification in breast cancer (Wu et al., 2016)[1]\n\nGoal: Regularized logistic regression should be used, along with network (biological network) information and pairwise interactions, to find biomarkers (both single and interacting pairs) for breast cancer.\n\nLink: https://www.researchgate.net/publication/296193700_Regularized_logistic_regression_with_network-based_pairwise_interaction_for_biomarker_identification_in_breast_cancer\nWhat made this paper interesting, or why the analysis is important: different biological processes has combined interactions between genes and proteins rather than single genes or proteins, like network topology or interaction information, which may result in better biomarkers that are biologically useful rather than statistically significant. Associating network knowledge may improve prediction or interpretability.\nMethodology: The analyst used a regularized logistic regression model that incorporates pairwise connections and protein-protein interaction (PPI) networks. they prioritized biologically plausible biomarker combinations and used an adaptive elastic net (a penalty that balances l1 and l2) with network constraints. Used breast cancer datasets (gene expression data) to discover key nodes and relationships.\nResult/conclusion: Their model outperforms simpler models in terms of predictive performance, and they were able to discover both individual biomarkers and interacting gene pairs. The interactions has been found to have some biological sense.\nLimitation: The risk of overfitting and model size are increased by the intricacy of incorporating relationships. The quality of the network and expression data determines the outcomes.\n\n\nUsing Genetic Algorithms and Sparse Logistic Regression to Find Gene Signatures for Chemosensitivity Prediction in Breast Cancer.[2]\nGoal: To identify “gene signatures” that predict chemosensitivity, that is, which tumors react to chemotherapy in breast cancer, combine genetic algorithms with sparse logistic regression.\nWhat made this paper interesting, or why the analysis is important:Predicting which patients will react to chemotherapy gives more personalized treatment. Potential biomarkers include gene signatures. However, there are several genes and possible combinations, like genetic algorithms that aid in searching space, while sparse logistic regression aids in reducing characteristics.\nMethodology: To create individuals’ “gene signature” subsets, first choose genes using a Genetic Algorithm (GA) from among overexpressed genes (or pathway-specific genes) and forecast response, create sparse logistic regression models using those subsets. Assess accuracy, sensitivity, specificity, and other metrics using both a training and a validation set.\nResult/ Conclusion : The results show that SLR-28 and Notch-86, two gene signatures, perform well on training and validation sets in terms of accuracy, specificity, sensitivity, and other metrics. In some reults we can see it performs better than previous signatures.\nLimitation: Generalization is uncertain due to the relatively small datasets. Randomness is added by the GA, signature stability may differ. Clinical validation also comes at a high expense. overfitting risk.\n\n\n\n\n1. Wu, M.-Y., Zhang, X.-F., Dai, D.-Q., Ou-Yang, L., Zhu, Y., & Yan, H. (2016). Regularized logistic regression with network-based pairwise interaction for biomarker identification in breast cancer. BMC Bioinformatics, 17(1), 108.\n\n\n2. Hu, W. (2016). Using genetic algorithms and sparse logistic regression to find gene signatures for chemosensitivity prediction in breast cancer. American Journal of Bioscience and Bioengineering, 4(2), 26–33."
  },
  {
    "objectID": "posts/shree-blog-post-week4/index.html#article-2",
    "href": "posts/shree-blog-post-week4/index.html#article-2",
    "title": "Literature Review Week 4",
    "section": "",
    "text": "Using Genetic Algorithms and Sparse Logistic Regression to Find Gene Signatures for Chemosensitivity Prediction in Breast Cancer.[2]\nGoal: To identify “gene signatures” that predict chemosensitivity, that is, which tumors react to chemotherapy in breast cancer, combine genetic algorithms with sparse logistic regression.\nWhat made this paper interesting, or why the analysis is important:Predicting which patients will react to chemotherapy gives more personalized treatment. Potential biomarkers include gene signatures. However, there are several genes and possible combinations, like genetic algorithms that aid in searching space, while sparse logistic regression aids in reducing characteristics.\nMethodology: To create individuals’ “gene signature” subsets, first choose genes using a Genetic Algorithm (GA) from among overexpressed genes (or pathway-specific genes) and forecast response, create sparse logistic regression models using those subsets. Assess accuracy, sensitivity, specificity, and other metrics using both a training and a validation set.\nResult/ Conclusion : The results show that SLR-28 and Notch-86, two gene signatures, perform well on training and validation sets in terms of accuracy, specificity, sensitivity, and other metrics. In some reults we can see it performs better than previous signatures.\nLimitation: Generalization is uncertain due to the relatively small datasets. Randomness is added by the GA, signature stability may differ. Clinical validation also comes at a high expense. overfitting risk.\n\n\n\n\n1. Wu, M.-Y., Zhang, X.-F., Dai, D.-Q., Ou-Yang, L., Zhu, Y., & Yan, H. (2016). Regularized logistic regression with network-based pairwise interaction for biomarker identification in breast cancer. BMC Bioinformatics, 17(1), 108.\n\n\n2. Hu, W. (2016). Using genetic algorithms and sparse logistic regression to find gene signatures for chemosensitivity prediction in breast cancer. American Journal of Bioscience and Bioengineering, 4(2), 26–33."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "Welcome to our team! We are a group of collaborators dedicated to…\n\n\n\nRenan Monteiro Barbosa\n\nData Scientist\n\n\n\nRenan Monteiro Barbosa\n\n\nJane is a data scientist with over 10 years of experience…\nContact * Email: jane@example.com * GitHub: github.com/jane\n\n\n\nKristina Kusem\n\nSoftware Engineer\n\n\n\nKristina Kusem\n\n\nJohn is a software engineer specializing in web development…\nContact * Email: john@example.com * Twitter: @john_smith\n\n\n\nShree Krishna Basnet\n\nResearch Analyst\n\n\n\nShree Krishna Basnet\n\n\nAlex works on developing new research methodologies for…\nContact * Email: alex@example.com * Website: alex-johnson.com"
  },
  {
    "objectID": "people/kusem-kristina/index.html#education",
    "href": "people/kusem-kristina/index.html#education",
    "title": "Kristina Kusem",
    "section": "Education",
    "text": "Education\nB.S. Mechanical Engineering | Univevrsity of West Florida"
  },
  {
    "objectID": "people/index.html",
    "href": "people/index.html",
    "title": "Meet the Group",
    "section": "",
    "text": "Graduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/index.html#graduate-students",
    "href": "people/index.html#graduate-students",
    "title": "Meet the Group",
    "section": "",
    "text": "Graduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Literature Review",
    "section": "",
    "text": "These are the literature review done by all the students during this semester.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset Exploration - Week 6\n\n\n\ndataset exploration\n\nweek 6\n\nrenan\n\n\n\nFor Week 6 we are exploring the Stroke Dataset\n\n\n\n\n\nRenan Monteiro Barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 2\n\n\n\nliterature review\n\nweek 2\n\nkristina\n\n\n\nLiterature review for the Week 2 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nKristina Kusem\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 2\n\n\n\nliterature review\n\nweek 2\n\nrenan\n\n\n\nLiterature review for the Week 2 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 2\n\n\n\nliterature review\n\nweek 2\n\nshree\n\n\n\nLiterature review for the Week 2 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nShree Krishna Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 3\n\n\n\nliterature review\n\nweek 3\n\nkristina\n\n\n\nLiterature review for the Week 3 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nKristina Kusem\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 3\n\n\n\nliterature review\n\nweek 3\n\nrenan\n\n\n\nLiterature review for the Week 3 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 3\n\n\n\nliterature review\n\nweek 3\n\nshree\n\n\n\nLiterature review for the Week 3 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nShree Krishna Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 4\n\n\n\nliterature review\n\nweek 4\n\nkristina\n\n\n\nLiterature review for the Week 4 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nKristina Kusem\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 4\n\n\n\nliterature review\n\nweek 4\n\nrenan\n\n\n\nLiterature review for the Week 4 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 4\n\n\n\nliterature review\n\nweek 4\n\nshree\n\n\n\nLiterature review for the Week 4 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nShree Krishna Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 5\n\n\n\nliterature review\n\nweek 5\n\nkristina\n\n\n\nLiterature review for the Week 5 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nKristina Kusem\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 5\n\n\n\nliterature review\n\nweek 5\n\nshree\n\n\n\nLiterature review for the Week 5 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nShree Krishna Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nProject Setup - Week 5\n\n\n\ngetting started\n\nweek 5\n\nrenan\n\n\n\nFor Week 5 setting up Quarto Website and Getting Started with R project\n\n\n\n\n\nRenan Monteiro Barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nReproducing Steve’s Code - Week 7\n\n\n\ncoding\n\nweek 7\n\nrenan\n\n\n\nFor Week 7 we are replicating Steve’s findings\n\n\n\n\n\nRenan Monteiro Barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nReproducing Steve’s Code - Week 8\n\n\n\ncoding\n\nweek 8\n\nrenan\n\n\n\nFor Week 8 we are replicating Steve’s findings\n\n\n\n\n\nRenan Monteiro Barbosa\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/renan-blog-post-week8/Stroke_Results_10182025_SW.html",
    "href": "posts/renan-blog-post-week8/Stroke_Results_10182025_SW.html",
    "title": "Reproducing Steve’s Code - Stroke_results_1019_2025_latest",
    "section": "",
    "text": "On this document I try to replicate Steve code: Stroke_Results_1019_2025_latest_440PM_SW_For_Group.R\n# install.packages(\"Hmisc\")\n# library(Hmisc)\nrcorr(as.matrix(LR_stroke1))\n\n                     gender   age hypertension heart_disease ever_married\ngender                 1.00 -0.06        -0.04         -0.10         0.03\nage                   -0.06  1.00         0.26          0.26        -0.49\nhypertension          -0.04  0.26         1.00          0.11        -0.11\nheart_disease         -0.10  0.26         0.11          1.00        -0.07\never_married           0.03 -0.49        -0.11         -0.07         1.00\nwork_type              0.01  0.14         0.05          0.03        -0.02\nResidence_type        -0.01 -0.02         0.00         -0.01         0.01\navg_glucose_level     -0.07  0.24         0.17          0.14        -0.12\nbmi                   -0.02  0.04         0.13          0.00        -0.13\nsmoking_status        -0.08  0.03        -0.01          0.06        -0.06\nstroke                -0.01  0.24         0.14          0.14        -0.07\ngenderadj              1.00 -0.06        -0.04         -0.10         0.03\nageadj                -0.06  1.00         0.26          0.26        -0.49\nhypertensionadj       -0.04  0.26         1.00          0.11        -0.11\nheart_diseaseadj      -0.10  0.26         0.11          1.00        -0.07\never_marriedadj        0.03 -0.49        -0.11         -0.07         1.00\nwork_typeadj           0.01  0.14         0.05          0.03        -0.02\nResidence_typeadj     -0.01 -0.02         0.00         -0.01         0.01\navg_glucose_leveladj  -0.07  0.24         0.17          0.14        -0.12\nbmiadj                -0.02  0.04         0.13          0.00        -0.13\nsmoking_statusadj     -0.08  0.03        -0.01          0.06        -0.06\n                     work_type Residence_type avg_glucose_level   bmi\ngender                    0.01          -0.01             -0.07 -0.02\nage                       0.14          -0.02              0.24  0.04\nhypertension              0.05           0.00              0.17  0.13\nheart_disease             0.03          -0.01              0.14  0.00\never_married             -0.02           0.01             -0.12 -0.13\nwork_type                 1.00          -0.01              0.03 -0.02\nResidence_type           -0.01           1.00              0.01  0.01\navg_glucose_level         0.03           0.01              1.00  0.16\nbmi                      -0.02           0.01              0.16  1.00\nsmoking_status           -0.02          -0.04              0.01  0.03\nstroke                    0.04          -0.01              0.14  0.01\ngenderadj                 0.01          -0.01             -0.07 -0.02\nageadj                    0.14          -0.02              0.24  0.04\nhypertensionadj           0.05           0.00              0.17  0.13\nheart_diseaseadj          0.03          -0.01              0.14  0.00\never_marriedadj          -0.02           0.01             -0.12 -0.13\nwork_typeadj              1.00          -0.01              0.03 -0.02\nResidence_typeadj        -0.01           1.00              0.01  0.01\navg_glucose_leveladj      0.03           0.01              1.00  0.16\nbmiadj                   -0.02           0.01              0.16  1.00\nsmoking_statusadj        -0.02          -0.04              0.01  0.03\n                     smoking_status stroke genderadj ageadj hypertensionadj\ngender                        -0.08  -0.01      1.00  -0.06           -0.04\nage                            0.03   0.24     -0.06   1.00            0.26\nhypertension                  -0.01   0.14     -0.04   0.26            1.00\nheart_disease                  0.06   0.14     -0.10   0.26            0.11\never_married                  -0.06  -0.07      0.03  -0.49           -0.11\nwork_type                     -0.02   0.04      0.01   0.14            0.05\nResidence_type                -0.04  -0.01     -0.01  -0.02            0.00\navg_glucose_level              0.01   0.14     -0.07   0.24            0.17\nbmi                            0.03   0.01     -0.02   0.04            0.13\nsmoking_status                 1.00   0.02     -0.08   0.03           -0.01\nstroke                         0.02   1.00     -0.01   0.24            0.14\ngenderadj                     -0.08  -0.01      1.00  -0.06           -0.04\nageadj                         0.03   0.24     -0.06   1.00            0.26\nhypertensionadj               -0.01   0.14     -0.04   0.26            1.00\nheart_diseaseadj               0.06   0.14     -0.10   0.26            0.11\never_marriedadj               -0.06  -0.07      0.03  -0.49           -0.11\nwork_typeadj                  -0.02   0.04      0.01   0.14            0.05\nResidence_typeadj             -0.04  -0.01     -0.01  -0.02            0.00\navg_glucose_leveladj           0.01   0.14     -0.07   0.24            0.17\nbmiadj                         0.03   0.01     -0.02   0.04            0.13\nsmoking_statusadj              1.00   0.02     -0.08   0.03           -0.01\n                     heart_diseaseadj ever_marriedadj work_typeadj\ngender                          -0.10            0.03         0.01\nage                              0.26           -0.49         0.14\nhypertension                     0.11           -0.11         0.05\nheart_disease                    1.00           -0.07         0.03\never_married                    -0.07            1.00        -0.02\nwork_type                        0.03           -0.02         1.00\nResidence_type                  -0.01            0.01        -0.01\navg_glucose_level                0.14           -0.12         0.03\nbmi                              0.00           -0.13        -0.02\nsmoking_status                   0.06           -0.06        -0.02\nstroke                           0.14           -0.07         0.04\ngenderadj                       -0.10            0.03         0.01\nageadj                           0.26           -0.49         0.14\nhypertensionadj                  0.11           -0.11         0.05\nheart_diseaseadj                 1.00           -0.07         0.03\never_marriedadj                 -0.07            1.00        -0.02\nwork_typeadj                     0.03           -0.02         1.00\nResidence_typeadj               -0.01            0.01        -0.01\navg_glucose_leveladj             0.14           -0.12         0.03\nbmiadj                           0.00           -0.13        -0.02\nsmoking_statusadj                0.06           -0.06        -0.02\n                     Residence_typeadj avg_glucose_leveladj bmiadj\ngender                           -0.01                -0.07  -0.02\nage                              -0.02                 0.24   0.04\nhypertension                      0.00                 0.17   0.13\nheart_disease                    -0.01                 0.14   0.00\never_married                      0.01                -0.12  -0.13\nwork_type                        -0.01                 0.03  -0.02\nResidence_type                    1.00                 0.01   0.01\navg_glucose_level                 0.01                 1.00   0.16\nbmi                               0.01                 0.16   1.00\nsmoking_status                   -0.04                 0.01   0.03\nstroke                           -0.01                 0.14   0.01\ngenderadj                        -0.01                -0.07  -0.02\nageadj                           -0.02                 0.24   0.04\nhypertensionadj                   0.00                 0.17   0.13\nheart_diseaseadj                 -0.01                 0.14   0.00\never_marriedadj                   0.01                -0.12  -0.13\nwork_typeadj                     -0.01                 0.03  -0.02\nResidence_typeadj                 1.00                 0.01   0.01\navg_glucose_leveladj              0.01                 1.00   0.16\nbmiadj                            0.01                 0.16   1.00\nsmoking_statusadj                -0.04                 0.01   0.03\n                     smoking_statusadj\ngender                           -0.08\nage                               0.03\nhypertension                     -0.01\nheart_disease                     0.06\never_married                     -0.06\nwork_type                        -0.02\nResidence_type                   -0.04\navg_glucose_level                 0.01\nbmi                               0.03\nsmoking_status                    1.00\nstroke                            0.02\ngenderadj                        -0.08\nageadj                            0.03\nhypertensionadj                  -0.01\nheart_diseaseadj                  0.06\never_marriedadj                  -0.06\nwork_typeadj                     -0.02\nResidence_typeadj                -0.04\navg_glucose_leveladj              0.01\nbmiadj                            0.03\nsmoking_statusadj                 1.00\n\nn= 3357 \n\n\nP\n                     gender age    hypertension heart_disease ever_married\ngender                      0.0012 0.0204       0.0000        0.1140      \nage                  0.0012        0.0000       0.0000        0.0000      \nhypertension         0.0204 0.0000              0.0000        0.0000      \nheart_disease        0.0000 0.0000 0.0000                     0.0000      \never_married         0.1140 0.0000 0.0000       0.0000                    \nwork_type            0.3863 0.0000 0.0051       0.0862        0.2313      \nResidence_type       0.5077 0.3076 0.8569       0.5527        0.5150      \navg_glucose_level    0.0000 0.0000 0.0000       0.0000        0.0000      \nbmi                  0.2487 0.0144 0.0000       0.8185        0.0000      \nsmoking_status       0.0000 0.0448 0.7537       0.0005        0.0009      \nstroke               0.4296 0.0000 0.0000       0.0000        0.0002      \ngenderadj            0.0000 0.0012 0.0204       0.0000        0.1140      \nageadj               0.0012 0.0000 0.0000       0.0000        0.0000      \nhypertensionadj      0.0204 0.0000 0.0000       0.0000        0.0000      \nheart_diseaseadj     0.0000 0.0000 0.0000       0.0000        0.0000      \never_marriedadj      0.1140 0.0000 0.0000       0.0000        0.0000      \nwork_typeadj         0.3863 0.0000 0.0051       0.0862        0.2313      \nResidence_typeadj    0.5077 0.3076 0.8569       0.5527        0.5150      \navg_glucose_leveladj 0.0000 0.0000 0.0000       0.0000        0.0000      \nbmiadj               0.2487 0.0144 0.0000       0.8185        0.0000      \nsmoking_statusadj    0.0000 0.0448 0.7537       0.0005        0.0009      \n                     work_type Residence_type avg_glucose_level bmi   \ngender               0.3863    0.5077         0.0000            0.2487\nage                  0.0000    0.3076         0.0000            0.0144\nhypertension         0.0051    0.8569         0.0000            0.0000\nheart_disease        0.0862    0.5527         0.0000            0.8185\never_married         0.2313    0.5150         0.0000            0.0000\nwork_type                      0.6768         0.0467            0.3243\nResidence_type       0.6768                   0.6427            0.5689\navg_glucose_level    0.0467    0.6427                           0.0000\nbmi                  0.3243    0.5689         0.0000                  \nsmoking_status       0.3764    0.0208         0.7679            0.0925\nstroke               0.0259    0.7171         0.0000            0.6866\ngenderadj            0.3863    0.5077         0.0000            0.2487\nageadj               0.0000    0.3076         0.0000            0.0144\nhypertensionadj      0.0051    0.8569         0.0000            0.0000\nheart_diseaseadj     0.0862    0.5527         0.0000            0.8185\never_marriedadj      0.2313    0.5150         0.0000            0.0000\nwork_typeadj         0.0000    0.6768         0.0467            0.3243\nResidence_typeadj    0.6768    0.0000         0.6427            0.5689\navg_glucose_leveladj 0.0467    0.6427         0.0000            0.0000\nbmiadj               0.3243    0.5689         0.0000            0.0000\nsmoking_statusadj    0.3764    0.0208         0.7679            0.0925\n                     smoking_status stroke genderadj ageadj hypertensionadj\ngender               0.0000         0.4296 0.0000    0.0012 0.0204         \nage                  0.0448         0.0000 0.0012    0.0000 0.0000         \nhypertension         0.7537         0.0000 0.0204    0.0000 0.0000         \nheart_disease        0.0005         0.0000 0.0000    0.0000 0.0000         \never_married         0.0009         0.0002 0.1140    0.0000 0.0000         \nwork_type            0.3764         0.0259 0.3863    0.0000 0.0051         \nResidence_type       0.0208         0.7171 0.5077    0.3076 0.8569         \navg_glucose_level    0.7679         0.0000 0.0000    0.0000 0.0000         \nbmi                  0.0925         0.6866 0.2487    0.0144 0.0000         \nsmoking_status                      0.2559 0.0000    0.0448 0.7537         \nstroke               0.2559                0.4296    0.0000 0.0000         \ngenderadj            0.0000         0.4296           0.0012 0.0204         \nageadj               0.0448         0.0000 0.0012           0.0000         \nhypertensionadj      0.7537         0.0000 0.0204    0.0000                \nheart_diseaseadj     0.0005         0.0000 0.0000    0.0000 0.0000         \never_marriedadj      0.0009         0.0002 0.1140    0.0000 0.0000         \nwork_typeadj         0.3764         0.0259 0.3863    0.0000 0.0051         \nResidence_typeadj    0.0208         0.7171 0.5077    0.3076 0.8569         \navg_glucose_leveladj 0.7679         0.0000 0.0000    0.0000 0.0000         \nbmiadj               0.0925         0.6866 0.2487    0.0144 0.0000         \nsmoking_statusadj    0.0000         0.2559 0.0000    0.0448 0.7537         \n                     heart_diseaseadj ever_marriedadj work_typeadj\ngender               0.0000           0.1140          0.3863      \nage                  0.0000           0.0000          0.0000      \nhypertension         0.0000           0.0000          0.0051      \nheart_disease        0.0000           0.0000          0.0862      \never_married         0.0000           0.0000          0.2313      \nwork_type            0.0862           0.2313          0.0000      \nResidence_type       0.5527           0.5150          0.6768      \navg_glucose_level    0.0000           0.0000          0.0467      \nbmi                  0.8185           0.0000          0.3243      \nsmoking_status       0.0005           0.0009          0.3764      \nstroke               0.0000           0.0002          0.0259      \ngenderadj            0.0000           0.1140          0.3863      \nageadj               0.0000           0.0000          0.0000      \nhypertensionadj      0.0000           0.0000          0.0051      \nheart_diseaseadj                      0.0000          0.0862      \never_marriedadj      0.0000                           0.2313      \nwork_typeadj         0.0862           0.2313                      \nResidence_typeadj    0.5527           0.5150          0.6768      \navg_glucose_leveladj 0.0000           0.0000          0.0467      \nbmiadj               0.8185           0.0000          0.3243      \nsmoking_statusadj    0.0005           0.0009          0.3764      \n                     Residence_typeadj avg_glucose_leveladj bmiadj\ngender               0.5077            0.0000               0.2487\nage                  0.3076            0.0000               0.0144\nhypertension         0.8569            0.0000               0.0000\nheart_disease        0.5527            0.0000               0.8185\never_married         0.5150            0.0000               0.0000\nwork_type            0.6768            0.0467               0.3243\nResidence_type       0.0000            0.6427               0.5689\navg_glucose_level    0.6427            0.0000               0.0000\nbmi                  0.5689            0.0000               0.0000\nsmoking_status       0.0208            0.7679               0.0925\nstroke               0.7171            0.0000               0.6866\ngenderadj            0.5077            0.0000               0.2487\nageadj               0.3076            0.0000               0.0144\nhypertensionadj      0.8569            0.0000               0.0000\nheart_diseaseadj     0.5527            0.0000               0.8185\never_marriedadj      0.5150            0.0000               0.0000\nwork_typeadj         0.6768            0.0467               0.3243\nResidence_typeadj                      0.6427               0.5689\navg_glucose_leveladj 0.6427                                 0.0000\nbmiadj               0.5689            0.0000                     \nsmoking_statusadj    0.0208            0.7679               0.0925\n                     smoking_statusadj\ngender               0.0000           \nage                  0.0448           \nhypertension         0.7537           \nheart_disease        0.0005           \never_married         0.0009           \nwork_type            0.3764           \nResidence_type       0.0208           \navg_glucose_level    0.7679           \nbmi                  0.0925           \nsmoking_status       0.0000           \nstroke               0.2559           \ngenderadj            0.0000           \nageadj               0.0448           \nhypertensionadj      0.7537           \nheart_diseaseadj     0.0005           \never_marriedadj      0.0009           \nwork_typeadj         0.3764           \nResidence_typeadj    0.0208           \navg_glucose_leveladj 0.7679           \nbmiadj               0.0925           \nsmoking_statusadj\n# install.packages(\"car\")\n# library(car)\ninfluencePlot(model)\n\n\n\n\n\n\n\n\n        StudRes          Hat       CookD\n83    2.6917353 0.0039267816 0.012237368\n84    1.5018344 0.0343771041 0.006641860\n87    3.0732709 0.0012042796 0.011476828\n131   3.1608870 0.0006013465 0.007697762\n152   3.1135601 0.0005613972 0.006237531\n2583 -0.8509399 0.0399302730 0.001668526\nCooks D ranges from 0 to .0122\nWhile the ideal size is 4/N (4/3357 = 0.012), its far outside the danger zone of .5\nConclusion: Assumption 3 is met - No substantial outliers"
  },
  {
    "objectID": "posts/renan-blog-post-week8/Stroke_Results_10182025_SW.html#setup-and-data-loading",
    "href": "posts/renan-blog-post-week8/Stroke_Results_10182025_SW.html#setup-and-data-loading",
    "title": "Reproducing Steve’s Code - Stroke_results_1019_2025_latest",
    "section": "1. Setup and Data Loading",
    "text": "1. Setup and Data Loading\nFirst we need to install all packages, system dependencies and solve conflicts to produce a new renv.lock file.\n\n1.1 Load Libraries\nInstall packages if necessary:\n\n\nCode\n# Run this once to install all the necessary packages\n# install.packages(\"dplyr\")\n# install.packages(\"car\")\n# install.packages(\"ResourceSelection\")\n# install.packages(\"caret\")\n# install.packages(\"rcompanion\")\n# install.packages(\"pROC\")\n# install.packages(\"cvAUC\")\n\n\nLoad Libraries:\n\n\nCode\n# For data manipulation and visualization\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(knitr)\nlibrary(ggpubr)\n\n# For data preprocessing and modeling\nlibrary(mice)\nlibrary(ROSE) # For SMOTE\nlibrary(ranger) # A fast implementation of random forests\n\n# For stacking/ensemble models\nlibrary(stacks)\nlibrary(tidymodels)\n\nlibrary(themis)\nlibrary(gghighlight)\n\nlibrary(pscl)\nlibrary(dplyr)\nlibrary(car)\nlibrary(ResourceSelection)\nlibrary(caret)\nlibrary(rcompanion)\nlibrary(Hmisc)\nlibrary(pROC)\nlibrary(cvAUC)\n\n# Set seed for reproducibility\nset.seed(123)\n\n\n\n1.1.1 Possible Issues and conflicts to resolve\n```{bash}\n\n```\n\n\n\n1.2 Load Data\nWill be using my original Dataset as well Steve’s Dataset and compare for differences.\n\nRenan: kaggle_data1\nSteve: stroke1\n\n\n1.2.1 Renan Dataset\nBelow will be loading the healthcare-dataset-stroke-data.csv and performing necessary changes to the dataset and loading into the DataFrame: kaggle_data1\n\n\nCode\nfind_git_root &lt;- function(start = getwd()) {\n  path &lt;- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path &lt;- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root &lt;- find_git_root()\ndatasets_path &lt;- file.path(repo_root, \"datasets\")\nkaggle_dataset_path &lt;- file.path(datasets_path, \"kaggle-healthcare-dataset-stroke-data/healthcare-dataset-stroke-data.csv\")\nkaggle_data1 = read_csv(kaggle_dataset_path, show_col_types = FALSE)\n\n# unique(kaggle_data1$bmi)\nkaggle_data1 &lt;- kaggle_data1 %&gt;%\n  mutate(bmi = na_if(bmi, \"N/A\")) %&gt;%   # Convert \"N/A\" string to NA\n  mutate(bmi = as.numeric(bmi))         # Convert from character to numeric\n\n# Remove the 'Other' gender row and the 'id' column\nkaggle_data1 &lt;- kaggle_data1 %&gt;%\n  filter(gender != \"Other\") %&gt;%\n  select(-id) %&gt;%\n  mutate_if(is.character, as.factor) # Convert character columns to factors for easier modeling\n\n\n\n\n1.2.1 Steve Dataset\nBelow will be loading the stroke.csv and performing necessary changes to the dataset and loading into the DataFrame: stroke1\n\n\nCode\n# Reading the datafile in (the same one you got for us Renan)#\nsteve_dataset_path &lt;- file.path(datasets_path, \"steve/stroke.csv\")\nstroke1 = read_csv(steve_dataset_path, show_col_types = FALSE)\n\n\n\n\n\n1.3 Prepare Dataset\n\nhead(stroke1)\nnrow(stroke1)\nsummary(stroke1)\ncount_tables &lt;- lapply(stroke1, table)\ncount_tables\n\nPart 1: preparing the data\n\nSmoking Status - remove unknown\nbmi - remove N/A\nWork type - remove children\nage - create numerical variable with 2 places after the decimal\ngender - remove other\n\n\nstroke1[stroke1 == \"N/A\"] &lt;- NA\nstroke1[stroke1 == \"Unknown\"] &lt;- NA\nstroke1[stroke1 == \"children\"] &lt;- NA\nstroke1[stroke1 == \"other\"] &lt;- NA\n\nstroke1$bmi &lt;- round(as.numeric(stroke1$bmi), 2)\n\nstroke1$gender[stroke1$gender == \"Male\"] &lt;- 1\nstroke1$gender[stroke1$gender == \"Female\"] &lt;- 2\nstroke1$gender &lt;- as.numeric(stroke1$gender)\n\nWarning: NAs introduced by coercion\n\nstroke1$ever_married[stroke1$ever_married == \"Yes\"] &lt;- 1\nstroke1$ever_married[stroke1$ever_married == \"No\"] &lt;- 2\nstroke1$ever_married &lt;- as.numeric(stroke1$ever_married)\n\nstroke1$work_type[stroke1$work_type == \"Govt_job\"] &lt;- 1\nstroke1$work_type[stroke1$work_type == \"Private\"] &lt;- 2\nstroke1$work_type[stroke1$work_type == \"Self-employed\"] &lt;- 3\nstroke1$work_type[stroke1$work_type == \"Never_worked\"] &lt;- 4\nstroke1$work_type &lt;- as.numeric(stroke1$work_type)\n\nstroke1$Residence_type[stroke1$Residence_type == \"Urban\"] &lt;- 1\nstroke1$Residence_type[stroke1$Residence_type == \"Rural\"] &lt;- 2\nstroke1$Residence_type &lt;- as.numeric(stroke1$Residence_type)\n\nstroke1$avg_glucose_level &lt;- as.numeric(stroke1$avg_glucose_level)\n\nstroke1$heart_disease &lt;- as.numeric(stroke1$heart_disease)\n\nstroke1$hypertension &lt;- as.numeric(stroke1$hypertension)\n\nstroke1$age &lt;- round(as.numeric(stroke1$age), 2)\n\nstroke1$stroke &lt;- as.numeric(stroke1$stroke)\n\nstroke1$smoking_status[stroke1$smoking_status == \"never smoked\"] &lt;- 1\nstroke1$smoking_status[stroke1$smoking_status == \"formerly smoked\"] &lt;- 2\nstroke1$smoking_status[stroke1$smoking_status == \"smokes\"] &lt;- 3\nstroke1$smoking_status &lt;- as.numeric(stroke1$smoking_status)\n\nstroke1 &lt;- stroke1[, !(names(stroke1) %in% \"id\")]\nstroke1_clean &lt;- na.omit(stroke1)\n\nconverted all columns to numeric and removed id\n\n# converted all columns to numeric and removed id\nstr(stroke1_clean)\nnrow(stroke1_clean)\nLR_stroke1 &lt;- stroke1_clean\nstr(LR_stroke1)\ncount_tables &lt;- lapply(LR_stroke1, table)\ncount_tables"
  },
  {
    "objectID": "posts/renan-blog-post-week8/Stroke_Results_10182025_SW.html#apply-logistic-regression",
    "href": "posts/renan-blog-post-week8/Stroke_Results_10182025_SW.html#apply-logistic-regression",
    "title": "Reproducing Steve’s Code - Stroke_results_1019_2025_latest",
    "section": "2. Apply Logistic Regression",
    "text": "2. Apply Logistic Regression\nPart 2:Create and Run the Logistic Regression model from the dataset\n\n# Part 2:Create and Run the Logistic Regression model from the  dataset\nmodel &lt;- glm(stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, data=LR_stroke1, family = binomial)\nsummary(model)\n\n\nCall:\nglm(formula = stroke ~ gender + age + hypertension + heart_disease + \n    ever_married + work_type + Residence_type + avg_glucose_level + \n    bmi + smoking_status, family = binomial, data = LR_stroke1)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -8.426854   0.873243  -9.650  &lt; 2e-16 ***\ngender             0.080370   0.167274   0.480 0.630893    \nage                0.070967   0.006845  10.368  &lt; 2e-16 ***\nhypertension       0.570797   0.182580   3.126 0.001770 ** \nheart_disease      0.417884   0.220311   1.897 0.057856 .  \never_married       0.174316   0.261832   0.666 0.505569    \nwork_type         -0.109615   0.126101  -0.869 0.384703    \nResidence_type     0.005932   0.162188   0.037 0.970822    \navg_glucose_level  0.004658   0.001375   3.388 0.000704 ***\nbmi                0.006275   0.012875   0.487 0.625954    \nsmoking_status     0.179921   0.106431   1.691 0.090932 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1403.5  on 3356  degrees of freedom\nResidual deviance: 1145.4  on 3346  degrees of freedom\nAIC: 1167.4\n\nNumber of Fisher Scoring iterations: 7"
  },
  {
    "objectID": "posts/renan-blog-post-week8/Stroke_Results_10182025_SW.html#testing-logistic-regression-model-assumptions",
    "href": "posts/renan-blog-post-week8/Stroke_Results_10182025_SW.html#testing-logistic-regression-model-assumptions",
    "title": "Reproducing Steve’s Code - Stroke_results_1019_2025_latest",
    "section": "3. Testing logistic Regression Model Assumptions",
    "text": "3. Testing logistic Regression Model Assumptions\nPart 3: Testing logistic Regression Model Assumptions\nThere are several assumptions for Logistic Regression. They are:\n\nThe Dependent Variable is binary (i.e, 0 or 1)\nThere is a linear relationship between th logit of the outcome and each predictor\nThere are NO high leverage outliers in the predictors\nThere is No high multicollinearity (ie strong correlations) between predictors\n\nNow to test each assumption\n\n3.1 Testing Assumption 1\nTesting Assumption 1: The Dependent Variable is binary (0 or 1)\n\nunique(LR_stroke1$stroke)\n\n[1] 1 0\n\n\n\n\n3.2 Testing Assumption 2\nTesting Assumption 2: There is a linear relationship between the outcome variable and each predictor\nfirst, adjust all predictors so all values are positive\nConclusion: For all continuous variables , ageadj, avg_glucose_leveladj, and bmiadj, the residual plots show linearity\nConclusion: all the other predictors are categorical, with the magenta line flat, and the values clustering around certain values, they are also appropriate for logistic regression\nConclusion for assumption 2 - Linearity is met\n\nLR_stroke1$genderadj &lt;- LR_stroke1$gender + abs(min(LR_stroke1$gender)) + 1\n\nLR_stroke1$ageadj &lt;- LR_stroke1$age + abs(min(LR_stroke1$age)) + 1\n\nLR_stroke1$hypertensionadj &lt;- LR_stroke1$hypertension + abs(min(LR_stroke1$hypertension)) + 1\n\nLR_stroke1$heart_diseaseadj &lt;- LR_stroke1$heart_disease + abs(min(LR_stroke1$hypertension)) + 1\n\nLR_stroke1$ever_marriedadj &lt;- LR_stroke1$ever_married + abs(min(LR_stroke1$ever_married)) + 1\n\nLR_stroke1$work_typeadj &lt;- LR_stroke1$work_type + abs(min(LR_stroke1$work_type)) + 1\n\nLR_stroke1$Residence_typeadj &lt;- LR_stroke1$Residence_type + abs(min(LR_stroke1$Residence_type)) + 1\n\nLR_stroke1$avg_glucose_leveladj &lt;- LR_stroke1$avg_glucose_level + abs(min(LR_stroke1$avg_glucose_level)) + 1\n\nLR_stroke1$bmiadj &lt;- LR_stroke1$bmi + abs(min(LR_stroke1$bmi)) + 1\n\nLR_stroke1$smoking_statusadj &lt;- LR_stroke1$smoking_status + abs(min(LR_stroke1$smoking_status)) + 1\n\nstr(LR_stroke1)\n\ntibble [3,357 × 21] (S3: tbl_df/tbl/data.frame)\n $ gender              : num [1:3357] 1 1 2 2 1 1 2 2 2 2 ...\n $ age                 : num [1:3357] 67 80 49 79 81 74 69 81 61 54 ...\n $ hypertension        : num [1:3357] 0 0 0 1 0 1 0 1 0 0 ...\n $ heart_disease       : num [1:3357] 1 1 0 0 0 1 0 0 1 0 ...\n $ ever_married        : num [1:3357] 1 1 1 1 1 1 2 1 1 1 ...\n $ work_type           : num [1:3357] 2 2 2 3 2 2 2 2 1 2 ...\n $ Residence_type      : num [1:3357] 1 2 1 2 1 2 1 2 2 1 ...\n $ avg_glucose_level   : num [1:3357] 229 106 171 174 186 ...\n $ bmi                 : num [1:3357] 36.6 32.5 34.4 24 29 27.4 22.8 29.7 36.8 27.3 ...\n $ smoking_status      : num [1:3357] 2 1 3 1 2 1 1 1 3 3 ...\n $ stroke              : num [1:3357] 1 1 1 1 1 1 1 1 1 1 ...\n $ genderadj           : num [1:3357] 3 3 4 4 3 3 4 4 4 4 ...\n $ ageadj              : num [1:3357] 81 94 63 93 95 88 83 95 75 68 ...\n $ hypertensionadj     : num [1:3357] 1 1 1 2 1 2 1 2 1 1 ...\n $ heart_diseaseadj    : num [1:3357] 2 2 1 1 1 2 1 1 2 1 ...\n $ ever_marriedadj     : num [1:3357] 3 3 3 3 3 3 4 3 3 3 ...\n $ work_typeadj        : num [1:3357] 4 4 4 5 4 4 4 4 3 4 ...\n $ Residence_typeadj   : num [1:3357] 3 4 3 4 3 4 3 4 4 3 ...\n $ avg_glucose_leveladj: num [1:3357] 285 162 227 230 242 ...\n $ bmiadj              : num [1:3357] 49.1 45 46.9 36.5 41.5 39.9 35.3 42.2 49.3 39.8 ...\n $ smoking_statusadj   : num [1:3357] 4 3 5 3 4 3 3 3 5 5 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:1753] 2 9 10 14 20 24 28 30 32 39 ...\n  ..- attr(*, \"names\")= chr [1:1753] \"2\" \"9\" \"10\" \"14\" ...\n\nStrokeAdj &lt;- LR_stroke1\n\nStrokeAdj &lt;- StrokeAdj[ , !(names(StrokeAdj) %in% c(\"gender\", \"age\", \"hypertension\", \"heart_disease\", \"ever_married\", \"work_type\", \"Residence_type\", \"avg_glucose_level\", \"bmi\", \"smoking_status\")) ]\n\nFit the model\n\nmod.2 &lt;- glm(stroke ~ genderadj + ageadj + hypertensionadj + heart_diseaseadj + ever_marriedadj + work_typeadj + Residence_typeadj + avg_glucose_leveladj + bmiadj + smoking_statusadj, data=StrokeAdj, family=binomial)\n\n\n# Plot Residuals\nresidualPlots(mod.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                     Test stat Pr(&gt;|Test stat|)  \ngenderadj               0.0000          1.00000  \nageadj                  2.0626          0.15095  \nhypertensionadj         0.0000          1.00000  \nheart_diseaseadj        0.0000          1.00000  \never_marriedadj         0.0000          1.00000  \nwork_typeadj            3.1406          0.07636 .\nResidence_typeadj       0.0000          1.00000  \navg_glucose_leveladj    0.0103          0.91921  \nbmiadj                  0.3947          0.52983  \nsmoking_statusadj       0.4775          0.48953  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n3.3 Testing Assumption 3\nTesting Assumption 3: assess influential outliers using car package and influencePlot\n\n# Where the Object Stroke.2 comes from\n# alias(Stroke.2)\nalias(model)\n\nModel :\nstroke ~ gender + age + hypertension + heart_disease + ever_married + \n    work_type + Residence_type + avg_glucose_level + bmi + smoking_status"
  },
  {
    "objectID": "posts/renan-blog-post-week8/Stroke_Results_10182025_SW.html#error---object-not-found",
    "href": "posts/renan-blog-post-week8/Stroke_Results_10182025_SW.html#error---object-not-found",
    "title": "Reproducing Steve’s Code - Stroke_results_1019_2025_latest",
    "section": "Error - Object not found",
    "text": "Error - Object not found\nError: object ‘Stroke.2’ not found. Where is it coming from ??"
  },
  {
    "objectID": "posts/renan-blog-post-week8/Stroke_Results_10182025_SW.html#analysis-of-the-model",
    "href": "posts/renan-blog-post-week8/Stroke_Results_10182025_SW.html#analysis-of-the-model",
    "title": "Reproducing Steve’s Code - Stroke_results_1019_2025_latest",
    "section": "4 Analysis of the Model",
    "text": "4 Analysis of the Model\nPart 4: Analysis of the Model\nThere are 2 issues with the model. Fit and Predictive Capability\n\n4.1 Use Hosmer-lemesho and Naglekerke R\nPart 1 fit. Use Hosmer-lemesho and Naglekerke R for non technical audience\n\n# install.packages(\"ResourceSelection\")\n# library(ResourceSelection)\nhoslem.test(model$y, fitted(model), g = 10)\n\n\n    Hosmer and Lemeshow goodness of fit (GOF) test\n\ndata:  model$y, fitted(model)\nX-squared = 5.2704, df = 8, p-value = 0.7283\n\n\n\n# install.packages(\"rcompanion\")\n# library(rcompanion)\nnagelkerke(model)\n\n$Models\n                                                                                                                                                                               \nModel: \"glm, stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, binomial, LR_stroke1\"\nNull:  \"glm, stroke ~ 1, binomial, LR_stroke1\"                                                                                                                                 \n\n$Pseudo.R.squared.for.model.vs.null\n                             Pseudo.R.squared\nMcFadden                            0.1838790\nCox and Snell (ML)                  0.0739944\nNagelkerke (Cragg and Uhler)        0.2165560\n\n$Likelihood.ratio.test\n Df.diff LogLik.diff  Chisq    p.value\n     -10     -129.03 258.07 1.0892e-49\n\n$Number.of.observations\n           \nModel: 3357\nNull:  3357\n\n$Messages\n[1] \"Note: For models fit with REML, these statistics are based on refitting with ML\"\n\n$Warnings\n[1] \"None\"\n\n\n\n\n4.2 Predictive Capability\nPart 2 - Predictive Capability\n\n# install.packages(\"pROC\")\n# library(pROC)\nprobs &lt;- predict(model, type = \"response\")\nroc_obj &lt;- roc(LR_stroke1$stroke, probs)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\nauc(roc_obj)\n\nArea under the curve: 0.8285\n\n\nPredict AUC cross validation\n\n\n\n\n\n\nNeed to implement AUC cross validation\n\n\n\nCould not understand yet how to implement the AUC cross validation\n\n\n\n# Predict AUC cross validation\n# install.packages(\"cvAUC\")\n# library(cvAUC)\n\nConfusion Matrix\n\n# Confusion Matrix\nLR_stroke1$gender &lt;- factor(LR_stroke1$gender)\nLR_stroke1$hypertension &lt;- factor(LR_stroke1$hypertension)\nLR_stroke1$heart_disease &lt;- factor(LR_stroke1$heart_disease)\nLR_stroke1$ever_married &lt;- factor(LR_stroke1$ever_married)\nLR_stroke1$work_type &lt;- factor(LR_stroke1$work_type)\nLR_stroke1$Residence_type &lt;- factor(LR_stroke1$Residence_type)\nLR_stroke1$smoking_status &lt;- factor(LR_stroke1$smoking_status)\nLR_stroke1$stroke &lt;- factor(LR_stroke1$stroke)\n\nfit logistic regression model\n\n# fit logistic regression model\nmodel_CM &lt;- glm(stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, data=LR_stroke1, family = binomial)\n\nGet Predicted Probabilities for each observation\n\n# Get Predicted Probabilities for each observation\npred_prob &lt;- predict(model_CM, type = \"response\")\n\nAt threshold of around 0.5\n\n# Classify prediction using a threshold (0.5 is common but can adjust)\n# at threshold of 0.5\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.5, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 3176  179\n         1    1    1\n                                          \n               Accuracy : 0.9464          \n                 95% CI : (0.9382, 0.9538)\n    No Information Rate : 0.9464          \n    P-Value [Acc &gt; NIR] : 0.5198          \n                                          \n                  Kappa : 0.0098          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.0055556       \n            Specificity : 0.9996852       \n         Pos Pred Value : 0.5000000       \n         Neg Pred Value : 0.9466468       \n             Prevalence : 0.0536193       \n         Detection Rate : 0.0002979       \n   Detection Prevalence : 0.0005958       \n      Balanced Accuracy : 0.5026204       \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nIF 1 row is all 0’s then model doesn’t show any predictability\nAt threshold of 0.3\n\n# at threshold of 0.3\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.3, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 3140  164\n         1   37   16\n                                          \n               Accuracy : 0.9401          \n                 95% CI : (0.9316, 0.9479)\n    No Information Rate : 0.9464          \n    P-Value [Acc &gt; NIR] : 0.9483          \n                                          \n                  Kappa : 0.1158          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.088889        \n            Specificity : 0.988354        \n         Pos Pred Value : 0.301887        \n         Neg Pred Value : 0.950363        \n             Prevalence : 0.053619        \n         Detection Rate : 0.004766        \n   Detection Prevalence : 0.015788        \n      Balanced Accuracy : 0.538621        \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nIF 1 row is all 0’s then model doesn’t show any predictability\nAt threshold of 0.2\n\n# at threshold of 0.2\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.2, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 3040  134\n         1  137   46\n                                          \n               Accuracy : 0.9193          \n                 95% CI : (0.9095, 0.9283)\n    No Information Rate : 0.9464          \n    P-Value [Acc &gt; NIR] : 1.0000          \n                                          \n                  Kappa : 0.2108          \n                                          \n Mcnemar's Test P-Value : 0.9033          \n                                          \n            Sensitivity : 0.25556         \n            Specificity : 0.95688         \n         Pos Pred Value : 0.25137         \n         Neg Pred Value : 0.95778         \n             Prevalence : 0.05362         \n         Detection Rate : 0.01370         \n   Detection Prevalence : 0.05451         \n      Balanced Accuracy : 0.60622         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nIF 1 row is all 0’s then model doesn’t show any predictability\nAt threshold of 0.18\n\n# At Threshold of 0.18\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.18, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 3003  126\n         1  174   54\n                                          \n               Accuracy : 0.9106          \n                 95% CI : (0.9005, 0.9201)\n    No Information Rate : 0.9464          \n    P-Value [Acc &gt; NIR] : 1.000000        \n                                          \n                  Kappa : 0.2178          \n                                          \n Mcnemar's Test P-Value : 0.006657        \n                                          \n            Sensitivity : 0.30000         \n            Specificity : 0.94523         \n         Pos Pred Value : 0.23684         \n         Neg Pred Value : 0.95973         \n             Prevalence : 0.05362         \n         Detection Rate : 0.01609         \n   Detection Prevalence : 0.06792         \n      Balanced Accuracy : 0.62262         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nAt threshold of 0.15\n\n# At threshold of 0.15\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.15, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 2917  102\n         1  260   78\n                                          \n               Accuracy : 0.8922          \n                 95% CI : (0.8812, 0.9025)\n    No Information Rate : 0.9464          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.2486          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.43333         \n            Specificity : 0.91816         \n         Pos Pred Value : 0.23077         \n         Neg Pred Value : 0.96621         \n             Prevalence : 0.05362         \n         Detection Rate : 0.02324         \n   Detection Prevalence : 0.10069         \n      Balanced Accuracy : 0.67575         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nat threshold of 0.6 that starts the models predictability\nAt threshold of 0.1\n\n# at threshold of 0.1#\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.1, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 2683   69\n         1  494  111\n                                          \n               Accuracy : 0.8323          \n                 95% CI : (0.8192, 0.8448)\n    No Information Rate : 0.9464          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.2182          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.61667         \n            Specificity : 0.84451         \n         Pos Pred Value : 0.18347         \n         Neg Pred Value : 0.97493         \n             Prevalence : 0.05362         \n         Detection Rate : 0.03307         \n   Detection Prevalence : 0.18022         \n      Balanced Accuracy : 0.73059         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nAt threshold of 0.05\n\n# at threshold of 0.05#\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.05, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 2256   37\n         1  921  143\n                                         \n               Accuracy : 0.7146         \n                 95% CI : (0.699, 0.7299)\n    No Information Rate : 0.9464         \n    P-Value [Acc &gt; NIR] : 1              \n                                         \n                  Kappa : 0.1521         \n                                         \n Mcnemar's Test P-Value : &lt;2e-16         \n                                         \n            Sensitivity : 0.79444        \n            Specificity : 0.71010        \n         Pos Pred Value : 0.13440        \n         Neg Pred Value : 0.98386        \n             Prevalence : 0.05362        \n         Detection Rate : 0.04260        \n   Detection Prevalence : 0.31695        \n      Balanced Accuracy : 0.75227        \n                                         \n       'Positive' Class : 1              \n                                         \n\n\nGenerate Confusion Matrix comparing model predictions to actual outcome\n\n\n\n\n\n\nConfused about this code chunk\n\n\n\nThis code serves no purpose at all\n\n\n\n#Generate Confusion Matrix comparing model predictions to actual outcome\n# install.packages(\"caret\")\n# library(caret)\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 2256   37\n         1  921  143\n                                         \n               Accuracy : 0.7146         \n                 95% CI : (0.699, 0.7299)\n    No Information Rate : 0.9464         \n    P-Value [Acc &gt; NIR] : 1              \n                                         \n                  Kappa : 0.1521         \n                                         \n Mcnemar's Test P-Value : &lt;2e-16         \n                                         \n            Sensitivity : 0.79444        \n            Specificity : 0.71010        \n         Pos Pred Value : 0.13440        \n         Neg Pred Value : 0.98386        \n             Prevalence : 0.05362        \n         Detection Rate : 0.04260        \n   Detection Prevalence : 0.31695        \n      Balanced Accuracy : 0.75227        \n                                         \n       'Positive' Class : 1              \n                                         \n\n\n\n\n\n\n\n\nError\n\n\n\nError: data and reference should be factors with the same levels.\n\n\n```{r}\nconf_matrix &lt;- confusionMatrix(pred_class, LR_stroke1, positive = \"1\")\nprint(conf_matrix)\n```\nError: data and reference should be factors with the same levels.\nSuppose you have your folds and predictions for each fold\n```{r}\n# Suppose you have your folds and predictions for each fold\n# Error: object 'folds' not found\ncvAUC::cvAUC(predictions, labels, folds = folds)\n```\n\n\n\n\n\n\nError\n\n\n\nError: object ‘folds’ not found"
  },
  {
    "objectID": "posts/renan-blog-post-week8/Stroke_Results_10182025_SW.html#original-code",
    "href": "posts/renan-blog-post-week8/Stroke_Results_10182025_SW.html#original-code",
    "title": "Reproducing Steve’s Code - Stroke_results_1019_2025_latest",
    "section": "Original Code",
    "text": "Original Code\nBelow we see the Original Code shared by Steve: Stroke_Results_1019_2025_latest_440PM_SW_For_Group.R\n```{r}\ninstall.packages(\"dplyr\")\ninstall.packages(\"car\")\ninstall.packages(\"ResourceSelection\")\nlibrary(dplyr)\nlibrary(car)\nlibrary(ResourceSelection)\nstroke1 &lt;- read.csv(\"D:\\\\stroke.csv\")\nhead(stroke1)\nnrow(stroke1)\nsummary(stroke1)\ncount_tables &lt;- lapply(stroke1, table)\ncount_tables\n#-------------------------------------------Part 1:  preparing the data---------------------------------#\n# Smoking Status - remove unknown#\n#bmi - remove N/A#\n# Work type - remove children#\n# age create numerical variable with 2 places after the decimal#\n#gender -remove other#\nstroke1[stroke1 == \"N/A\"] &lt;- NA\nstroke1[stroke1 == \"Unknown\"] &lt;- NA\nstroke1[stroke1 == \"children\"] &lt;- NA\nstroke1[stroke1 == \"other\"] &lt;- NA\nstroke1$bmi &lt;- round(as.numeric(stroke1$bmi), 2)\nstroke1$gender[stroke1$gender == \"Male\"] &lt;- 1\nstroke1$gender[stroke1$gender == \"Female\"] &lt;- 2\nstroke1$gender &lt;- as.numeric(stroke1$gender)\nstroke1$ever_married[stroke1$ever_married == \"Yes\"] &lt;- 1\nstroke1$ever_married[stroke1$ever_married == \"No\"] &lt;- 2\nstroke1$ever_married &lt;- as.numeric(stroke1$ever_married)\nstroke1$work_type[stroke1$work_type == \"Govt_job\"] &lt;- 1\nstroke1$work_type[stroke1$work_type == \"Private\"] &lt;- 2\nstroke1$work_type[stroke1$work_type == \"Self-employed\"] &lt;- 3\nstroke1$work_type[stroke1$work_type == \"Never_worked\"] &lt;- 4\nstroke1$work_type &lt;- as.numeric(stroke1$work_type)\nstroke1$Residence_type[stroke1$Residence_type == \"Urban\"] &lt;- 1\nstroke1$Residence_type[stroke1$Residence_type == \"Rural\"] &lt;- 2\nstroke1$Residence_type &lt;- as.numeric(stroke1$Residence_type)\nstroke1$avg_glucose_level &lt;- as.numeric(stroke1$avg_glucose_level)\nstroke1$heart_disease &lt;- as.numeric(stroke1$heart_disease)\nstroke1$hypertension &lt;- as.numeric(stroke1$hypertension)\nstroke1$age &lt;- round(as.numeric(stroke1$age), 2)\nstroke1$stroke &lt;- as.numeric(stroke1$stroke)\nstroke1$smoking_status[stroke1$smoking_status == \"never smoked\"] &lt;- 1\nstroke1$smoking_status[stroke1$smoking_status == \"formerly smoked\"] &lt;- 2\nstroke1$smoking_status[stroke1$smoking_status == \"smokes\"] &lt;- 3\nstroke1$smoking_status &lt;- as.numeric(stroke1$smoking_status)\nstroke1 &lt;- stroke1[, !(names(stroke1) %in% \"id\")]\nstroke1_clean &lt;- na.omit(stroke1)\n#converted all columns to numeric and removed id#\nstr(stroke1_clean)\nnrow(stroke1_clean)\nLR_stroke1 &lt;- stroke1_clean\nstr(LR_stroke1)\n#-------------------------Part 2:Create and Run the Logistic Regression model from the  dataset-------------#\nmodel &lt;- glm(stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, data=LR_stroke1, family = binomial)\nsummary(model)\n# ----------------------------------Part 3: Testing logistic Regression Model Assumptions------------------------#\n# There are several assumptions for Logistic Regression#\n# They are:#\n#(1) The Dependent Variable is binary (i.e, 0 or 1)#\n#(2) There is a linear relationship between th logit of the outcome and each predictor#\n#(3) There are NO high leverage outliers in the predictors#\n#(4) There is No high multicollinearity (ie strong correlations) between predictors#\n####################################:Now to test each assumption: ################\n# Testing Assumption 1: The Dependent Variable is binary (0 or 1)#\nunique(LR_stroke1$stroke)\n#Testing Assumption 2: There is a linear relationship between the outcome variable and each predictor\n#first,  adjust all predictors so all values are positive#\nLR_stroke1$genderadj &lt;- LR_stroke1$gender + abs(min(LR_stroke1$gender)) + 1\nLR_stroke1$ageadj &lt;- LR_stroke1$age + abs(min(LR_stroke1$age)) + 1\nLR_stroke1$hypertensionadj &lt;- LR_stroke1$hypertension + abs(min(LR_stroke1$hypertension)) + 1\nLR_stroke1$heart_diseaseadj &lt;- LR_stroke1$heart_disease + abs(min(LR_stroke1$hypertension)) + 1\nLR_stroke1$ever_marriedadj &lt;- LR_stroke1$ever_married + abs(min(LR_stroke1$ever_married)) + 1\nLR_stroke1$work_typeadj &lt;- LR_stroke1$work_type + abs(min(LR_stroke1$work_type)) + 1\nLR_stroke1$Residence_typeadj &lt;- LR_stroke1$Residence_type + abs(min(LR_stroke1$Residence_type)) + 1\nLR_stroke1$avg_glucose_leveladj &lt;- LR_stroke1$avg_glucose_level + abs(min(LR_stroke1$avg_glucose_level)) + 1\nLR_stroke1$bmiadj &lt;- LR_stroke1$bmi + abs(min(LR_stroke1$bmi)) + 1\nLR_stroke1$smoking_statusadj &lt;- LR_stroke1$smoking_status + abs(min(LR_stroke1$smoking_status)) + 1\nstr(LR_stroke1)\nStrokeAdj &lt;- LR_stroke1\nStrokeAdj &lt;- StrokeAdj[ , !(names(StrokeAdj) %in% c(\"gender\", \"age\", \"hypertension\", \"heart_disease\", \"ever_married\", \"work_type\", \"Residence_type\", \"avg_glucose_level\", \"bmi\", \"smoking_status\")) ]\nmod.2 &lt;- glm(stroke ~ genderadj + ageadj + hypertensionadj + heart_diseaseadj + ever_marriedadj + work_typeadj + Residence_typeadj + avg_glucose_leveladj + bmiadj + smoking_statusadj, data=StrokeAdj, family=binomial)\nresidualPlots(mod.2)\n# Conclusion: For all continuous variables , ageadj, avg_glucose_leveladj, and bniadj, the residual plots show linearity#\n# Conclusion:all the other predictors are categorical, with the magenta line flat, and the values clustering around certain values, they are also appropriate for logistic regression#\n# ---------------------------------------Conclusion for assumption 2 - Linearity is met--------------------------------#\n# Testing Assumption 3: assess influential outliers using car package and influencePlot#\nalias(Stroke.2)\ninstall.packages(\"Hmisc\")\nlibrary(Hmisc)\nrcorr(as.matrix(LR_stroke1))\ninfluencePlot(model)\n# Cooks D ranges from 0 to .0122 While the ideal size is 4/N (4/3357 = 0.012), its far outside the danger zone of .5\n#--------------------------------- Conclusion: Assumption 3 is met - No substantial outliers---------------------#\n# Testing Assumption 4 : Multicollinearity using vif in the care package#\nvif(model)\n#--------------------Conclusion. All vif values are below  5 or 10. Ideally most values should be around 1. Range for all#\n# the predictors is between: 1.01 - 1.21. Way below the danger threshold of 5 to 10. Conclusion: No Multicollinearity####\n#####################################################################################################################\n# -----------------Final Conclusion: All4 assumptions are met, logistic regression is a valid model---------------#\n####################################################################################################################\n# ---------------------------------------------Part 4: Analysis of the Model----------------------------------------#\n# -----------------------There are 2 issues with the model. Fit and Predictive Capability---------------------------#\n# ----------------Part 1  fit. Use Hosmer-lemesho and Naglekerke R for non technical audience-------------------#\nhoslem.test(model$y, fitted(model), g = 10)\ninstall.packages(\"rcompanion\")\nlibrary(rcompanion)\nnagelkerke(model)\n# -----------------------Part 2 - Predictive Capability-----------------------------------------------------------\ninstall.packages(\"pROC\")\nlibrary(pROC)\nprobs &lt;- predict(model, type = \"response\")\nroc_obj &lt;- roc(LR_stroke1$stroke, probs)\nauc(roc_obj)\n# Predictict AUC cross validation\ninstall.packages(\"cvAUC\")\nlibrary(cvAUC)\n# Confusion Matrix\nLR_stroke1$gender &lt;- factor(LR_stroke1$gender)\nLR_stroke1$hypertension &lt;- factor(LR_stroke1$hypertension)\nLR_stroke1$heart_disease &lt;- factor(LR_stroke1$heart_disease)\nLR_stroke1$ever_married &lt;- factor(LR_stroke1$ever_married)\nLR_stroke1$work_type &lt;- factor(LR_stroke1$work_type)\nLR_stroke1$Residence_type &lt;- factor(LR_stroke1$Residence_type)\nLR_stroke1$smoking_status &lt;- factor(LR_stroke1$smoking_status)\nLR_stroke1$stroke &lt;- factor(LR_stroke1$stroke)\n# fit logistic regression model\nmodel_CM &lt;- glm(stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, data=LR_stroke1, family = binomial)\n# Get Predicted Probabilities for each observation\npred_prob &lt;- predict(model_CM, type = \"response\")\ninstall.packages(\"caret\")\nlibrary(caret)\n#Classify prediction using a threshold (0.5 is common but can adjust)\n# art threshold of 0.5#\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.5, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix)\n# at threshold of 0.3#\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.3, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix)\n# at threshold of 0.2\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.2, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix)\n# At Threshold of 0.18#\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.18, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix)\n# At threshold of 0.15#\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.15, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix)\n\n# at threshold of 0.1#\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.1, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix)\n# at threshold of 0.05#\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.05, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix)\n\n\n\n#Generate Confusion Matrix comparing model predictions to actual outcome\ninstall.packages(\"caret\")\nlibrary(caret)\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix)\n\n\n\n\n\n\nconf_matrix &lt;- confusionMatrix(pred_class, LR_stroke1, positive = \"1\")\nprint(conf_matrix)\n\n\n\n\n# Suppose you have your folds and predictions for each fold\ncvAUC::cvAUC(predictions, labels, folds = folds)\n\n```\n\nReferences"
  },
  {
    "objectID": "posts/renan-blog-post-week8/index.html",
    "href": "posts/renan-blog-post-week8/index.html",
    "title": "Reproducing Steve’s Code - Week 8",
    "section": "",
    "text": "For the Week 8 we will continue to reproduce Steve’s findings with the dataset[1].\nYou can download the Dataset from the following link: Stroke Prediction Dataset"
  },
  {
    "objectID": "posts/renan-blog-post-week8/index.html#setup-and-data-loading",
    "href": "posts/renan-blog-post-week8/index.html#setup-and-data-loading",
    "title": "Reproducing Steve’s Code - Week 8",
    "section": "1. Setup and Data Loading",
    "text": "1. Setup and Data Loading\nFirst we need to install all packages, system dependencies and solve conflicts to produce a new renv.lock file.\n\n1.1 Load Libraries\n\n\nCode\n# Run this once to install all the necessary packages\n# install.packages(\"dplyr\")\n# install.packages(\"car\")\n# install.packages(\"ResourceSelection\")\n# install.packages(\"caret\")\n# install.packages(\"rcompanion\")\n# install.packages(\"pROC\")\n# install.packages(\"cvAUC\")\n\n\nWe can use this to check installed packages:\n```{r}\nrenv::activate(\"website\")\n\"yardstick\" %in% rownames(installed.packages())\n```\n\n\nCode\n# For data manipulation and visualization\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(knitr)\nlibrary(ggpubr)\n\n# For data preprocessing and modeling\nlibrary(mice)\nlibrary(ROSE) # For SMOTE\nlibrary(ranger) # A fast implementation of random forests\n\n# For stacking/ensemble models\nlibrary(stacks)\nlibrary(tidymodels)\n\nlibrary(themis)\nlibrary(gghighlight)\n\nlibrary(pscl)\nlibrary(dplyr)\nlibrary(car)\nlibrary(ResourceSelection)\nlibrary(caret)\nlibrary(rcompanion)\nlibrary(Hmisc)\nlibrary(pROC)\nlibrary(cvAUC)\n\n# Set seed for reproducibility\nset.seed(123)\n\n\n\n\n\n\n\n\nPossible Dependencies Conflict\n\n\n\nNeed to further analyse if there are conflicts and System Dependency issues.\n\n\n\n\n1.2 Load Data\nWill be using my original Dataset as well Steve’s Dataset and compare for differences.\nRenan: kaggle_data1 Steve: stroke1\n\n1.2.1 Renan Dataset\nBelow will be loading the healthcare-dataset-stroke-data.csv and performing necessary changes to the dataset and loading into the DataFrame: kaggle_data1\n\n\nCode\nfind_git_root &lt;- function(start = getwd()) {\n  path &lt;- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path &lt;- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root &lt;- find_git_root()\ndatasets_path &lt;- file.path(repo_root, \"datasets\")\nkaggle_dataset_path &lt;- file.path(datasets_path, \"kaggle-healthcare-dataset-stroke-data/healthcare-dataset-stroke-data.csv\")\nkaggle_data1 = read_csv(kaggle_dataset_path, show_col_types = FALSE)\n\n# unique(kaggle_data1$bmi)\nkaggle_data1 &lt;- kaggle_data1 %&gt;%\n  mutate(bmi = na_if(bmi, \"N/A\")) %&gt;%   # Convert \"N/A\" string to NA\n  mutate(bmi = as.numeric(bmi))         # Convert from character to numeric\n\n# Remove the 'Other' gender row and the 'id' column\nkaggle_data1 &lt;- kaggle_data1 %&gt;%\n  filter(gender != \"Other\") %&gt;%\n  select(-id) %&gt;%\n  mutate_if(is.character, as.factor) # Convert character columns to factors for easier modeling\n\n\n\n\n1.2.1 Steve Dataset\nBelow will be loading the stroke.csv and performing necessary changes to the dataset and loading into the DataFrame: stroke1\n\n\nCode\n# Reading the datafile in (the same one you got for us Renan)#\nsteve_dataset_path &lt;- file.path(datasets_path, \"steve/stroke.csv\")\nstroke1 = read_csv(steve_dataset_path, show_col_types = FALSE)\n\n\n\n\n\n1.3 Prepare Dataset\nFor each Column…removing the unncessary or unusable variables:\n\nSmoking Status - remove unknown\nbmi - remove N/A\nWork type - remove children\nage create numerical variable with 2 places after the decimal\ngender -remove other\n\nExploring Dataset so we can plan on how to proceed and possible changes.\n\n\nCode\nhead(stroke1)\nnrow(stroke1)\nsummary(stroke1)\ncount_tables &lt;- lapply(stroke1, table)\ncount_tables\n\n\nIn each column..that has data points that are not usable, recoding those datapoints to become”N/A”\n\n\nCode\nstroke1[stroke1 == \"N/A\"] &lt;- NA\nstroke1[stroke1 == \"Unknown\"] &lt;- NA\nstroke1[stroke1 == \"children\"] &lt;- NA\nstroke1[stroke1 == \"other\"] &lt;- NA\n\nstroke1$bmi &lt;- round(as.numeric(stroke1$bmi), 2)\n\nstroke1$gender[stroke1$gender == \"Male\"] &lt;- 1\nstroke1$gender[stroke1$gender == \"Female\"] &lt;- 2\nstroke1$gender &lt;- as.numeric(stroke1$gender)\n\n\nWarning: NAs introduced by coercion\n\n\nCode\nstroke1$ever_married[stroke1$ever_married == \"Yes\"] &lt;- 1\nstroke1$ever_married[stroke1$ever_married == \"No\"] &lt;- 2\nstroke1$ever_married &lt;- as.numeric(stroke1$ever_married)\n\nstroke1$work_type[stroke1$work_type == \"Govt_job\"] &lt;- 1\nstroke1$work_type[stroke1$work_type == \"Private\"] &lt;- 2\nstroke1$work_type[stroke1$work_type == \"Self-employed\"] &lt;- 3\nstroke1$work_type[stroke1$work_type == \"Never_worked\"] &lt;- 4\nstroke1$work_type &lt;- as.numeric(stroke1$work_type)\n\nstroke1$Residence_type[stroke1$Residence_type == \"Urban\"] &lt;- 1\nstroke1$Residence_type[stroke1$Residence_type == \"Rural\"] &lt;- 2\nstroke1$Residence_type &lt;- as.numeric(stroke1$Residence_type)\n\nstroke1$avg_glucose_level &lt;- as.numeric(stroke1$avg_glucose_level)\n\nstroke1$heart_disease &lt;- as.numeric(stroke1$heart_disease)\n\nstroke1$hypertension &lt;- as.numeric(stroke1$hypertension)\n\nstroke1$age &lt;- round(as.numeric(stroke1$age), 2)\n\nstroke1$stroke &lt;- as.numeric(stroke1$stroke)\n\nstroke1$smoking_status[stroke1$smoking_status == \"never smoked\"] &lt;- 1\nstroke1$smoking_status[stroke1$smoking_status == \"formerly smoked\"] &lt;- 2\nstroke1$smoking_status[stroke1$smoking_status == \"smokes\"] &lt;- 3\nstroke1$smoking_status &lt;- as.numeric(stroke1$smoking_status)\n\nstroke1 &lt;- stroke1[, !(names(stroke1) %in% \"id\")]\nstroke1_clean &lt;- na.omit(stroke1)\n\n\nconverted all columns to numeric and removed id\n\n# converted all columns to numeric and removed id\nstr(stroke1_clean)\nnrow(stroke1_clean)\nLR_stroke1 &lt;- stroke1_clean\nstr(LR_stroke1)\ncount_tables &lt;- lapply(LR_stroke1, table)\ncount_tables"
  },
  {
    "objectID": "posts/renan-blog-post-week8/index.html#apply-logistic-regression",
    "href": "posts/renan-blog-post-week8/index.html#apply-logistic-regression",
    "title": "Reproducing Steve’s Code - Week 8",
    "section": "2. Apply Logistic Regression",
    "text": "2. Apply Logistic Regression\nPart 2:Create and Run the Logistic Regression model from the dataset\n\n# Part 2:Create and Run the Logistic Regression model from the  dataset\nmodel &lt;- glm(stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, data=LR_stroke1, family = binomial)\nsummary(model)\n\n\nCall:\nglm(formula = stroke ~ gender + age + hypertension + heart_disease + \n    ever_married + work_type + Residence_type + avg_glucose_level + \n    bmi + smoking_status, family = binomial, data = LR_stroke1)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -8.426854   0.873243  -9.650  &lt; 2e-16 ***\ngender             0.080370   0.167274   0.480 0.630893    \nage                0.070967   0.006845  10.368  &lt; 2e-16 ***\nhypertension       0.570797   0.182580   3.126 0.001770 ** \nheart_disease      0.417884   0.220311   1.897 0.057856 .  \never_married       0.174316   0.261832   0.666 0.505569    \nwork_type         -0.109615   0.126101  -0.869 0.384703    \nResidence_type     0.005932   0.162188   0.037 0.970822    \navg_glucose_level  0.004658   0.001375   3.388 0.000704 ***\nbmi                0.006275   0.012875   0.487 0.625954    \nsmoking_status     0.179921   0.106431   1.691 0.090932 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1403.5  on 3356  degrees of freedom\nResidual deviance: 1145.4  on 3346  degrees of freedom\nAIC: 1167.4\n\nNumber of Fisher Scoring iterations: 7\n\n\n\n\n\n\n\n\nIssue - Unbalanced Data set\n\n\n\nThe dataset is oversampling stroke rate by 77%.\n\n\nIssue - Unbalanced Data set\nThe \\(stroke rate = 180/3357 = 054%\\) . But the stroke rate in the US is 3.1% by the CDC, AHA, and the NCHS.\nThe dataset is oversampling stroke rate by 77%. We can either SMOTE, under/over sample….but too much time\nBest way is to recalibrate the intercept for this model (Ie change it here) so it can be used from now on\n\n# Issue _ Unbalanced Data set #\n# The stroke rate = 180/3357 = 054%. But the stroke rate in the US is 3.1% by the CDC, AHA, and the NCHS. #\n# The dataset is oversampling stroke rate by 77%. We can either SMOTE, under/over sample....but too much time #\n# Best way is to recalibrate the intercept for this model (Ie change it here) so it can be used from now on #\nds_prev &lt;- .054\npop_prev &lt;- .031\nlog_odds_ds &lt;- qlogis(ds_prev)\nlog_odds_pop &lt;- qlogis(pop_prev)\noffset &lt;- log_odds_pop - log_odds_ds\ncoefs &lt;- coef(model)\ncoefs[1] &lt;- coefs[1] + offset\nprint(coefs)\n\n      (Intercept)            gender               age      hypertension \n     -9.005873116       0.080369937       0.070967479       0.570796782 \n    heart_disease      ever_married         work_type    Residence_type \n      0.417883562       0.174315603      -0.109615162       0.005932360 \navg_glucose_level               bmi    smoking_status \n      0.004657820       0.006275415       0.179921439 \n\n\nOriginal Intercept Coeff = -8.426854231\nChanged intercept Coefficent to take into account current stroke rate or 3.1% = -9.005873116\nall the other intercepts remain the same"
  },
  {
    "objectID": "posts/renan-blog-post-week8/index.html#testing-logistic-regression-model-assumptions",
    "href": "posts/renan-blog-post-week8/index.html#testing-logistic-regression-model-assumptions",
    "title": "Reproducing Steve’s Code - Week 8",
    "section": "3. Testing logistic Regression Model Assumptions",
    "text": "3. Testing logistic Regression Model Assumptions\nPart 3: Testing logistic Regression Model Assumptions\nThere are several assumptions for Logistic Regression. They are:\n\nThe Dependent Variable is binary (i.e, 0 or 1)\nThere is a linear relationship between th logit of the outcome and each predictor\nThere are NO high leverage outliers in the predictors\nThere is No high multicollinearity (ie strong correlations) between predictors\n\n\n3.1 Testing Assumption 1\nTesting Assumption 1: The Dependent Variable is binary (0 or 1)\n\nunique(LR_stroke1$stroke)\n\n[1] 1 0\n\n\n\n\n3.2 Testing Assumption 2\nTesting Assumption 2: There is a linear relationship between the outcome variable and each predictor\nfirst, adjust all predictors so all values are positive\nConclusion: For all continuous variables , ageadj, avg_glucose_leveladj, and bniadj, the residual plots show linearity\nConclusion: all the other predictors are categorical, with the magenta line flat, and the values clustering around certain values, they are also appropriate for logistic regression\nConclusion for assumption 2 - Linearity is met\n\nLR_stroke1$genderadj &lt;- LR_stroke1$gender + abs(min(LR_stroke1$gender)) + 1\n\nLR_stroke1$ageadj &lt;- LR_stroke1$age + abs(min(LR_stroke1$age)) + 1\n\nLR_stroke1$hypertensionadj &lt;- LR_stroke1$hypertension + abs(min(LR_stroke1$hypertension)) + 1\n\nLR_stroke1$heart_diseaseadj &lt;- LR_stroke1$heart_disease + abs(min(LR_stroke1$hypertension)) + 1\n\nLR_stroke1$ever_marriedadj &lt;- LR_stroke1$ever_married + abs(min(LR_stroke1$ever_married)) + 1\n\nLR_stroke1$work_typeadj &lt;- LR_stroke1$work_type + abs(min(LR_stroke1$work_type)) + 1\n\nLR_stroke1$Residence_typeadj &lt;- LR_stroke1$Residence_type + abs(min(LR_stroke1$Residence_type)) + 1\n\nLR_stroke1$avg_glucose_leveladj &lt;- LR_stroke1$avg_glucose_level + abs(min(LR_stroke1$avg_glucose_level)) + 1\n\nLR_stroke1$bmiadj &lt;- LR_stroke1$bmi + abs(min(LR_stroke1$bmi)) + 1\n\nLR_stroke1$smoking_statusadj &lt;- LR_stroke1$smoking_status + abs(min(LR_stroke1$smoking_status)) + 1\n\nstr(LR_stroke1)\n\ntibble [3,357 × 21] (S3: tbl_df/tbl/data.frame)\n $ gender              : num [1:3357] 1 1 2 2 1 1 2 2 2 2 ...\n $ age                 : num [1:3357] 67 80 49 79 81 74 69 81 61 54 ...\n $ hypertension        : num [1:3357] 0 0 0 1 0 1 0 1 0 0 ...\n $ heart_disease       : num [1:3357] 1 1 0 0 0 1 0 0 1 0 ...\n $ ever_married        : num [1:3357] 1 1 1 1 1 1 2 1 1 1 ...\n $ work_type           : num [1:3357] 2 2 2 3 2 2 2 2 1 2 ...\n $ Residence_type      : num [1:3357] 1 2 1 2 1 2 1 2 2 1 ...\n $ avg_glucose_level   : num [1:3357] 229 106 171 174 186 ...\n $ bmi                 : num [1:3357] 36.6 32.5 34.4 24 29 27.4 22.8 29.7 36.8 27.3 ...\n $ smoking_status      : num [1:3357] 2 1 3 1 2 1 1 1 3 3 ...\n $ stroke              : num [1:3357] 1 1 1 1 1 1 1 1 1 1 ...\n $ genderadj           : num [1:3357] 3 3 4 4 3 3 4 4 4 4 ...\n $ ageadj              : num [1:3357] 81 94 63 93 95 88 83 95 75 68 ...\n $ hypertensionadj     : num [1:3357] 1 1 1 2 1 2 1 2 1 1 ...\n $ heart_diseaseadj    : num [1:3357] 2 2 1 1 1 2 1 1 2 1 ...\n $ ever_marriedadj     : num [1:3357] 3 3 3 3 3 3 4 3 3 3 ...\n $ work_typeadj        : num [1:3357] 4 4 4 5 4 4 4 4 3 4 ...\n $ Residence_typeadj   : num [1:3357] 3 4 3 4 3 4 3 4 4 3 ...\n $ avg_glucose_leveladj: num [1:3357] 285 162 227 230 242 ...\n $ bmiadj              : num [1:3357] 49.1 45 46.9 36.5 41.5 39.9 35.3 42.2 49.3 39.8 ...\n $ smoking_statusadj   : num [1:3357] 4 3 5 3 4 3 3 3 5 5 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:1753] 2 9 10 14 20 24 28 30 32 39 ...\n  ..- attr(*, \"names\")= chr [1:1753] \"2\" \"9\" \"10\" \"14\" ...\n\nStrokeAdj &lt;- LR_stroke1\n\nStrokeAdj &lt;- StrokeAdj[ , !(names(StrokeAdj) %in% c(\"gender\", \"age\", \"hypertension\", \"heart_disease\", \"ever_married\", \"work_type\", \"Residence_type\", \"avg_glucose_level\", \"bmi\", \"smoking_status\")) ]\n\nFit the model\n\nmod.2 &lt;- glm(stroke ~ genderadj + ageadj + hypertensionadj + heart_diseaseadj + ever_marriedadj + work_typeadj + Residence_typeadj + avg_glucose_leveladj + bmiadj + smoking_statusadj, data=StrokeAdj, family=binomial)\n# Plot Residuals\nresidualPlots(mod.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                     Test stat Pr(&gt;|Test stat|)  \ngenderadj               0.0000          1.00000  \nageadj                  2.0626          0.15095  \nhypertensionadj         0.0000          1.00000  \nheart_diseaseadj        0.0000          1.00000  \never_marriedadj         0.0000          1.00000  \nwork_typeadj            3.1406          0.07636 .\nResidence_typeadj       0.0000          1.00000  \navg_glucose_leveladj    0.0103          0.91921  \nbmiadj                  0.3947          0.52983  \nsmoking_statusadj       0.4775          0.48953  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n3.3 Testing Assumption 3\nTesting Assumption 3: assess influential outliers using car package and influencePlot\n\nalias(model)\n\nModel :\nstroke ~ gender + age + hypertension + heart_disease + ever_married + \n    work_type + Residence_type + avg_glucose_level + bmi + smoking_status\n\n# install.packages(\"Hmisc\")\n# library(Hmisc)\nrcorr(as.matrix(LR_stroke1))\n\n                     gender   age hypertension heart_disease ever_married\ngender                 1.00 -0.06        -0.04         -0.10         0.03\nage                   -0.06  1.00         0.26          0.26        -0.49\nhypertension          -0.04  0.26         1.00          0.11        -0.11\nheart_disease         -0.10  0.26         0.11          1.00        -0.07\never_married           0.03 -0.49        -0.11         -0.07         1.00\nwork_type              0.01  0.14         0.05          0.03        -0.02\nResidence_type        -0.01 -0.02         0.00         -0.01         0.01\navg_glucose_level     -0.07  0.24         0.17          0.14        -0.12\nbmi                   -0.02  0.04         0.13          0.00        -0.13\nsmoking_status        -0.08  0.03        -0.01          0.06        -0.06\nstroke                -0.01  0.24         0.14          0.14        -0.07\ngenderadj              1.00 -0.06        -0.04         -0.10         0.03\nageadj                -0.06  1.00         0.26          0.26        -0.49\nhypertensionadj       -0.04  0.26         1.00          0.11        -0.11\nheart_diseaseadj      -0.10  0.26         0.11          1.00        -0.07\never_marriedadj        0.03 -0.49        -0.11         -0.07         1.00\nwork_typeadj           0.01  0.14         0.05          0.03        -0.02\nResidence_typeadj     -0.01 -0.02         0.00         -0.01         0.01\navg_glucose_leveladj  -0.07  0.24         0.17          0.14        -0.12\nbmiadj                -0.02  0.04         0.13          0.00        -0.13\nsmoking_statusadj     -0.08  0.03        -0.01          0.06        -0.06\n                     work_type Residence_type avg_glucose_level   bmi\ngender                    0.01          -0.01             -0.07 -0.02\nage                       0.14          -0.02              0.24  0.04\nhypertension              0.05           0.00              0.17  0.13\nheart_disease             0.03          -0.01              0.14  0.00\never_married             -0.02           0.01             -0.12 -0.13\nwork_type                 1.00          -0.01              0.03 -0.02\nResidence_type           -0.01           1.00              0.01  0.01\navg_glucose_level         0.03           0.01              1.00  0.16\nbmi                      -0.02           0.01              0.16  1.00\nsmoking_status           -0.02          -0.04              0.01  0.03\nstroke                    0.04          -0.01              0.14  0.01\ngenderadj                 0.01          -0.01             -0.07 -0.02\nageadj                    0.14          -0.02              0.24  0.04\nhypertensionadj           0.05           0.00              0.17  0.13\nheart_diseaseadj          0.03          -0.01              0.14  0.00\never_marriedadj          -0.02           0.01             -0.12 -0.13\nwork_typeadj              1.00          -0.01              0.03 -0.02\nResidence_typeadj        -0.01           1.00              0.01  0.01\navg_glucose_leveladj      0.03           0.01              1.00  0.16\nbmiadj                   -0.02           0.01              0.16  1.00\nsmoking_statusadj        -0.02          -0.04              0.01  0.03\n                     smoking_status stroke genderadj ageadj hypertensionadj\ngender                        -0.08  -0.01      1.00  -0.06           -0.04\nage                            0.03   0.24     -0.06   1.00            0.26\nhypertension                  -0.01   0.14     -0.04   0.26            1.00\nheart_disease                  0.06   0.14     -0.10   0.26            0.11\never_married                  -0.06  -0.07      0.03  -0.49           -0.11\nwork_type                     -0.02   0.04      0.01   0.14            0.05\nResidence_type                -0.04  -0.01     -0.01  -0.02            0.00\navg_glucose_level              0.01   0.14     -0.07   0.24            0.17\nbmi                            0.03   0.01     -0.02   0.04            0.13\nsmoking_status                 1.00   0.02     -0.08   0.03           -0.01\nstroke                         0.02   1.00     -0.01   0.24            0.14\ngenderadj                     -0.08  -0.01      1.00  -0.06           -0.04\nageadj                         0.03   0.24     -0.06   1.00            0.26\nhypertensionadj               -0.01   0.14     -0.04   0.26            1.00\nheart_diseaseadj               0.06   0.14     -0.10   0.26            0.11\never_marriedadj               -0.06  -0.07      0.03  -0.49           -0.11\nwork_typeadj                  -0.02   0.04      0.01   0.14            0.05\nResidence_typeadj             -0.04  -0.01     -0.01  -0.02            0.00\navg_glucose_leveladj           0.01   0.14     -0.07   0.24            0.17\nbmiadj                         0.03   0.01     -0.02   0.04            0.13\nsmoking_statusadj              1.00   0.02     -0.08   0.03           -0.01\n                     heart_diseaseadj ever_marriedadj work_typeadj\ngender                          -0.10            0.03         0.01\nage                              0.26           -0.49         0.14\nhypertension                     0.11           -0.11         0.05\nheart_disease                    1.00           -0.07         0.03\never_married                    -0.07            1.00        -0.02\nwork_type                        0.03           -0.02         1.00\nResidence_type                  -0.01            0.01        -0.01\navg_glucose_level                0.14           -0.12         0.03\nbmi                              0.00           -0.13        -0.02\nsmoking_status                   0.06           -0.06        -0.02\nstroke                           0.14           -0.07         0.04\ngenderadj                       -0.10            0.03         0.01\nageadj                           0.26           -0.49         0.14\nhypertensionadj                  0.11           -0.11         0.05\nheart_diseaseadj                 1.00           -0.07         0.03\never_marriedadj                 -0.07            1.00        -0.02\nwork_typeadj                     0.03           -0.02         1.00\nResidence_typeadj               -0.01            0.01        -0.01\navg_glucose_leveladj             0.14           -0.12         0.03\nbmiadj                           0.00           -0.13        -0.02\nsmoking_statusadj                0.06           -0.06        -0.02\n                     Residence_typeadj avg_glucose_leveladj bmiadj\ngender                           -0.01                -0.07  -0.02\nage                              -0.02                 0.24   0.04\nhypertension                      0.00                 0.17   0.13\nheart_disease                    -0.01                 0.14   0.00\never_married                      0.01                -0.12  -0.13\nwork_type                        -0.01                 0.03  -0.02\nResidence_type                    1.00                 0.01   0.01\navg_glucose_level                 0.01                 1.00   0.16\nbmi                               0.01                 0.16   1.00\nsmoking_status                   -0.04                 0.01   0.03\nstroke                           -0.01                 0.14   0.01\ngenderadj                        -0.01                -0.07  -0.02\nageadj                           -0.02                 0.24   0.04\nhypertensionadj                   0.00                 0.17   0.13\nheart_diseaseadj                 -0.01                 0.14   0.00\never_marriedadj                   0.01                -0.12  -0.13\nwork_typeadj                     -0.01                 0.03  -0.02\nResidence_typeadj                 1.00                 0.01   0.01\navg_glucose_leveladj              0.01                 1.00   0.16\nbmiadj                            0.01                 0.16   1.00\nsmoking_statusadj                -0.04                 0.01   0.03\n                     smoking_statusadj\ngender                           -0.08\nage                               0.03\nhypertension                     -0.01\nheart_disease                     0.06\never_married                     -0.06\nwork_type                        -0.02\nResidence_type                   -0.04\navg_glucose_level                 0.01\nbmi                               0.03\nsmoking_status                    1.00\nstroke                            0.02\ngenderadj                        -0.08\nageadj                            0.03\nhypertensionadj                  -0.01\nheart_diseaseadj                  0.06\never_marriedadj                  -0.06\nwork_typeadj                     -0.02\nResidence_typeadj                -0.04\navg_glucose_leveladj              0.01\nbmiadj                            0.03\nsmoking_statusadj                 1.00\n\nn= 3357 \n\n\nP\n                     gender age    hypertension heart_disease ever_married\ngender                      0.0012 0.0204       0.0000        0.1140      \nage                  0.0012        0.0000       0.0000        0.0000      \nhypertension         0.0204 0.0000              0.0000        0.0000      \nheart_disease        0.0000 0.0000 0.0000                     0.0000      \never_married         0.1140 0.0000 0.0000       0.0000                    \nwork_type            0.3863 0.0000 0.0051       0.0862        0.2313      \nResidence_type       0.5077 0.3076 0.8569       0.5527        0.5150      \navg_glucose_level    0.0000 0.0000 0.0000       0.0000        0.0000      \nbmi                  0.2487 0.0144 0.0000       0.8185        0.0000      \nsmoking_status       0.0000 0.0448 0.7537       0.0005        0.0009      \nstroke               0.4296 0.0000 0.0000       0.0000        0.0002      \ngenderadj            0.0000 0.0012 0.0204       0.0000        0.1140      \nageadj               0.0012 0.0000 0.0000       0.0000        0.0000      \nhypertensionadj      0.0204 0.0000 0.0000       0.0000        0.0000      \nheart_diseaseadj     0.0000 0.0000 0.0000       0.0000        0.0000      \never_marriedadj      0.1140 0.0000 0.0000       0.0000        0.0000      \nwork_typeadj         0.3863 0.0000 0.0051       0.0862        0.2313      \nResidence_typeadj    0.5077 0.3076 0.8569       0.5527        0.5150      \navg_glucose_leveladj 0.0000 0.0000 0.0000       0.0000        0.0000      \nbmiadj               0.2487 0.0144 0.0000       0.8185        0.0000      \nsmoking_statusadj    0.0000 0.0448 0.7537       0.0005        0.0009      \n                     work_type Residence_type avg_glucose_level bmi   \ngender               0.3863    0.5077         0.0000            0.2487\nage                  0.0000    0.3076         0.0000            0.0144\nhypertension         0.0051    0.8569         0.0000            0.0000\nheart_disease        0.0862    0.5527         0.0000            0.8185\never_married         0.2313    0.5150         0.0000            0.0000\nwork_type                      0.6768         0.0467            0.3243\nResidence_type       0.6768                   0.6427            0.5689\navg_glucose_level    0.0467    0.6427                           0.0000\nbmi                  0.3243    0.5689         0.0000                  \nsmoking_status       0.3764    0.0208         0.7679            0.0925\nstroke               0.0259    0.7171         0.0000            0.6866\ngenderadj            0.3863    0.5077         0.0000            0.2487\nageadj               0.0000    0.3076         0.0000            0.0144\nhypertensionadj      0.0051    0.8569         0.0000            0.0000\nheart_diseaseadj     0.0862    0.5527         0.0000            0.8185\never_marriedadj      0.2313    0.5150         0.0000            0.0000\nwork_typeadj         0.0000    0.6768         0.0467            0.3243\nResidence_typeadj    0.6768    0.0000         0.6427            0.5689\navg_glucose_leveladj 0.0467    0.6427         0.0000            0.0000\nbmiadj               0.3243    0.5689         0.0000            0.0000\nsmoking_statusadj    0.3764    0.0208         0.7679            0.0925\n                     smoking_status stroke genderadj ageadj hypertensionadj\ngender               0.0000         0.4296 0.0000    0.0012 0.0204         \nage                  0.0448         0.0000 0.0012    0.0000 0.0000         \nhypertension         0.7537         0.0000 0.0204    0.0000 0.0000         \nheart_disease        0.0005         0.0000 0.0000    0.0000 0.0000         \never_married         0.0009         0.0002 0.1140    0.0000 0.0000         \nwork_type            0.3764         0.0259 0.3863    0.0000 0.0051         \nResidence_type       0.0208         0.7171 0.5077    0.3076 0.8569         \navg_glucose_level    0.7679         0.0000 0.0000    0.0000 0.0000         \nbmi                  0.0925         0.6866 0.2487    0.0144 0.0000         \nsmoking_status                      0.2559 0.0000    0.0448 0.7537         \nstroke               0.2559                0.4296    0.0000 0.0000         \ngenderadj            0.0000         0.4296           0.0012 0.0204         \nageadj               0.0448         0.0000 0.0012           0.0000         \nhypertensionadj      0.7537         0.0000 0.0204    0.0000                \nheart_diseaseadj     0.0005         0.0000 0.0000    0.0000 0.0000         \never_marriedadj      0.0009         0.0002 0.1140    0.0000 0.0000         \nwork_typeadj         0.3764         0.0259 0.3863    0.0000 0.0051         \nResidence_typeadj    0.0208         0.7171 0.5077    0.3076 0.8569         \navg_glucose_leveladj 0.7679         0.0000 0.0000    0.0000 0.0000         \nbmiadj               0.0925         0.6866 0.2487    0.0144 0.0000         \nsmoking_statusadj    0.0000         0.2559 0.0000    0.0448 0.7537         \n                     heart_diseaseadj ever_marriedadj work_typeadj\ngender               0.0000           0.1140          0.3863      \nage                  0.0000           0.0000          0.0000      \nhypertension         0.0000           0.0000          0.0051      \nheart_disease        0.0000           0.0000          0.0862      \never_married         0.0000           0.0000          0.2313      \nwork_type            0.0862           0.2313          0.0000      \nResidence_type       0.5527           0.5150          0.6768      \navg_glucose_level    0.0000           0.0000          0.0467      \nbmi                  0.8185           0.0000          0.3243      \nsmoking_status       0.0005           0.0009          0.3764      \nstroke               0.0000           0.0002          0.0259      \ngenderadj            0.0000           0.1140          0.3863      \nageadj               0.0000           0.0000          0.0000      \nhypertensionadj      0.0000           0.0000          0.0051      \nheart_diseaseadj                      0.0000          0.0862      \never_marriedadj      0.0000                           0.2313      \nwork_typeadj         0.0862           0.2313                      \nResidence_typeadj    0.5527           0.5150          0.6768      \navg_glucose_leveladj 0.0000           0.0000          0.0467      \nbmiadj               0.8185           0.0000          0.3243      \nsmoking_statusadj    0.0005           0.0009          0.3764      \n                     Residence_typeadj avg_glucose_leveladj bmiadj\ngender               0.5077            0.0000               0.2487\nage                  0.3076            0.0000               0.0144\nhypertension         0.8569            0.0000               0.0000\nheart_disease        0.5527            0.0000               0.8185\never_married         0.5150            0.0000               0.0000\nwork_type            0.6768            0.0467               0.3243\nResidence_type       0.0000            0.6427               0.5689\navg_glucose_level    0.6427            0.0000               0.0000\nbmi                  0.5689            0.0000               0.0000\nsmoking_status       0.0208            0.7679               0.0925\nstroke               0.7171            0.0000               0.6866\ngenderadj            0.5077            0.0000               0.2487\nageadj               0.3076            0.0000               0.0144\nhypertensionadj      0.8569            0.0000               0.0000\nheart_diseaseadj     0.5527            0.0000               0.8185\never_marriedadj      0.5150            0.0000               0.0000\nwork_typeadj         0.6768            0.0467               0.3243\nResidence_typeadj                      0.6427               0.5689\navg_glucose_leveladj 0.6427                                 0.0000\nbmiadj               0.5689            0.0000                     \nsmoking_statusadj    0.0208            0.7679               0.0925\n                     smoking_statusadj\ngender               0.0000           \nage                  0.0448           \nhypertension         0.7537           \nheart_disease        0.0005           \never_married         0.0009           \nwork_type            0.3764           \nResidence_type       0.0208           \navg_glucose_level    0.7679           \nbmi                  0.0925           \nsmoking_status       0.0000           \nstroke               0.2559           \ngenderadj            0.0000           \nageadj               0.0448           \nhypertensionadj      0.7537           \nheart_diseaseadj     0.0005           \never_marriedadj      0.0009           \nwork_typeadj         0.3764           \nResidence_typeadj    0.0208           \navg_glucose_leveladj 0.7679           \nbmiadj               0.0925           \nsmoking_statusadj                     \n\n\n\n# install.packages(\"car\")\n# library(car)\ninfluencePlot(model)\n\n\n\n\n\n\n\n\n        StudRes          Hat       CookD\n83    2.6917353 0.0039267816 0.012237368\n84    1.5018344 0.0343771041 0.006641860\n87    3.0732709 0.0012042796 0.011476828\n131   3.1608870 0.0006013465 0.007697762\n152   3.1135601 0.0005613972 0.006237531\n2583 -0.8509399 0.0399302730 0.001668526\n\n\nCooks D ranges from 0 to .0122\nWhile the ideal size is 4/N (4/3357 = 0.012), its far outside the danger zone of .5\nConclusion: Assumption 3 is met - No substantial outliers\n\n\n3.4 Testing Assumption 4\nTesting Assumption 4 : Multicollinearity using vif in the care package\n\nvif(model)\n\n           gender               age      hypertension     heart_disease \n         1.041499          1.213552          1.051213          1.083661 \n     ever_married         work_type    Residence_type avg_glucose_level \n         1.018892          1.051698          1.006965          1.105268 \n              bmi    smoking_status \n         1.117138          1.049260 \n\n\nConclusion. All vif values are below 5 or 10. Ideally most values should be around 1. Range for all\nthe predictors is between: 1.01 - 1.21. Way below the danger threshold of 5 to 10.\nConclusion: No Multicollinearity\nFinal Conclusion: All4 assumptions are met, logistic regression is a valid model\n\n\n3.5 Conclusion of Testing Assumptions\nFinal Conclusion: All 4 assumptions are met, logistic regression is a valid model"
  },
  {
    "objectID": "posts/renan-blog-post-week8/index.html#analysis-of-the-model",
    "href": "posts/renan-blog-post-week8/index.html#analysis-of-the-model",
    "title": "Reproducing Steve’s Code - Week 8",
    "section": "4 Analysis of the Model",
    "text": "4 Analysis of the Model\nPart 4: Analysis of the Model\nThere are 2 issues with the model. Fit and Predictive Capability\n\n4.1 Use Hosmer-lemesho and Naglekerke R\nPart 1 fit. Use Hosmer-lemesho and Naglekerke R for non technical audience\n\n# install.packages(\"ResourceSelection\")\n# library(ResourceSelection)\nhoslem.test(model$y, fitted(model), g = 10)\n\n\n    Hosmer and Lemeshow goodness of fit (GOF) test\n\ndata:  model$y, fitted(model)\nX-squared = 5.2704, df = 8, p-value = 0.7283\n\n\n\n# install.packages(\"rcompanion\")\n# library(rcompanion)\nnagelkerke(model)\n\n$Models\n                                                                                                                                                                               \nModel: \"glm, stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, binomial, LR_stroke1\"\nNull:  \"glm, stroke ~ 1, binomial, LR_stroke1\"                                                                                                                                 \n\n$Pseudo.R.squared.for.model.vs.null\n                             Pseudo.R.squared\nMcFadden                            0.1838790\nCox and Snell (ML)                  0.0739944\nNagelkerke (Cragg and Uhler)        0.2165560\n\n$Likelihood.ratio.test\n Df.diff LogLik.diff  Chisq    p.value\n     -10     -129.03 258.07 1.0892e-49\n\n$Number.of.observations\n           \nModel: 3357\nNull:  3357\n\n$Messages\n[1] \"Note: For models fit with REML, these statistics are based on refitting with ML\"\n\n$Warnings\n[1] \"None\"\n\n\n\n\n4.2 Predictive Capability\nPart 2 - Predictive Capability\n\n# install.packages(\"pROC\")\n# library(pROC)\nprobs &lt;- predict(model, type = \"response\")\nroc_obj &lt;- roc(LR_stroke1$stroke, probs)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\nauc(roc_obj)\n\nArea under the curve: 0.8285\n\n\nPredict AUC cross validation\n\n\n\n\n\n\nNeed to implement AUC cross validation\n\n\n\nCould not understand yet how to implement the AUC cross validation\n\n\n\n# Predict AUC cross validation\n# install.packages(\"cvAUC\")\n# library(cvAUC)\n\nConfusion Matrix\n\n# Confusion Matrix\nLR_stroke1$gender &lt;- factor(LR_stroke1$gender)\nLR_stroke1$hypertension &lt;- factor(LR_stroke1$hypertension)\nLR_stroke1$heart_disease &lt;- factor(LR_stroke1$heart_disease)\nLR_stroke1$ever_married &lt;- factor(LR_stroke1$ever_married)\nLR_stroke1$work_type &lt;- factor(LR_stroke1$work_type)\nLR_stroke1$Residence_type &lt;- factor(LR_stroke1$Residence_type)\nLR_stroke1$smoking_status &lt;- factor(LR_stroke1$smoking_status)\nLR_stroke1$stroke &lt;- factor(LR_stroke1$stroke)\n\nfit logistic regression model\n\n# fit logistic regression model\nmodel_CM &lt;- glm(stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, data=LR_stroke1, family = binomial)\n\nGet Predicted Probabilities for each observation\n\n# Get Predicted Probabilities for each observation\npred_prob &lt;- predict(model_CM, type = \"response\")\n\ncreate 10 confusion matrices at threshold intervals between 1 and 0 to create a ROC\nClassify prediction using a threshold (0.5 is common but can adjust)\nIF 1 row is all 0’s then model doesn’t show any predictability\nAt threshold of around 1.0\n\n# create 10 confusion matrices at threshold intervals between 1 and 0 to create a ROC\n# Classify prediction using a threshold (0.5 is common but can adjust)\n# IF 1 row is all 0's then model doesn't show any predictability\n# At threshold of around 1.0\npred_class &lt;- factor(ifelse(pred_prob &gt; .99, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix$table)\n\n          Reference\nPrediction    0    1\n         0 3177  180\n         1    0    0\n\n\nIF 1 row is all 0’s then model doesn’t show any predictability\nAt threshold of 0.9\n\n# At threshold of 0.9\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.9, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix$table)\n\n          Reference\nPrediction    0    1\n         0 3177  180\n         1    0    0\n\n\nIF 1 row is all 0’s then model doesn’t show any predictability\nAt threshold of 0.8\n\n# At threshold of 0.8\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.8, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix$table)\n\n          Reference\nPrediction    0    1\n         0 3177  180\n         1    0    0\n\n\nIF 1 row is all 0’s then model doesn’t show any predictability\nAt threshold of 0.7\n\n# At threshold of 0.7\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.7, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix$table)\n\n          Reference\nPrediction    0    1\n         0 3177  180\n         1    0    0\n\n\nAt threshold of 0.6\n\n# At threshold of 0.6\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.6, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix$table)\n\n          Reference\nPrediction    0    1\n         0 3177  179\n         1    0    1\n\n\nat threshold of 0.6 that starts the models predictability\nExtract precision,Recall and F1 from confusion matrix using the caret package\n\n# Extract precision, Recall and F1 from confusion matrix using the caret package\nprecision &lt;- conf_matrix$byClass[\"Pos Pred Value\"] \nrecall &lt;- conf_matrix$byClass[\"Sensitivity\"]\nf1 &lt;- 2 * ((precision * recall)/ (precision + recall))\nprint(sprintf(\"F1: %f\", f1))\n\n[1] \"F1: 0.011050\"\n\nprint(sprintf(\"recall: %f\", recall))\n\n[1] \"recall: 0.005556\"\n\nprint(sprintf(\"precision: %f\", precision))\n\n[1] \"precision: 1.000000\"\n\n\nat threshold of 0.5\n\n# at threshold of 0.5\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.5, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix$table)\n\n          Reference\nPrediction    0    1\n         0 3176  179\n         1    1    1\n\n\nExtract precision,Recall and F1 from confusion matrix using the caret package\n\n# Extract precision,Recall and F1 from confusion matrix using the caret package\nprecision &lt;- conf_matrix$byClass[\"Pos Pred Value\"] \nrecall &lt;- conf_matrix$byClass[\"Sensitivity\"]\nf1 &lt;- 2 * ((precision * recall)/ (precision + recall))\nprint(sprintf(\"F1: %f\", f1))\n\n[1] \"F1: 0.010989\"\n\nprint(sprintf(\"recall: %f\", recall))\n\n[1] \"recall: 0.005556\"\n\nprint(sprintf(\"precision: %f\", precision))\n\n[1] \"precision: 0.500000\"\n\n\nAt threshold of 0.4\n\n# At threshold of 0.4\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.4, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix$table)\n\n          Reference\nPrediction    0    1\n         0 3171  174\n         1    6    6\n\n\nExtract precision,Recall and F1 from confusion matrix using the caret package\n\n# Extract precision,Recall and F1 from confusion matrix using the caret package\nprecision &lt;- conf_matrix$byClass[\"Pos Pred Value\"] \nrecall &lt;- conf_matrix$byClass[\"Sensitivity\"]\nf1 &lt;- 2 * ((precision * recall)/ (precision + recall))\nprint(sprintf(\"F1: %f\", f1))\n\n[1] \"F1: 0.062500\"\n\nprint(sprintf(\"recall: %f\", recall))\n\n[1] \"recall: 0.033333\"\n\nprint(sprintf(\"precision: %f\", precision))\n\n[1] \"precision: 0.500000\"\n\n\nat threshold of 0.3\n\n# at threshold of 0.3\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.3, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix$table)\n\n          Reference\nPrediction    0    1\n         0 3140  164\n         1   37   16\n\n\nExtract precision,Recall and F1 from confusion matrix using the caret package\n\n# Extract precision,Recall and F1 from confusion matrix using the caret package\nprecision &lt;- conf_matrix$byClass[\"Pos Pred Value\"] \nrecall &lt;- conf_matrix$byClass[\"Sensitivity\"]\nf1 &lt;- 2 * ((precision * recall)/ (precision + recall))\nprint(sprintf(\"F1: %f\", f1))\n\n[1] \"F1: 0.137339\"\n\nprint(sprintf(\"recall: %f\", recall))\n\n[1] \"recall: 0.088889\"\n\nprint(sprintf(\"precision: %f\", precision))\n\n[1] \"precision: 0.301887\"\n\n\nat threshold of 0.2\n\n# at threshold of 0.2\npred_class &lt;- factor(ifelse(pred_prob &gt; 0.2, 1, 0), levels = c(0, 1))\nconf_matrix &lt;- confusionMatrix(pred_class, factor(LR_stroke1$stroke, levels = c(\"0\", \"1\")), positive = \"1\")\nprint(conf_matrix$table)\n\n          Reference\nPrediction    0    1\n         0 3040  134\n         1  137   46\n\n\nExtract precision,Recall and F1 from confusion matrix using the caret package\n\n# Extract precision,Recall and F1 from confusion matrix using the caret package\nprecision &lt;- conf_matrix$byClass[\"Pos Pred Value\"] \nrecall &lt;- conf_matrix$byClass[\"Sensitivity\"]\nf1 &lt;- 2 * ((precision * recall)/ (precision + recall))\nprint(sprintf(\"F1: %f\", f1))\n\n[1] \"F1: 0.253444\"\n\nprint(sprintf(\"recall: %f\", recall))\n\n[1] \"recall: 0.255556\"\n\nprint(sprintf(\"precision: %f\", precision))\n\n[1] \"precision: 0.251366\"\n\n\nUsing the different Confusion Matrices, Create the ROC curve"
  },
  {
    "objectID": "posts/renan-blog-post-week8/index.html#conclusion",
    "href": "posts/renan-blog-post-week8/index.html#conclusion",
    "title": "Reproducing Steve’s Code - Week 8",
    "section": "Conclusion",
    "text": "Conclusion\nThe code is unreadable and has several mistakes from Syntax to several implementation errors and System dependencies that I could not meet. So I had to do my best interpretation to reproduce it in Quarto.\n\nIdeas for improving precision, recall and f1_score\nAddress imbalance by upsample (add stroke cases), downsample (remove non strokecases) and or SMOTE (Synthetic data)\nChange the classification threshold\nCompare with Alternative Models such as random forrests or XGBoost\n\n\nReferences\n\n\n1. fedesoriano. (n.d.). Stroke Prediction Dataset. https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset"
  }
]