[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Logistic Regression Group",
    "section": "",
    "text": "As part of the Fall-2025 Capstone Projects in Data Science (IDC-6940) at University of West Florida (UWF), our team provided a systematic review of logistic regresion models and their applications with focus to healthcare data.\nThis project was completed under the guidance of Dr. Achraf Cohen (Achraf Cohen | Dr. Achraf Cohen)."
  },
  {
    "objectID": "people/cohen-achraf/index.html#education",
    "href": "people/cohen-achraf/index.html#education",
    "title": "Achraf Cohen",
    "section": "Education",
    "text": "Education\nPhD in Applied Mathematics/Statistics | University of Angers, France"
  },
  {
    "objectID": "people/index.html",
    "href": "people/index.html",
    "title": "Meet the Group",
    "section": "",
    "text": "Graduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/index.html#graduate-students",
    "href": "people/index.html#graduate-students",
    "title": "Meet the Group",
    "section": "",
    "text": "Graduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/index.html#orientator",
    "href": "people/index.html#orientator",
    "title": "Meet the Group",
    "section": "Orientator",
    "text": "Orientator\n\n\n\n\n\n\n\n\n\n\nAchraf Cohen\n\n\nAssociate Professor of Statistics/Data Science\n\n\n\n\n\neducation\n\n\nPhD in Applied Mathematics/Statistics | University of Angers, France\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/kusem-kristina/index.html#education",
    "href": "people/kusem-kristina/index.html#education",
    "title": "Kristina Kusem",
    "section": "Education",
    "text": "Education\nB.S. Mechanical Engineering | Univevrsity of West Florida"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "Welcome to our team! We are a group of collaborators dedicated to…\n\n\n\nRenan Monteiro Barbosa\n\nData Scientist\n\n\n\nRenan Monteiro Barbosa\n\n\nJane is a data scientist with over 10 years of experience…\nContact * Email: jane@example.com * GitHub: github.com/jane\n\n\n\nKristina Kusem\n\nSoftware Engineer\n\n\n\nKristina Kusem\n\n\nJohn is a software engineer specializing in web development…\nContact * Email: john@example.com * Twitter: @john_smith\n\n\n\nShree Krishna Basnet\n\nResearch Analyst\n\n\n\nShree Krishna Basnet\n\n\nAlex works on developing new research methodologies for…\nContact * Email: alex@example.com * Website: alex-johnson.com"
  },
  {
    "objectID": "posts/shree-blog-post-week4/index.html",
    "href": "posts/shree-blog-post-week4/index.html",
    "title": "Literature Review Week 4",
    "section": "",
    "text": "Regularized logistic regression with network‐based pairwise interaction for biomarker identification in breast cancer (Wu et al., 2016) (1)\n\nGoal: Regularized logistic regression should be used, along with network (biological network) information and pairwise interactions, to find biomarkers (both single and interacting pairs) for breast cancer.\n\nLink: https://www.researchgate.net/publication/296193700_Regularized_logistic_regression_with_network-based_pairwise_interaction_for_biomarker_identification_in_breast_cancer\nWhat made this paper interesting, or why the analysis is important: different biological processes has combined interactions between genes and proteins rather than single genes or proteins, like network topology or interaction information, which may result in better biomarkers that are biologically useful rather than statistically significant. Associating network knowledge may improve prediction or interpretability.\nMethodology: The analyst used a regularized logistic regression model that incorporates pairwise connections and protein-protein interaction (PPI) networks. they prioritized biologically plausible biomarker combinations and used an adaptive elastic net (a penalty that balances l1 and l2) with network constraints. Used breast cancer datasets (gene expression data) to discover key nodes and relationships.\nResult/conclusion: Their model outperforms simpler models in terms of predictive performance, and they were able to discover both individual biomarkers and interacting gene pairs. The interactions has been found to have some biological sense.\nLimitation: The risk of overfitting and model size are increased by the intricacy of incorporating relationships. The quality of the network and expression data determines the outcomes.\n\n\nUsing Genetic Algorithms and Sparse Logistic Regression to Find Gene Signatures for Chemosensitivity Prediction in Breast Cancer. (2)\nGoal: To identify “gene signatures” that predict chemosensitivity, that is, which tumors react to chemotherapy in breast cancer, combine genetic algorithms with sparse logistic regression.\nWhat made this paper interesting, or why the analysis is important:Predicting which patients will react to chemotherapy gives more personalized treatment. Potential biomarkers include gene signatures. However, there are several genes and possible combinations, like genetic algorithms that aid in searching space, while sparse logistic regression aids in reducing characteristics.\nMethodology: To create individuals’ “gene signature” subsets, first choose genes using a Genetic Algorithm (GA) from among overexpressed genes (or pathway-specific genes) and forecast response, create sparse logistic regression models using those subsets. Assess accuracy, sensitivity, specificity, and other metrics using both a training and a validation set.\nResult/ Conclusion : The results show that SLR-28 and Notch-86, two gene signatures, perform well on training and validation sets in terms of accuracy, specificity, sensitivity, and other metrics. In some reults we can see it performs better than previous signatures.\nLimitation: Generalization is uncertain due to the relatively small datasets. Randomness is added by the GA, signature stability may differ. Clinical validation also comes at a high expense. overfitting risk.\n\n\n\n\n1. Wu MY, Zhang XF, Dai DQ, Ou-Yang L, Zhu Y, Yan H. Regularized logistic regression with network-based pairwise interaction for biomarker identification in breast cancer. BMC bioinformatics. 2016;17(1):108. \n\n\n2. Hu W. Using genetic algorithms and sparse logistic regression to find gene signatures for chemosensitivity prediction in breast cancer. American Journal of Bioscience and Bioengineering. 2016;4(2):26–33."
  },
  {
    "objectID": "posts/shree-blog-post-week4/index.html#article-2",
    "href": "posts/shree-blog-post-week4/index.html#article-2",
    "title": "Literature Review Week 4",
    "section": "",
    "text": "Using Genetic Algorithms and Sparse Logistic Regression to Find Gene Signatures for Chemosensitivity Prediction in Breast Cancer. (2)\nGoal: To identify “gene signatures” that predict chemosensitivity, that is, which tumors react to chemotherapy in breast cancer, combine genetic algorithms with sparse logistic regression.\nWhat made this paper interesting, or why the analysis is important:Predicting which patients will react to chemotherapy gives more personalized treatment. Potential biomarkers include gene signatures. However, there are several genes and possible combinations, like genetic algorithms that aid in searching space, while sparse logistic regression aids in reducing characteristics.\nMethodology: To create individuals’ “gene signature” subsets, first choose genes using a Genetic Algorithm (GA) from among overexpressed genes (or pathway-specific genes) and forecast response, create sparse logistic regression models using those subsets. Assess accuracy, sensitivity, specificity, and other metrics using both a training and a validation set.\nResult/ Conclusion : The results show that SLR-28 and Notch-86, two gene signatures, perform well on training and validation sets in terms of accuracy, specificity, sensitivity, and other metrics. In some reults we can see it performs better than previous signatures.\nLimitation: Generalization is uncertain due to the relatively small datasets. Randomness is added by the GA, signature stability may differ. Clinical validation also comes at a high expense. overfitting risk.\n\n\n\n\n1. Wu MY, Zhang XF, Dai DQ, Ou-Yang L, Zhu Y, Yan H. Regularized logistic regression with network-based pairwise interaction for biomarker identification in breast cancer. BMC bioinformatics. 2016;17(1):108. \n\n\n2. Hu W. Using genetic algorithms and sparse logistic regression to find gene signatures for chemosensitivity prediction in breast cancer. American Journal of Bioscience and Bioengineering. 2016;4(2):26–33."
  },
  {
    "objectID": "posts/kristina-blog-post-week5/index.html",
    "href": "posts/kristina-blog-post-week5/index.html",
    "title": "Literature Review Week 5",
    "section": "",
    "text": "Article Title: Identification of factors influencing severity of motorcycle crashes in Dhaka, Bangladesh using binary logistic regression model. (1)\nAuthors: Hamidur Rahman, Niaz Mahmud Zafri, Tamanna Akter, Shahrior Pervaz\nProblem: - One of the most common unnatural causes of death across the world is road accidents, so it is important to identify strong predictors associated with such accidents. - According to the World Health Organization, most of the road crash deaths that occur worldwide happen in developing countries. - Researchers focus on motorcycle crashes in Dhaka, the capital city of Bangladesh. It is stated that the rate of road crashes in Bangladesh is significantly greater than other developing countries, and Dhaka has the greatest amount of motorists and reported motorcycle crashes. - It is noted that most research about this topic is done on data from developed countries and not developing countries. So the conclusions drawn from existing studies may not be applicable to the problems the developing countries are facing - Knowing which predictors are strongly associated with motorcycle crashes in Dhaka helps builders and developers eliminate or reduce these risk factors as they are building new roads. This study serves as a step in preventing more motorcycle road crashes as developing countries are being built.\nSolution: - Researchers conduct a binary logistic regression analysis to identify predictors most strongly associated with the occurrence of the outcome, motorcycle crash severity (fatal/ non- fatal) - They began the study by choosing predictors identified in previous literature about similar topics. Commonly identified predictors of motorcycle crashes are grouped into five broad categories: environment, road characteristics, driver characteristics, motorcycle features, and type of collision. - The binary logistic regression equation is given as the log odds of the probability of the occurrence of the outcome. An explanation of every variable in the equation is given (slopes, intercept, odds ratio, and relation to the outcome variable).\nData: - Data was collected from 2006 to 2015 from the Accident Research Institute of Bangladesh University of Engineering and Technology. Only 316 data points were used, and each contained information about motorcycle crashes. - There are five broad categories encompassing all predictors. The five categories and predictors are: 1. Environmental factors- date/time, lighting, weather 2. Collision type- five different types of collisions 3. Driver characteristics- gender, age, alcohol consumption, and use of a helmet 4. Road characteristics-location, traffic characteristics, road conditions 5. vehicle characteristics- type of other vehicle in crash, weight of other vehicle in crash, motorcycle condition, and more - The outcome variable, crash injury severity, originally had four levels. Observations from this predictor were then reclassified as either “fatal” or “non-fatal” for a binary outcome. - To determine which predictors to include in the dataset, researchers conducted a univariate analysis and a chi- square test of each individual predictor to assess significance. All significant predicators were then chosen for the dataset, used for the binary logistic regression. After this, multicollinearity was assessed using the VIF and none of the predictors showed multicollinearity.\nResults/ Conclusions: - After conducting the binary logistic regression, 11 out of the 16 included predictors were found to be significantly associated with the outcome. The significant predictors were day of the week, seasonal condition, time of day, three types of road characteristics, crash type, condition of motorcycle, type of other vehicle in accident, use of helmet, and alcohol consumption - The regression curve from the analysis was also found to be significant - Goodness of fit was assessed using a test called the Hosmer and Lemeshow test - The discussion section details each significant predictor, and interpretations of slopes are given in relation to the outcome variable and the reference groups. - Some of the findings were consistent with other studies, and some of the findings contradict conclusions in previous studies. - Most notable conclusions from this study that researchers believe would improve road safety and reduce motorcycle accident severity in developing countries: 1. better lighting conditions for enhanced visibility 2. solution to wet/ slippery roads during rainy season 3. educate drivers about how to drive during unsafe conditions, such as nighttime, heavy rain, and heavy traffic on weekends. Also educate drivers to follow proper safety and speeding regulations. And better education/ training/ evaluation for drivers operating larger vehicles 4. improved pedestrian walkways and road areas 5. strict enforcement of laws regarding helmet use and alcohol while driving Limitations: - Missing information in the data: Some important predictors were entirely excluded from the study due to missing information in the dataset. So there may be some extremely relevant predictors that have yet to be studied. There is also an ongoing issue of accidents that go unreported due to lack of fatality, and drivers do not report these incident."
  },
  {
    "objectID": "posts/kristina-blog-post-week5/index.html#article-1",
    "href": "posts/kristina-blog-post-week5/index.html#article-1",
    "title": "Literature Review Week 5",
    "section": "",
    "text": "Article Title: Identification of factors influencing severity of motorcycle crashes in Dhaka, Bangladesh using binary logistic regression model. (1)\nAuthors: Hamidur Rahman, Niaz Mahmud Zafri, Tamanna Akter, Shahrior Pervaz\nProblem: - One of the most common unnatural causes of death across the world is road accidents, so it is important to identify strong predictors associated with such accidents. - According to the World Health Organization, most of the road crash deaths that occur worldwide happen in developing countries. - Researchers focus on motorcycle crashes in Dhaka, the capital city of Bangladesh. It is stated that the rate of road crashes in Bangladesh is significantly greater than other developing countries, and Dhaka has the greatest amount of motorists and reported motorcycle crashes. - It is noted that most research about this topic is done on data from developed countries and not developing countries. So the conclusions drawn from existing studies may not be applicable to the problems the developing countries are facing - Knowing which predictors are strongly associated with motorcycle crashes in Dhaka helps builders and developers eliminate or reduce these risk factors as they are building new roads. This study serves as a step in preventing more motorcycle road crashes as developing countries are being built.\nSolution: - Researchers conduct a binary logistic regression analysis to identify predictors most strongly associated with the occurrence of the outcome, motorcycle crash severity (fatal/ non- fatal) - They began the study by choosing predictors identified in previous literature about similar topics. Commonly identified predictors of motorcycle crashes are grouped into five broad categories: environment, road characteristics, driver characteristics, motorcycle features, and type of collision. - The binary logistic regression equation is given as the log odds of the probability of the occurrence of the outcome. An explanation of every variable in the equation is given (slopes, intercept, odds ratio, and relation to the outcome variable).\nData: - Data was collected from 2006 to 2015 from the Accident Research Institute of Bangladesh University of Engineering and Technology. Only 316 data points were used, and each contained information about motorcycle crashes. - There are five broad categories encompassing all predictors. The five categories and predictors are: 1. Environmental factors- date/time, lighting, weather 2. Collision type- five different types of collisions 3. Driver characteristics- gender, age, alcohol consumption, and use of a helmet 4. Road characteristics-location, traffic characteristics, road conditions 5. vehicle characteristics- type of other vehicle in crash, weight of other vehicle in crash, motorcycle condition, and more - The outcome variable, crash injury severity, originally had four levels. Observations from this predictor were then reclassified as either “fatal” or “non-fatal” for a binary outcome. - To determine which predictors to include in the dataset, researchers conducted a univariate analysis and a chi- square test of each individual predictor to assess significance. All significant predicators were then chosen for the dataset, used for the binary logistic regression. After this, multicollinearity was assessed using the VIF and none of the predictors showed multicollinearity.\nResults/ Conclusions: - After conducting the binary logistic regression, 11 out of the 16 included predictors were found to be significantly associated with the outcome. The significant predictors were day of the week, seasonal condition, time of day, three types of road characteristics, crash type, condition of motorcycle, type of other vehicle in accident, use of helmet, and alcohol consumption - The regression curve from the analysis was also found to be significant - Goodness of fit was assessed using a test called the Hosmer and Lemeshow test - The discussion section details each significant predictor, and interpretations of slopes are given in relation to the outcome variable and the reference groups. - Some of the findings were consistent with other studies, and some of the findings contradict conclusions in previous studies. - Most notable conclusions from this study that researchers believe would improve road safety and reduce motorcycle accident severity in developing countries: 1. better lighting conditions for enhanced visibility 2. solution to wet/ slippery roads during rainy season 3. educate drivers about how to drive during unsafe conditions, such as nighttime, heavy rain, and heavy traffic on weekends. Also educate drivers to follow proper safety and speeding regulations. And better education/ training/ evaluation for drivers operating larger vehicles 4. improved pedestrian walkways and road areas 5. strict enforcement of laws regarding helmet use and alcohol while driving Limitations: - Missing information in the data: Some important predictors were entirely excluded from the study due to missing information in the dataset. So there may be some extremely relevant predictors that have yet to be studied. There is also an ongoing issue of accidents that go unreported due to lack of fatality, and drivers do not report these incident."
  },
  {
    "objectID": "posts/kristina-blog-post-week5/index.html#article-2",
    "href": "posts/kristina-blog-post-week5/index.html#article-2",
    "title": "Literature Review Week 5",
    "section": "Article 2",
    "text": "Article 2\nTitle of article: Risk factors for airplane headache: A multivariate logistic regression analysis in a population of career flight personnel. (2)\nAuthors: Johannes Prottengeier, Isabelle Kaiser, Andreas Moritz, Fabian Konrad\nProblem: - Airplane headache (AH) is a headache disorder described as a headache induced while taking off or landing in an airplane. It was not until 2004 that the disorder was recognized and classified as a medical issue. Because this disorder has just recently been recognized, there is little existing research on it. - There is a lot of previous literature and research regarding other headache types and disorders, but AH is still lacking appropriate research. - Specifically, this study aims to identify predictors and risk factors that occur before onset of airplane headache. Knowing the predictors would help both travelers and employees of airlines. - AH is a common disorder affecting about 65 million people worldwide every year, so helping prevent and treat it is crucial. Future research is therefore a must.\nSolution: - Conduct a logistic regression analysis to determine significant predictors of airplane headache. Two binary logistic regression models were constructed; one model’s outcome was either airplane headache (1) or no headache (0), and the other model’s outcomes were airplane headache (1) or other headache (0). - Used R to conduct statistical analysis\nData: - Data was collected from a voluntary online survey sent out to about 20,000 pilots who fly frequently due to work. A total of 2237 complete questionnaires from a 3 month period in 2014 were received and used in the dataset for this study. - The data/ questions in the survey were determined by pain specialists - Predictors included in the survey were: demographic information, health history, substance use, medication use, stress levels, and headache/ physical symptoms while flying. - The outcome variable was divided into three categories: airplane headache, no headache, and other type of headache. - Predictors were tested for multicollinearity before models were constructed - Data was further reduced using stepwise backward elimination (starting with all predictors and then eliminating least significant predictors)\nResults/ Conclusions: - Survey results revealed that a vast majority of participants said they had some form of headache while flying on an airplane (82%) - The first model comparing airplane headache with no headache was found to have 10 significant predictors. The AUC for this model showed that it had strong predictive power. - The second model comparing airplane headache with other headache was found to have only four significant predictors. The AUC also showed that this model had low predictive power. This is likely attributed to the fact that airplane headache and other types of headaches all have similar predictors, so it is not easy to distinguish between strong predictors for just AH as compared to all types of headaches. - Conclusions: supplementing with folic acid before flight may help reduce risk of airplane headache. Other strong predictors of airplane headache are occupation, work stress, and preexisting headache medical conditions. - Further research about airplane headache is necessary because of all the ongoing negative subsequent events caused by it. It causes people loss of productivity, avoidance behaviors, stress, and anxiety.\nLimitations: - Only 12% of surveyed individuals submitted their survey. It could be the case that only the people who suffered from headaches responded to the survey, so the proportions of individuals with airplane headache in this study are not representative of the entire population. This is called positive selection bias. - Participants in the survey may not be reporting accurate information because a lot of time passes between the event of their airplane travel and the time when they take the survey.\n\nReferences\n\n\n1. Rahman MH, Zafri NM, Akter T, Pervaz S. Identification of factors influencing severity of motorcycle crashes in dhaka, bangladesh using binary logistic regression model. International journal of injury control and safety promotion. 2021;28(2):141–52. \n\n\n2. Prottengeier J, Kaiser I, Moritz A, Konrad F. Risk factors for airplane headache: A multivariable logistic regression analysis in a population of career flight personnel. Cephalalgia. 2025;45(4):03331024251329837."
  },
  {
    "objectID": "posts/renan-blog-post-week2/index.html",
    "href": "posts/renan-blog-post-week2/index.html",
    "title": "Literature Review Week 2",
    "section": "",
    "text": "This week I review 2 articles"
  },
  {
    "objectID": "posts/renan-blog-post-week2/index.html#article-1",
    "href": "posts/renan-blog-post-week2/index.html#article-1",
    "title": "Literature Review Week 2",
    "section": "Article 1",
    "text": "Article 1\nFrom Logistic Regression to the Perceptron Algorithm: Exploring Gradient Descent with Large Step Sizes. (1)\nThe author presents some interesting findings that by connecting Logistic regression with gradient descent there is a link with the perceptron algorithm. With really large steps it acts like a perceptron which in some sense links it back to the Deep Equilibrium networks study. This paper is interesting because it is counter intuitive and brings a lot of things to reflect about classification and optimization theory."
  },
  {
    "objectID": "posts/renan-blog-post-week2/index.html#article-2",
    "href": "posts/renan-blog-post-week2/index.html#article-2",
    "title": "Literature Review Week 2",
    "section": "Article 2",
    "text": "Article 2\nLarge Language Model Confidence Estimation via Black-Box Access. (2)\nThis paper addresses the problem of estimating the confidence of large language model (LLM) outputs when only black-box (query-only) access is available. It is a simple technique that uses Logistic Regression to classify and validate the confidence of the outputs. The problem of using the black-box models is that there is no control over the model itself, but in some cases the benefits and the value of buying these services that provide a black-box model outweighs training your own custom so this is a framework that attempts to overcome the challenges.\n\nReferences\n\n\n1. Tyurin A. From logistic regression to the perceptron algorithm: Exploring gradient descent with large step sizes [Internet]. 2024. Available from: https://arxiv.org/abs/2412.08424\n\n\n2. Pedapati T, Dhurandhar A, Ghosh S, Dan S, Sattigeri P. Large language model confidence estimation via black-box access [Internet]. 2025. Available from: https://arxiv.org/abs/2406.04370"
  },
  {
    "objectID": "posts/renan-blog-post-week3/index.html",
    "href": "posts/renan-blog-post-week3/index.html",
    "title": "Literature Review Week 3",
    "section": "",
    "text": "This week I review 2 articles"
  },
  {
    "objectID": "posts/renan-blog-post-week3/index.html#article-1",
    "href": "posts/renan-blog-post-week3/index.html#article-1",
    "title": "Literature Review Week 3",
    "section": "Article 1",
    "text": "Article 1\nBERT or FastText? A Comparative Analysis of Contextual as well as Non-Contextual Embeddings. (1)\nMy personal opinion: This research doesn’t explicitly state why Logistic Regression is important, but it did use it as the classifier for all of the experiments to maintain methodological simplicity. All embeddings were passed to a multinomial logistic regression (MLR) classifier for classification into target labels. Which shows the versatility of logistic regression when elaborating an experiment to test a hypothesis.\nThe main goal of the paper is to analyze the effectiveness of non-contextual embeddings from BERT models (MuRIL and MahaBERT) and FastText models (IndicFT and MahaFT) for NLP tasks. The authors compare these embeddings to contextual and compressed variants of BERT aiming to fill a research gap, because previous research did not explore non-contextual embeddings.\nThe research is important because it addresses the challenges faced by NLP in low-resource languages (The ones that lack big annotated datasets to properly train). The selection of an effective embedding method is extremely important for strong NLP performance. The research tries a promising alternative, non-contextual BERT embeddings, which can be obtained through a simple table lookup, unlike contextual embeddings that require a full forward pass through the model. This is particularly relevant for getting model performance with much better computational efficiency.\nThe methodology is quite interesting. For the FastText, which is a non-contextual embedding by default, they had to create a custom vocabulary. Which was achieved by concatenating the training and validation datasets and then passing them through a text vectorizer, which generated vectors for every word in the dataset. The vectorizer returned the vocabulary as a list of words in decreasing order of their frequency. Then the FastText model was then loaded using the FastText library, and for each word in the vocabulary, a word vector was retrieved to construct the embedding matrix. For each sentence, the text was split into individual words, and the corresponding embeddings were retrieved from the embedding matrix. These embeddings were then averaged to produce the final sentence embeddings.\nFurthermore they did not stop with FastText, they also experimented with compressed embeddings by reducing the dimensionality from 768 (the traditional BERT embedding dimension) to 300. This compression was performed using Singular Value Decomposition (SVD) to select the most relevant features, extracting the top 300 components for all the combinations of contextual as well as non-contextual for MahaBERT as well as Muril.\nIn this approach it’s interesting how they did use Logistic regression for simplicity. All embeddings were then passed to a multiple logistic regression(MLR) classifier for classification into target labels.\nI understood that as a result they did show that contextual BERT embeddings perform better than non-contextual ones, including both non-contextual BERT embeddings and FastText. So in the end they proved that their approach did not improve much or provided much resource to support this different approach. They also showed that when non-contextual BERT embeddings are compressed, their performance drops, and FastText performs better than compressed noncontextual BERT. But this is a questionable finding.\nThe limitations of the research is that even in most cases it was apparent that compression lowers the performance of non-contextual BERT embeddings. The effect of compression on contextual embeddings varies across datasets and there is no consistent way to properly derive conclusions."
  },
  {
    "objectID": "posts/renan-blog-post-week3/index.html#article-2",
    "href": "posts/renan-blog-post-week3/index.html#article-2",
    "title": "Literature Review Week 3",
    "section": "Article 2",
    "text": "Article 2\nPriority prediction of Asian Hornet sighting report using machine learning methods. (2)\nThe goal of the research is to create an automated system to predict the priority of Asian giant hornet sighting reports. Asian giant hornets are an invasive species that poses a significant threat to native bee populations and local beekeeping, as well as to public safety due to their aggressive nature and potent venom. So it’s very important that reports are properly assessed for priority.\nThe authors did model the priority prediction of sighting reports as a two-classification problem. This approach was pretty clever and simple. The goal was to just classify reports as either a “true positive” or a “false positive”.\nTheir methodology is a straightforward application of logistic regression with feature extraction. They came to realize that they needed Location Feature, Time Feature, Image Feature, Text Feature.\nLocation Feature considers the probability of a hornet being observed at a specific location based on known hornet migration patterns and habits. Time Feature accounts for the hornet’s seasonal behavior. Since hornets are most active from April to December, a report submitted during this period is more likely to be positive. Image Feature is the number of images attached to a report and they came to notice that it is correlated with the increase of its credibility. Text Feature is the textual description’s length and keywords. A longer text is considered more credible because it contains more evidence. The model also uses a specific dictionary of hornet characteristics to identify relevant keywords.\nThey then used a weighted binary cross-entropy function and the logistic regression is just mapping the probability given the feature vector.\nThe model achieved an average prediction accuracy of 83.5% on positive reports with the best weighting parameter settings, but still far from other works which achieved about 93% using Deep Learning. So this is the main limitation, still needs a lot of improvement or maybe it will never outmatch other methods due to hidden limitations.\nMy opinion on this paper is that the Logistic Regression has interesting properties, after all it is a generalized linear model, which conducts mapping from any real number to probability values.\n\nReferences\n\n\n1. Shanbhag A, Jadhav S, Thakurdesai A, Sinare R, Joshi R. Non-contextual BERT or FastText? A comparative analysis [Internet]. 2025. Available from: https://arxiv.org/abs/2411.17661\n\n\n2. Liu Y, Guo J, Dong J, Jiang L, Ouyang H. Priority prediction of asian hornet sighting report using machine learning methods. In: 2021 IEEE international conference on software engineering and artificial intelligence (SEAI) [Internet]. IEEE; 2021. p. 7–11. Available from: http://dx.doi.org/10.1109/SEAI52285.2021.9477549"
  },
  {
    "objectID": "posts/renan-blog-post-week4/index.html",
    "href": "posts/renan-blog-post-week4/index.html",
    "title": "Literature Review Week 4",
    "section": "",
    "text": "This week I review 2 articles"
  },
  {
    "objectID": "posts/renan-blog-post-week4/index.html#article-1",
    "href": "posts/renan-blog-post-week4/index.html#article-1",
    "title": "Literature Review Week 4",
    "section": "Article 1",
    "text": "Article 1\nIncorporating LLM Priors into Tabular Learners. (1)\nThere have been implementations of transformer based architectures for tabular data. Most of the time it has been utilized for generating synthetic data for likelihood free models or for cases where there is not enough data for fitting a model.\nThe goal of this research was to bootstrap a way so one could use off the shelf models like Chatgpt which are really good at generalization to perform similarly to dedicated models trained on tabular data such as tabLLM. This is important because it is fairly cheaper and more accessible than training a model from scratch and it overcomes the complexities of developing a specialized encoder.\nThe methodology is a pretty hacky solution where they serialized the tabular data so they could prompt the models are are just trying to obtain back a categorization through prompt engineering which will be attributed a value which is manually tuned by the authors and this value is later used on the Monotonic Logistic Regression.\nThe limitations are quite clear. There is no way to guarantee the black box model output will be consistent. You have to manually categorize and do some prompt engineering. The model has bias so it either works really well or it doesn’t.\nThe bright side is that this approach is extremely cheap and is accessible. It can be used to test ideas and hypothesis as well rapidly prototype before committing to a more definite solution such as tabLLM."
  },
  {
    "objectID": "posts/renan-blog-post-week4/index.html#article-2",
    "href": "posts/renan-blog-post-week4/index.html#article-2",
    "title": "Literature Review Week 4",
    "section": "Article 2",
    "text": "Article 2\nUsing a monotonic density ratio model to increase the power of the goodness-of-fit test for logistic regression models with case-control data. (2)\nCase-control sampling is used because it is a quick, economical, and efficient method for studying rare diseases or outcomes, long latent periods, or outbreaks. It allows researchers to investigate multiple potential risk factors simultaneously for a single outcome and is especially useful when prospective cohort studies are not feasible.\nThe author’s goal seems to be to improve the statistical power of the goodness-of-fit test for logistic regression models when used with case-control data. They improved upon a previous popular method from Qin and Zhang, instead of using the nonparametric empirical distribution function, we use the constrained nonparametric MLE of G(x) to further improve the power performance of the Kolmogorov-Smirnov-type goodness-of-fit test for logistic models.\nBefore drawing conclusions from a logistic regression model, it’s crucial to verify that the model’s assumptions hold true for the data and there are many limitations. Case Study data is specially complicated because there is not enough data and there are too many unknowns.\nThe authors spare no comments on the limitations, the bigger limitations are: - The test is designed for goodness-of-fit and cannot be used to compare two different logistic regression models - The test has no power when the only covariate is categorical. In this situation, the logistic model is “saturated,” meaning it perfectly fits the data by definition and cannot be misspecified.\nTheir results are overall quite interesting as they demonstrated that they could bootstrap an algorithm that is quite clever and intuitively it shouldn’t work. It is a hard problem to solve so it is interesting out of the box thinking\n\nReferences\n\n\n1. Zhu M, Stanivuk S, Petrovic A, Nikolic M, Lio P. Incorporating LLM priors into tabular learners [Internet]. 2023. Available from: https://arxiv.org/abs/2311.11628\n\n\n2. Wang C, Liu Z, Wang X. Using a monotonic density ratio model to increase the power of the goodness-of-fit test for logistic regression models with case-control data. Statistics in Medicine. 2024;43(22):4272–86."
  },
  {
    "objectID": "posts/shree-blog-post-week5/index.html",
    "href": "posts/shree-blog-post-week5/index.html",
    "title": "Literature Review Week 5",
    "section": "",
    "text": "Link: https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2839330\nPrevalence of Clinical Obesity in US Adults Based on a Newly Proposed Definition (1)\nThe Lancet Diabetes & Endocrinology Commission has proposed a new definition of clinical obesity that includes evidence of organ malfunction or physiological impairment in addition to direct measures of adiposity.\nIn order to find populations who might have been incorrectly classified under previous BMI thresholds, the researchers aimed to compare BMI-based obesity with clinical obesity using national data from the United States.\nMethodology:\n\nCross-sectional study using NHANES 2017–2018 data (nationally representative, multistage sampling)\nAnalysis: Conducted in Stata 18; weighted percentages with 95% CIs; significance at p&lt;0.05\nGuidelines: Followed STROBE reporting standard.\n\nKey points - BMI and Clinical Impact : BMI data is insufficient as it leaves who already experience obesity-related dysfunction. - Older adults are more seen in clinical obesity even at lower BMIs = highlights BMI’s limitation in aging populations. - Younger, higher-income adults are seen with mostly BMI-only obese: high weight but not yet showing dysfunction. - Public health implication: Using the clinical definition could better target interventions (medication, surgery, lifestyle) and identify people at risk earlier.\nSummary: Although the overall obesity rates determined by BMI and clinical criteria are comparable, they distinguish distinct populations. The clinical definition emphasizes the significance of early prevention for individuals with preclinical obesity and more accurately reflects the health effects, particularly in older and underprivileged populations."
  },
  {
    "objectID": "posts/shree-blog-post-week5/index.html#article-1",
    "href": "posts/shree-blog-post-week5/index.html#article-1",
    "title": "Literature Review Week 5",
    "section": "",
    "text": "Link: https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2839330\nPrevalence of Clinical Obesity in US Adults Based on a Newly Proposed Definition (1)\nThe Lancet Diabetes & Endocrinology Commission has proposed a new definition of clinical obesity that includes evidence of organ malfunction or physiological impairment in addition to direct measures of adiposity.\nIn order to find populations who might have been incorrectly classified under previous BMI thresholds, the researchers aimed to compare BMI-based obesity with clinical obesity using national data from the United States.\nMethodology:\n\nCross-sectional study using NHANES 2017–2018 data (nationally representative, multistage sampling)\nAnalysis: Conducted in Stata 18; weighted percentages with 95% CIs; significance at p&lt;0.05\nGuidelines: Followed STROBE reporting standard.\n\nKey points - BMI and Clinical Impact : BMI data is insufficient as it leaves who already experience obesity-related dysfunction. - Older adults are more seen in clinical obesity even at lower BMIs = highlights BMI’s limitation in aging populations. - Younger, higher-income adults are seen with mostly BMI-only obese: high weight but not yet showing dysfunction. - Public health implication: Using the clinical definition could better target interventions (medication, surgery, lifestyle) and identify people at risk earlier.\nSummary: Although the overall obesity rates determined by BMI and clinical criteria are comparable, they distinguish distinct populations. The clinical definition emphasizes the significance of early prevention for individuals with preclinical obesity and more accurately reflects the health effects, particularly in older and underprivileged populations."
  },
  {
    "objectID": "posts/shree-blog-post-week5/index.html#article-2",
    "href": "posts/shree-blog-post-week5/index.html#article-2",
    "title": "Literature Review Week 5",
    "section": "Article 2",
    "text": "Article 2\nBenefit-Risk Reporting for FDA-Cleared Artificial Intelligence−Enabled Medical Devices (2)\nLink: https://jamanetwork.com/journals/jama-health-forum/fullarticle/2839236\nSummary:\nThe effectiveness with which FDA-approved AI/ML-enabled medical devices disclose their advantages, hazards, effectiveness, and safety before to and following approval was investigated in this study\nScope: Using FDA records (decision summaries, adverse events, and recalls), 691 AI/ML devices that were approved between 1995 and 2023 were analyzed.\nResults:\nKey reporting was absent from many devices:\n46.7% did not report the study design.\n53.3% of the training sample size is missing.\n95.5% of demographic information is lacking.\nJust 3 devices (&lt;1%) reported patient outcomes, while only 6 devices (1.6%) used randomized clinical trials. Sensitivity (24%), specificity (22%), and other performance indicators were not reported by many.Just 28.2% of devices had safety assessments, and only 8.7% had bias assessments.Adverse events: 489 incidents involving 36 devices (5.2%), comprising 30 injuries, 1 fatality, and 458 malfunctions.40 devices (5.8%) were recalled 113 times, primarily due to software problems.\nTrends:\nAlthough there was a little improvement in the reporting of bias checks, efficacy, and outcomes for devices cleared after 2021, these devices were less likely to have safety evaluations or peer-reviewed publications.\nConclusion: Standardized reporting of risk, safety, and efficacy is lacking in regulatory monitoring of AI/ML medical devices. The study highlights the necessity of an FDA regulatory approach specifically for AI/ML devices. more robust postmarket surveillance networks.Increased health fairness and transparency (e.g., improved demographic reporting to prevent prejudice).\n\nReferences\n\n\n1. Park D, Lee DH, Kim R, Shin MJ, Subramanian S. Prevalence of clinical obesity in US adults based on a newly proposed definition. JAMA Network Open. 2025;8(9):e2533806–6. \n\n\n2. Lin JC, Jain B, Iyer JM, Rola I, Srinivasan AR, Kang C, et al. Benefit-risk reporting for FDA-cleared artificial intelligence- enabled medical devices. In: JAMA health forum. American Medical Association; 2025. p. e253351–1."
  },
  {
    "objectID": "posts/kristina-blog-post-week2/index.html",
    "href": "posts/kristina-blog-post-week2/index.html",
    "title": "Literature Review Week 2",
    "section": "",
    "text": "Article title: Understanding logistic regression analysis (1)\nData used: Synthetic data about the effect of a drug treatment. There are two treatments: standard and new drug treatments. Shows a binary outcome, either died or survived after drug treatment.\nProblem: The problem introduced in this article is needing a method to study the joint relationship between two or more predictors and the target variable. One way to solve this problem is to calculate a weighted odds ratio that accounts for all the relationships and predictors. However, as the number of predictors increases, weighted odds ratio calculations can become very complicated. Also, these calculations require only categorical variables as input and no continuous variables may be used. A solution to this problem is to use logistic regression.\nSolution to problem: - Article explains what logistic regression is useful for. Advantages are that it can be used when there are more than two predictors and we want to analyze how they all simultaneously affect the target variable. Also useful for when we have any number of continuous predictors. - Gives the logistic regression model equation and explains the meaning of all variables (intercept, slops, and symbols). The outcome in the model (left hand side of equation) is the log of the odds. The paper goes into detail about how to interpret coefficients in the model; you must take the exponentials of the coefficients to understand the chances (the probability) of an event happening (the event in this paper is death).\nLimitations: - Differences between odds, odds ratios, and probabilities are discussed. Understanding the differences is key to interpreting results, and if you do not understand the differences, you cannot easily interpret the output of a logistic regression model. - If there is a predictor with more than two levels, you must create n-1 dummy variables for n number of categories within the predictor. This is considered a limitation because dataset manipulation must be done prior to constructing a model. - Interpreting coefficients of continuous variables is explained. It is different than interpreting coefficients of categorical variables. Interpreting these results can be complicated and should be done carefully. Exponential of the coefficient of a continuous variable is the chance of an event happening in relation to one unit of the continuous predictor. - Models with too many predictors can be too saturated, and researchers may miss associations. An association may be present, but the model will not have enough statistical power with too many predictors. Solution: build a model with less predictors. You can start with all predictors and drop one at a time, or start with 0 predictors and add one at a time (keeping only the most important predictors). Starting with a full model is better. - A way to test the importance of each variable is to create a univariate model for each individual predictor at a time to see which are most strongly correlated with the outcome. - How to choose the reference group is explained. Usually, the reference group is the lowest level or the highest level in a group of ordered categories. But if there is no order to the categories then there may be no clear reference group. Results vary when choosing differing reference groups.\nResult: The researchers conclude by stating that logistic regression is a very powerful and useful way to analyze epidemiologic data."
  },
  {
    "objectID": "posts/kristina-blog-post-week2/index.html#article-1",
    "href": "posts/kristina-blog-post-week2/index.html#article-1",
    "title": "Literature Review Week 2",
    "section": "",
    "text": "Article title: Understanding logistic regression analysis (1)\nData used: Synthetic data about the effect of a drug treatment. There are two treatments: standard and new drug treatments. Shows a binary outcome, either died or survived after drug treatment.\nProblem: The problem introduced in this article is needing a method to study the joint relationship between two or more predictors and the target variable. One way to solve this problem is to calculate a weighted odds ratio that accounts for all the relationships and predictors. However, as the number of predictors increases, weighted odds ratio calculations can become very complicated. Also, these calculations require only categorical variables as input and no continuous variables may be used. A solution to this problem is to use logistic regression.\nSolution to problem: - Article explains what logistic regression is useful for. Advantages are that it can be used when there are more than two predictors and we want to analyze how they all simultaneously affect the target variable. Also useful for when we have any number of continuous predictors. - Gives the logistic regression model equation and explains the meaning of all variables (intercept, slops, and symbols). The outcome in the model (left hand side of equation) is the log of the odds. The paper goes into detail about how to interpret coefficients in the model; you must take the exponentials of the coefficients to understand the chances (the probability) of an event happening (the event in this paper is death).\nLimitations: - Differences between odds, odds ratios, and probabilities are discussed. Understanding the differences is key to interpreting results, and if you do not understand the differences, you cannot easily interpret the output of a logistic regression model. - If there is a predictor with more than two levels, you must create n-1 dummy variables for n number of categories within the predictor. This is considered a limitation because dataset manipulation must be done prior to constructing a model. - Interpreting coefficients of continuous variables is explained. It is different than interpreting coefficients of categorical variables. Interpreting these results can be complicated and should be done carefully. Exponential of the coefficient of a continuous variable is the chance of an event happening in relation to one unit of the continuous predictor. - Models with too many predictors can be too saturated, and researchers may miss associations. An association may be present, but the model will not have enough statistical power with too many predictors. Solution: build a model with less predictors. You can start with all predictors and drop one at a time, or start with 0 predictors and add one at a time (keeping only the most important predictors). Starting with a full model is better. - A way to test the importance of each variable is to create a univariate model for each individual predictor at a time to see which are most strongly correlated with the outcome. - How to choose the reference group is explained. Usually, the reference group is the lowest level or the highest level in a group of ordered categories. But if there is no order to the categories then there may be no clear reference group. Results vary when choosing differing reference groups.\nResult: The researchers conclude by stating that logistic regression is a very powerful and useful way to analyze epidemiologic data."
  },
  {
    "objectID": "posts/kristina-blog-post-week2/index.html#article-2",
    "href": "posts/kristina-blog-post-week2/index.html#article-2",
    "title": "Literature Review Week 2",
    "section": "Article 2",
    "text": "Article 2\nArticle used: Binary logistic regression analysis of factors affecting urban road traffic safety. (2)\nProblem: The introduction features a literature review establishing relevance of the topic. Several studies about traffic accidents are discussed and cited. Traffic accidents are becoming more common as traffic increases due to population increases. Researchers aim to find which factors in traffic are more closely associated with the occurrence of traffic accidents.\nSolution: The researchers use a binary logistic regression model to study which factors are more correlated to the occurrence of traffic accidents. The advantages of a binary logistic regression are discussed. Logistic regression allows researchers to make predictions about probability of a dependent variable being sorted into a certain class. Logistic regression also allows for the researchers to determine which predictors more significantly impact the outcome variable. To set up the study, researchers defined the dependent variable, y, as a binary outcome of either no accident (value of 0) or presence of an accident (value of 1). The independent variables are defined as 25 factors that are grouped under four categories consisting of environmental factors, driver attributes, road attributes, and vehicle factors. Before analyzing the data, it was preprocessed to eliminate outliers, normalize all predictor values to similar scales, eliminate redundancy, Then, the predictors were run through a multicollinearity test to determine if any needed to be excluded from the model; none of the factors showed significant multicollinearity and all 25 were kept in the model. The calculation used for collinearity involved the correlation coefficient, R, tolerance (T), and variance inflation factor (V).\nResults: A binary logistic regression model was fitted to the data, and it was found that the model fit well. To assess the goodness of fit, the determination coefficient, R^2, value was calculated. The strongest predictors of a traffic incident were found to be driver behavior, weather, road conditions, and lighting.\nLimitations: The paper states that research in this area can be improved by using real- time data about weather, road conditions, and driver status. Using data as it occurs in real time may help make better predictions about traffic safety risks.\nDataset: The original data was sourced from the International Transport Forum with 5350 datapoints. After data preprocessing, the data was reduced to 3500 data points.\n\nReferences\n\n\n1. Sperandei S. Understanding logistic regression analysis. Biochemia medica. 2014;24(1):12–8. \n\n\n2. Chen Y, You P, Chang Z. Binary logistic regression analysis of factors affecting urban road traffic safety. Advances in Transportation Studies. 2024;3."
  },
  {
    "objectID": "posts/kristina-blog-post-week3/index.html",
    "href": "posts/kristina-blog-post-week3/index.html",
    "title": "Literature Review Week 3",
    "section": "",
    "text": "Title: Determinants of coexistence of undernutrition and anemia among under- five children in Rwanda; evidence from 2019/20 demographic health survey: Application of bivariate binary logistic regression model. (1)\nAuthors: Abebew Aklog smare, Yitateku Adugna Agmas\nProblem: - In children under five years of age, malnutrition and anemia have been an ongoing problem in many African countries. This paper focuses on studying malnutrition and anemia in Rwanda in particular; Rwanda went through a civil war, and after the war, rates of malnutrition and anemia decreased as the country was rebuilt. However, different parts of the country experienced faster or slower rates of improvement of malnutrition and anemia after the war, and it is unclear which factors are associated with improved rates of malnutrition and anemia. This study aims to identify key predictors correlated with these health ailments, so that the areas of the country still suffering from high rates of these health problems can be given the correct types of aid to fix the problem of malnutrition and anemia. - The introduction of the paper cites a few studies that identify strong correlations with malnutrition and anemia in children under five. Some strong predictors found by other studies are the child’s age, parents’ education level, household economic class, geographic location, household food availability, child birth size, family size, maternal age, and more. - The introduction states that this topic is relevant because although there is already existing research on malnutrition and anemia in African countries, there is not much literature studying the relationship between the two health conditions. This study aims to analyze the relationship between malnutrition and anemia in children in addition to identifying strong predictors of the conditions.\nData: - Data is supplied by the 2019/20 Rwanda Demographic and Health Survey. The researchers obtained the samples themselves, and the sampling method is described in depth. The researchers chose 500 clusters from different areas all over the country, and then households were selected at random from these clusters to be surveyed. The resulting data consisted of 3205 data points consisting of data about children under the age of 5.\nSolution to problem: - Researchers used a bivariate binary logistic regression model. This model helps understand the relationship between the outcome variables, presence of malnutrition and presence of anemia. The outcome variables are both binary, taking on a value of 1 for present, or 0 for not present. The predictors consist of about 26 variables relating to the child’s health conditions, family information, details about the parents, and relevant geographic information. - Three models are presented. The first model is the bivariate binary logistic regression model. The second model is the equivalent, but it is in the form of the log odds. The third model discussed is the odds ratio, and it is used to assess the relationship between categorical predictors in the model. - The researchers used SPSS and R software to perform the analyses.\nResults: - Results shows that nearly half of the study participants had anemia and about one fifth had malnutrition. - Six significant predictors were found: mother’s age, drinking water, other children in household, child gender, birth order, and gender of household head. - The odds ratio was a value that was not 1, which indicates that the outcome variables are not statistically independent. The relationship that exists between the outcome variables is significant. - The goodness fit test used was a proportion of correct predictions to the number of observations. This result was about 89%, so the researchers conclude that the model was a good fit. - There is a discussion about possible causes of the significant relationship that was found between malnutrition and anemia in children under 5. Researchers cite other facts and figures about why these health conditions are strongly correlated.\nConclusion: - Increasing maternal education, supplementing with vitamin A and other nutrient dense foods, providing a healthy/ clean/ safe environment, and decreasing maternal anemia may help improve rates of malnutrition and anemia in children.\nLimitations of the study: - The only limitation discussed is that the data collected may be prone to errors. This means researchers can conclude there are strong correlations, but it cannot be stated that any of the relationships are causal."
  },
  {
    "objectID": "posts/kristina-blog-post-week3/index.html#article-1",
    "href": "posts/kristina-blog-post-week3/index.html#article-1",
    "title": "Literature Review Week 3",
    "section": "",
    "text": "Title: Determinants of coexistence of undernutrition and anemia among under- five children in Rwanda; evidence from 2019/20 demographic health survey: Application of bivariate binary logistic regression model. (1)\nAuthors: Abebew Aklog smare, Yitateku Adugna Agmas\nProblem: - In children under five years of age, malnutrition and anemia have been an ongoing problem in many African countries. This paper focuses on studying malnutrition and anemia in Rwanda in particular; Rwanda went through a civil war, and after the war, rates of malnutrition and anemia decreased as the country was rebuilt. However, different parts of the country experienced faster or slower rates of improvement of malnutrition and anemia after the war, and it is unclear which factors are associated with improved rates of malnutrition and anemia. This study aims to identify key predictors correlated with these health ailments, so that the areas of the country still suffering from high rates of these health problems can be given the correct types of aid to fix the problem of malnutrition and anemia. - The introduction of the paper cites a few studies that identify strong correlations with malnutrition and anemia in children under five. Some strong predictors found by other studies are the child’s age, parents’ education level, household economic class, geographic location, household food availability, child birth size, family size, maternal age, and more. - The introduction states that this topic is relevant because although there is already existing research on malnutrition and anemia in African countries, there is not much literature studying the relationship between the two health conditions. This study aims to analyze the relationship between malnutrition and anemia in children in addition to identifying strong predictors of the conditions.\nData: - Data is supplied by the 2019/20 Rwanda Demographic and Health Survey. The researchers obtained the samples themselves, and the sampling method is described in depth. The researchers chose 500 clusters from different areas all over the country, and then households were selected at random from these clusters to be surveyed. The resulting data consisted of 3205 data points consisting of data about children under the age of 5.\nSolution to problem: - Researchers used a bivariate binary logistic regression model. This model helps understand the relationship between the outcome variables, presence of malnutrition and presence of anemia. The outcome variables are both binary, taking on a value of 1 for present, or 0 for not present. The predictors consist of about 26 variables relating to the child’s health conditions, family information, details about the parents, and relevant geographic information. - Three models are presented. The first model is the bivariate binary logistic regression model. The second model is the equivalent, but it is in the form of the log odds. The third model discussed is the odds ratio, and it is used to assess the relationship between categorical predictors in the model. - The researchers used SPSS and R software to perform the analyses.\nResults: - Results shows that nearly half of the study participants had anemia and about one fifth had malnutrition. - Six significant predictors were found: mother’s age, drinking water, other children in household, child gender, birth order, and gender of household head. - The odds ratio was a value that was not 1, which indicates that the outcome variables are not statistically independent. The relationship that exists between the outcome variables is significant. - The goodness fit test used was a proportion of correct predictions to the number of observations. This result was about 89%, so the researchers conclude that the model was a good fit. - There is a discussion about possible causes of the significant relationship that was found between malnutrition and anemia in children under 5. Researchers cite other facts and figures about why these health conditions are strongly correlated.\nConclusion: - Increasing maternal education, supplementing with vitamin A and other nutrient dense foods, providing a healthy/ clean/ safe environment, and decreasing maternal anemia may help improve rates of malnutrition and anemia in children.\nLimitations of the study: - The only limitation discussed is that the data collected may be prone to errors. This means researchers can conclude there are strong correlations, but it cannot be stated that any of the relationships are causal."
  },
  {
    "objectID": "posts/kristina-blog-post-week3/index.html#article-2",
    "href": "posts/kristina-blog-post-week3/index.html#article-2",
    "title": "Literature Review Week 3",
    "section": "Article 2",
    "text": "Article 2\nArticle Title: Using Binary logistic Regression to Detect Health Insurance Fraud. (2)\nAuthor: Baraah Samara, Ph.D. Student\nProblem: - Insurance fraud in health insurance industry. Specifically, patients treated at private clinics or hospitals. - Intro of the article explains why this topic is relevant. Between 1965 and 2008, the cost of healthcare increased significantly, and as a result, health insurance fraud has increased. There needs to be effective tools at detecting this fraud. - If fraud is decreased, it will help the economy as a whole, it will help insurance companies, and it will lower premium payments made by customers. - Fraud is committed by three types of entities: consumer, provider, and payer fraud. - Literature review cites common predictors of health insurance fraud: diagnoses, service cost, number of claims from individual, greatest costing claim, probability of anomaly, excessive charges by care facilities, and more.\nData: - Original dataset contained 26 independent variables - Data was collected from a time span of January 2022 through November 2022 - about 123,000 data points with no missing values. - The predictors are of varying types, including numerical, categorical, and binary values - The dependent variable is fraud, with a value of 1 for fraud present, or 0 for no fraud present.\nSolution to Problem:\nWhy they selected this model: - Building a binary logistic regression model to detect health insurance fraud - logistic regression is selected as the analytic technique because they want to assess effects of categorical variables on a categorical dependent variable. They also cite that logistic regression is the most accurate type of regression model with the kind of classification they are performing in this study. - Fraud detection commonly employs binary prediction models - The logistic regression model also provides estimates between 0 and 1, which help investigators estimate probability of fraud - Researchers provide the equation they use to calculate the log odds of the event of interest (occurrence of fraud)\nThe method: - They calculate likelihood of an individual committing fraud - They calculate total cost accrued by an individual. Then they perform the logistic regression using this calculation - The model works by identifying outliers and classifies them as potential fraudulent activity - Before testing the model, researchers hypothesize that there will be a positive relationship between overall cost accrued by patient and likelihood of fraud. Costs include doctor visit costs, prescription drug costs, lab costs, costs of medical symptoms, and total cost of expensive prescriptions. - When running the model, none of the coefficients were zero, which means that there exists a significant relationship between the outcome and the predictor variables. - Predictors were tested for multicollinearity before the model was run. Pearson correlation coefficients were obtained, and any predictors with a Pearson value of greater than 0.8 were excluded from the model. Only eight predictors remained after removing the predictors that were strongly correlated. - Different models were constructed using only the most important predictors. When taking away the least important predictor, the log likelihood was calculated to assess the accuracy of the model. The best performing model contained six of the original predictors.\nResults: - Six predictors were found to be significant in predicting health insurance fraud. The predictors are office visit cost, prescription costs, lab costs, symptom cost, and two expensive prescription drug costs. - There is a thorough interpretation of model slopes. For example, “the likelihood of fraud increases by .005188 for every unit increase in pharmacy cost.” Interpretations for the most significant predictors are given in this way. - A Chi-Square test for independence was used to determine whether at least one predictor was significantly related to the outcome. It was concluded that at least one of the six predictors was significant. - The model was found to be about 99% accurate when predicting no fraud, but only about 76% accurate when predicting fraud. - An example is included that shows how to calculate the probability that an individual will commit insurance fraud, given values for the six predictors in the equation. - The study concludes by stating the importance and relevance of continuing to develop new fraud detection models.\nLimitations: - No limitations are explicitly stated in this paper. However, it can be considered a limitation that only data from middle eastern countries was used in the study. To make the results of the study more generalizable, data from other regions of the world should be included in a more comprehensive study.\n\nReferences\n\n\n1. Asmare AA, Agmas YA. Determinants of coexistence of undernutrition and anemia among under-five children in rwanda; evidence from 2019/20 demographic health survey: Application of bivariate binary logistic regression model. Plos one. 2024;19(4):e0290111. \n\n\n2. Samara B. Using binary logistic regression to detect health insurance fraud. Pakistan Journal of Life & Social Sciences. 2024;22(2)."
  },
  {
    "objectID": "posts/kristina-blog-post-week4/index.html",
    "href": "posts/kristina-blog-post-week4/index.html",
    "title": "Literature Review Week 4",
    "section": "",
    "text": "Article Title: Exploring the medical decision-making patterns and influencing factors among the general Chinese public: a binary logistic regression analysis. (1)\nAuthors: Yuwen Lyu, Qian Xu, Junrong Liu\nProblem: - Researchers are seeking to understand top driving factors behind decisions made about healthcare and medical issues. The population of interest is the general Chinese public. - Previous research in this field has identified two main types of medical decision making: unilateral and collaborative decision making - Unilateral decision making means there is one main entity making the medical decision, such as a single patient, a patient’s family, or a doctor. Previous research shows that patient families have a very strong influence over a patient’s medical decisions. - Collaborative decision making means there are two or more parties involved in the decision making process. Three subgroups are defined: doctor group, doctor- patient group, patient- family group, and patient-doctor-family group. - There is a lack in research in this field. More needs to be known about factors that play a role in medical decision making processes. This study’s results will be generalizable to China as well as nations around the world.\nSolution: - Use binary logistic regression to classify points into two categories: unilateral decision making (value of 1), or collaborative decision making (value of 0) - This model is ideal because it takes into account variable interactions. Also used often in the medical field - The equation of the model is given. It is in the form of the log odds of the desired event happening.\nData: - 2696 data points with attributes including age, education, occupation, family situation, religion, economic status and medical payment methods - Data was collected via survey and included only residents of China from 31 provinces - The data was gathered by the researchers that wrote this study - A power analysis was conducted to determine how many data points would be needed in order to have reliable results after statistical analysis. A G-power test showed that only 2040 valid data points were needed\nResults: - Survey results showed that 30% of responses were categorized as unilateral decision making, while 70% were categorized as collaborative decision making. The top category of unilateral decision making was doctor- led decisions, while the top category for collaborative decision making was patient- doctor- family decisions. - Significant predictors were identified with p-values less than 0.05. Significant predictors of unilateral decision making were gender, education level, family status, and religious beliefs. Different occupations also significantly predicted unilateral decision making. - Odds ratios are given for some predictors, with researchers stating that certain categories of specific predictors are x.xx times more likely than the reference group to be a unilateral decision maker. - The significance of the regression model’s intercept is interpreted, and it is significant. This means when all variables are at their reference levels, there is a low likelihood of the outcome variable taking on a value of a unilateral decision making process. - The goodness of fit test used in this study is McFadden’s R-squared value. The value was0.065, and researchers state that this value shows a good fit of the model. It is explained that R squared values for studies in the social sciences are rarely ever close to a perfect fit.\nConclusions: - The researchers discuss why there are contrasting results from this study versus studies in Western countries. They identify different cultural values in different geographic regions, which ultimately lead to different medical decision making processes. - Results are discussed more in depth, with researchers attempting to identify causes behind the correlations that were identified.\nLimitations: - The study’s data is solely from China, so results may not be generalizable to global populations. - The binary logistic regression model may not be complex enough to account for the complexities of all the predictors involved in healthcare decision making. Researchers suggest using more complex models in future research."
  },
  {
    "objectID": "posts/kristina-blog-post-week4/index.html#article-1",
    "href": "posts/kristina-blog-post-week4/index.html#article-1",
    "title": "Literature Review Week 4",
    "section": "",
    "text": "Article Title: Exploring the medical decision-making patterns and influencing factors among the general Chinese public: a binary logistic regression analysis. (1)\nAuthors: Yuwen Lyu, Qian Xu, Junrong Liu\nProblem: - Researchers are seeking to understand top driving factors behind decisions made about healthcare and medical issues. The population of interest is the general Chinese public. - Previous research in this field has identified two main types of medical decision making: unilateral and collaborative decision making - Unilateral decision making means there is one main entity making the medical decision, such as a single patient, a patient’s family, or a doctor. Previous research shows that patient families have a very strong influence over a patient’s medical decisions. - Collaborative decision making means there are two or more parties involved in the decision making process. Three subgroups are defined: doctor group, doctor- patient group, patient- family group, and patient-doctor-family group. - There is a lack in research in this field. More needs to be known about factors that play a role in medical decision making processes. This study’s results will be generalizable to China as well as nations around the world.\nSolution: - Use binary logistic regression to classify points into two categories: unilateral decision making (value of 1), or collaborative decision making (value of 0) - This model is ideal because it takes into account variable interactions. Also used often in the medical field - The equation of the model is given. It is in the form of the log odds of the desired event happening.\nData: - 2696 data points with attributes including age, education, occupation, family situation, religion, economic status and medical payment methods - Data was collected via survey and included only residents of China from 31 provinces - The data was gathered by the researchers that wrote this study - A power analysis was conducted to determine how many data points would be needed in order to have reliable results after statistical analysis. A G-power test showed that only 2040 valid data points were needed\nResults: - Survey results showed that 30% of responses were categorized as unilateral decision making, while 70% were categorized as collaborative decision making. The top category of unilateral decision making was doctor- led decisions, while the top category for collaborative decision making was patient- doctor- family decisions. - Significant predictors were identified with p-values less than 0.05. Significant predictors of unilateral decision making were gender, education level, family status, and religious beliefs. Different occupations also significantly predicted unilateral decision making. - Odds ratios are given for some predictors, with researchers stating that certain categories of specific predictors are x.xx times more likely than the reference group to be a unilateral decision maker. - The significance of the regression model’s intercept is interpreted, and it is significant. This means when all variables are at their reference levels, there is a low likelihood of the outcome variable taking on a value of a unilateral decision making process. - The goodness of fit test used in this study is McFadden’s R-squared value. The value was0.065, and researchers state that this value shows a good fit of the model. It is explained that R squared values for studies in the social sciences are rarely ever close to a perfect fit.\nConclusions: - The researchers discuss why there are contrasting results from this study versus studies in Western countries. They identify different cultural values in different geographic regions, which ultimately lead to different medical decision making processes. - Results are discussed more in depth, with researchers attempting to identify causes behind the correlations that were identified.\nLimitations: - The study’s data is solely from China, so results may not be generalizable to global populations. - The binary logistic regression model may not be complex enough to account for the complexities of all the predictors involved in healthcare decision making. Researchers suggest using more complex models in future research."
  },
  {
    "objectID": "posts/kristina-blog-post-week4/index.html#article-2",
    "href": "posts/kristina-blog-post-week4/index.html#article-2",
    "title": "Literature Review Week 4",
    "section": "Article 2",
    "text": "Article 2\nArticle Title: Predictors of hospital admission when presenting with acute on chronic breathlessness: Binary logistic regression. (2)\nAuthors: Ann Hutchinson, Alastair Pickering, Paul Williams, Miriam Johnson\nProblem: - People presenting to the emergency room with breathlessness often do not require hospitalization and can be sent home. Only an average of 50% to 67% of these patients require admittance to the hospital. This research focuses on patients presenting with breathlessness to the ER and seeks to identify significant predictors of hospitalization of these individuals. Doctors and emergency department staff would benefit from identifying predictors present in individuals that would need to be admitted to the hospital.\nSolution: - Researchers used a binary logistic regression analysis to identify the predictors most strongly correlated with patients with breathlessness being admitted to the hospital from the ER. - First, in order to identify which predictors to include in the binary logistic regression, researchers analyzed 48 total predictors individually. They conducted a separate univariate analysis for each variable to see which were most significantly correlated with hospitalization from the ER. Only seven predictors were significant, and of those, only five were included in the final model (due to eliminating variables with strong multicollinearity).\nData: - Data was collected from a single hospital from December 2015 to May 2015. - Only 171 datapoints are included, as only 171 patients with acute breathlessness consented to have their survey used for research - Predictors included: demographics, preexisting medical conditions, severity of breathlessness, and other vital signs - To determine which predictors were most important, researchers used a stepwise backward elimination process, and excluded one predictor at a time. It was determined that only five predictors were needed. - After univariate analysis, researchers constructed a binary logistic regression model incorporating all of the selected predictors.\nResults and Conclusions: - Results are presented with an odds ratio for every predictor in the model. The odds of being admitted to the hospital increased by a certain factor for every one unit increase in a specific predictor. - The odds of admission to the hospital were positively correlated with age, talking to a doctor about symptoms, and the presence of preexisting heart conditions. The odds of being admitted to the hospital were negatively associated with blood oxygen levels. - The researchers state that this study is meant to only be exploratory and the results should not be used for making future predictions. Rather, the results should be used to aid in identifying strong predictors so these variables can be included in future similar studies. - Results of this study are compared to results of other studies, and the findings are consistent. A consistent predictor of hospital admission from this study and other studies includes older age. Another common predictor is tachycardia, but this study did not include this information.\nLimitations: - The study’s data is very limited. The data is supplied from just one hospital, and there were only 171 data points analyzed. The results of the study are therefore not as generalizable to global populations as a study analyzing broader populations. - The data also did not include responses from patients with very severe breathlessness because their state of health was too severe for them to be able to complete a survey. So the results of this study do not reflect patients with extreme symptoms.\n\nReferences\n\n\n1. Lyu Y, Xu Q, Liu J. Exploring the medical decision-making patterns and influencing factors among the general chinese public: A binary logistic regression analysis. BMC public health. 2024;24(1):887. \n\n\n2. Hutchinson A, Pickering A, Williams P, Johnson M. Predictors of hospital admission when presenting with acute-on-chronic breathlessness: Binary logistic regression. PLoS One. 2023;18(8):e0289263."
  },
  {
    "objectID": "posts/shree-blog-post-week3/index.html",
    "href": "posts/shree-blog-post-week3/index.html",
    "title": "Literature Review Week 3",
    "section": "",
    "text": "Modeling Road Accident Severity with Logistic Regression (comparison study) (1)\nLink: https://www.mdpi.com/2078-2489/11/5/270\nGoal: the goial was to understand any major outcomes that would happen (deaths or serious injuries) or small misfortunes, such as property damage and light casualties,. this is exciting research as transportation planners, governments, and law enforcement can make focused safety policies, like stricter enforcement, better road design, or public awareness campaigns, by knowing what factors affect how bad an accident is likely to be, like drunk driving, weather, and time of day.\nmethods used and approach\ndataset: Fatality Analysis Reporting System (FARS) predators: Driver demographics (age, gender) Environmental factors (weather, road condition, time of day, lighting) Driving behaviors (speeding, alcohol involvement, seat belt use) Vehicle types (motorcycles, trucks, cars)\nto distinguish between severe and non-severe accidents, logistic regression was used. To assess trade-offs between interpretability and predictive capability, models were contrasted with gradient boosting machines (GBMs) and decision trees. Performance was compared using metrics like accuracy, precision, recall, and AUC. Clear, comprehensible patterns were found using logistic regression: Severity was greatly worsened by low lighting, inclement weather, and night driving. Two of the best indicators of fatal collisions were speeding and alcohol use. Compared to other vehicles, motorcycles posed a disproportionately high severity risk. The prediction accuracy of tree-based models (GBM) was somewhat greater, but the results of logistic regression were clear and understandable.\nthere were some bad aspect of the appracoach or lets say disadvantage of technique used as: nonlinear interactions are not captured: for example, bad weather and night driving together may have a worse effect than additive driving, but conventional LR ignores this. data imbalance- Without corrections, logistic regression may become skewed toward the majority class; severe crashes are less common than non-severe ones."
  },
  {
    "objectID": "posts/shree-blog-post-week3/index.html#article-1",
    "href": "posts/shree-blog-post-week3/index.html#article-1",
    "title": "Literature Review Week 3",
    "section": "",
    "text": "Modeling Road Accident Severity with Logistic Regression (comparison study) (1)\nLink: https://www.mdpi.com/2078-2489/11/5/270\nGoal: the goial was to understand any major outcomes that would happen (deaths or serious injuries) or small misfortunes, such as property damage and light casualties,. this is exciting research as transportation planners, governments, and law enforcement can make focused safety policies, like stricter enforcement, better road design, or public awareness campaigns, by knowing what factors affect how bad an accident is likely to be, like drunk driving, weather, and time of day.\nmethods used and approach\ndataset: Fatality Analysis Reporting System (FARS) predators: Driver demographics (age, gender) Environmental factors (weather, road condition, time of day, lighting) Driving behaviors (speeding, alcohol involvement, seat belt use) Vehicle types (motorcycles, trucks, cars)\nto distinguish between severe and non-severe accidents, logistic regression was used. To assess trade-offs between interpretability and predictive capability, models were contrasted with gradient boosting machines (GBMs) and decision trees. Performance was compared using metrics like accuracy, precision, recall, and AUC. Clear, comprehensible patterns were found using logistic regression: Severity was greatly worsened by low lighting, inclement weather, and night driving. Two of the best indicators of fatal collisions were speeding and alcohol use. Compared to other vehicles, motorcycles posed a disproportionately high severity risk. The prediction accuracy of tree-based models (GBM) was somewhat greater, but the results of logistic regression were clear and understandable.\nthere were some bad aspect of the appracoach or lets say disadvantage of technique used as: nonlinear interactions are not captured: for example, bad weather and night driving together may have a worse effect than additive driving, but conventional LR ignores this. data imbalance- Without corrections, logistic regression may become skewed toward the majority class; severe crashes are less common than non-severe ones."
  },
  {
    "objectID": "posts/shree-blog-post-week3/index.html#article-2",
    "href": "posts/shree-blog-post-week3/index.html#article-2",
    "title": "Literature Review Week 3",
    "section": "Article 2",
    "text": "Article 2\nPredicting Uber Demand Using Spatio-Temporal Features and Logistic Regression (2017) (2)\nLink: https://escholarship.org/content/qt80q5f8t9/qt80q5f8t9_noSplash_59a1830fd88a360df43b9c6aff1446c7.pdf\nGoal: to forecast if Uber demand would outpace supply at a specific place and time window (for example, fifteen minutes in advance), resulting in an increase in pricing. Classifying each region for price changewas the classification challenge.\nMethodology:\nWhen demand exceeds supply within a zone or period, labels for “surge” events are created. surge/no surge classification using a logistic regression model. compared the effectiveness of Random Forests with Support Vector Machines (SVMs). evaluated on training/test splits using cross-validation.\nResult:\nAs a baseline model, logistic regression did fairly well, identifying significant demand trends such as 1. busy hours in the morning and evening. 2. Manhattan’s nightlife during weekends. 3. weather peaks (demand was greatly raised by rain).\nHowever, logistic regression was marginally outperformed by more sophisticated models (random forest, SVM), particularly when it came to capturing nonlinearities and interactions.\nThe good aspect of research :\nInterpretability: Coefficients showed which characteristics influenced demand (for example, rainfall significantly raised the likelihood of a surge). Low computational cost: LR trained quickly on a sizable NYC dataset, in contrast to more intricate models. Scalability: As an early warning system, a straightforward model may be quickly implemented for real-time demand forecasts.\nLimitiation or what the analysic could not get right:\nGeographic restrictions: Patterns may not transfer to smaller cities or suburban settings because they were trained in New York City. Limitation of binary classification: The model merely forecasted a surge or no surge; however, actual demand is continuous. More good models forecast the magnitude of the surge. Poor performance compared to more sophisticated models: Random forest produced higher accuracy by better capturing interactions.\n\nReferences\n\n\n1. Chen MM, Chen MC. Modeling road accident severity with comparisons of logistic regression, decision tree and random forest. Information. 2020;11(5):270. \n\n\n2. Faghih S, Safikhani A, Moghimi B, Kamga C. Predicting short-term uber demand in new york city using spatiotemporal modeling. Journal of Computing in Civil Engineering. 2019;33(3):05019002."
  },
  {
    "objectID": "posts/shree-blog-post-week2/index.html",
    "href": "posts/shree-blog-post-week2/index.html",
    "title": "Literature Review Week 2",
    "section": "",
    "text": "link: https://robertominguez.altervista.org/DocumentacionAcreditativa/Articulos/GuancheMM13.pdf\nAutoregressive Logistic Regression Applied to Atmospheric Circulation Patterns” (Guanche, Mínguez & Méndez, 2013). (1)\nArticel incorporates autoregressive time dependencies into logistic regression for climate modeling. work with complex climatological dynamics instead of common data set like health or business. Explains both interpretation and simulation capabilities for weather patterns.\nData used : They took data of measured sea-level pressure (SLP) fields to determine daily atmospheric circulation patterns over the Northeastern Atlantic. A limited number of circulation types (weather regimes) are created by summarizing the SLP fields, for example, through clustering or categorization. Data setup for the autoregressive logistic regression applied to weather types” shows how they organized lagged types, covariates,\nSteps on summary: - Sort daily SLP data into distinct circulation categories. - create autoregressive terms, covariates, and lagged indicators (trend, seasonal) - Comparing anticipated and empirical probability allows for diagnostic checks. - Utilize the fitted model to replicate artificial circulatory state sequences. - Check for simulation statistics (frequencies, transitions, persistence) against historical data."
  },
  {
    "objectID": "posts/shree-blog-post-week2/index.html#article-1",
    "href": "posts/shree-blog-post-week2/index.html#article-1",
    "title": "Literature Review Week 2",
    "section": "",
    "text": "link: https://robertominguez.altervista.org/DocumentacionAcreditativa/Articulos/GuancheMM13.pdf\nAutoregressive Logistic Regression Applied to Atmospheric Circulation Patterns” (Guanche, Mínguez & Méndez, 2013). (1)\nArticel incorporates autoregressive time dependencies into logistic regression for climate modeling. work with complex climatological dynamics instead of common data set like health or business. Explains both interpretation and simulation capabilities for weather patterns.\nData used : They took data of measured sea-level pressure (SLP) fields to determine daily atmospheric circulation patterns over the Northeastern Atlantic. A limited number of circulation types (weather regimes) are created by summarizing the SLP fields, for example, through clustering or categorization. Data setup for the autoregressive logistic regression applied to weather types” shows how they organized lagged types, covariates,\nSteps on summary: - Sort daily SLP data into distinct circulation categories. - create autoregressive terms, covariates, and lagged indicators (trend, seasonal) - Comparing anticipated and empirical probability allows for diagnostic checks. - Utilize the fitted model to replicate artificial circulatory state sequences. - Check for simulation statistics (frequencies, transitions, persistence) against historical data."
  },
  {
    "objectID": "posts/shree-blog-post-week2/index.html#article-2",
    "href": "posts/shree-blog-post-week2/index.html#article-2",
    "title": "Literature Review Week 2",
    "section": "Article 2",
    "text": "Article 2\nA Descriptive Study of Variable Discretization and Cost-Sensitive Logistic Regression on Imbalanced Credit Data. (2)\nLink: https://arxiv.org/pdf/1812.10857\nPurpose : The author looks out for a problem related to credit scoring where the minority class (defaults and delinquencies) is comparatively uncommon. Their main concept is to contrast cost-sensitive logistic regression (assigning various misclassification fees) with variable discretization (converting continuous predictors into categorical bins) in order to reduce class bias. When used real credit scoring dataset event rate was 6.68% that was highly imbalanced.\nModels used: - trained logestic regression in different versions\n- Standard logistic regression on continuous predictors (baseline). - Logistic regression on discretized predictors (using different binning strategies). - Cost-sensitive logistic regression on the continuous predictors (i.e. weight adjustments for minority class). - Possibly combined approaches (discretized + cost-sensitive). - They use 10-fold cross-validation to ensure robustness. PMC - Performance metrics include: - ROC / AUC - Type I error (false positive rate) - Type II error (false negative rate) - Accuracy - F1 score - They also examine coefficient estimates and interpretability\nSummary:\nIn their study of unbalanced credit scoring (default rate ~6.68%), Zhang et al. used 10-fold CV to examine standard, discretized, and cost-sensitive logistic regression. They discovered that variable discretization works better than cost-sensitive weighting, producing models that are more resilient, stable, and interpretable. These models also generalize well to other domains, such as biology and wine quality.\n\nReferences\n\n\n1. Guanche Y, Mı́nguez R, Méndez FJ. Autoregressive logistic regression applied to atmospheric circulation patterns. Climate dynamics. 2014;42(1):537–52. \n\n\n2. Zhang L, Ray H, Priestley J, Tan S. A descriptive study of variable discretization and cost-sensitive logistic regression on imbalanced credit data. Journal of Applied Statistics. 2020;47(3):568–81."
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Logistic Regression Group - Report",
    "section": "",
    "text": "The report goes here need some work\nSlides: slides.html"
  },
  {
    "objectID": "report.html#introduction",
    "href": "report.html#introduction",
    "title": "Logistic Regression Group - Report",
    "section": "Introduction",
    "text": "Introduction\nLorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.\nLorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.\n\nReferences"
  },
  {
    "objectID": "slides.html#introduction",
    "href": "slides.html#introduction",
    "title": "Present a great story for data science projects",
    "section": "Introduction",
    "text": "Introduction\n\nDevelop a storyline that captures attention and maintains interest.\nYour audience is your peers\nClearly state the problem or question you’re addressing.\nIntroduce why it is relevant needs.\nProvide an overview of your approach.\n\nIn kernel estimator, weight function is known as kernel function (1). Cite this paper (2). The GEE (3). The PCA (4)*"
  },
  {
    "objectID": "slides.html#methods",
    "href": "slides.html#methods",
    "title": "Present a great story for data science projects",
    "section": "Methods",
    "text": "Methods\n\nDetail the models or algorithms used.\nJustify your choices based on the problem and data."
  },
  {
    "objectID": "slides.html#data-exploration-and-visualization",
    "href": "slides.html#data-exploration-and-visualization",
    "title": "Present a great story for data science projects",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\n\nDescribe your data sources and collection process.\nPresent initial findings and insights through visualizations.\nHighlight unexpected patterns or anomalies."
  },
  {
    "objectID": "slides.html#data-exploration-and-visualization-1",
    "href": "slides.html#data-exploration-and-visualization-1",
    "title": "Present a great story for data science projects",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\nA study was conducted to determine how…"
  },
  {
    "objectID": "slides.html#modeling-and-results",
    "href": "slides.html#modeling-and-results",
    "title": "Present a great story for data science projects",
    "section": "Modeling and Results",
    "text": "Modeling and Results\n\nExplain your data preprocessing and cleaning steps.\nPresent your key findings in a clear and concise manner.\nUse visuals to support your claims.\nTell a story about what the data reveals."
  },
  {
    "objectID": "slides.html#conclusion",
    "href": "slides.html#conclusion",
    "title": "Present a great story for data science projects",
    "section": "Conclusion",
    "text": "Conclusion\n\nSummarize your key findings.\nDiscuss the implications of your results."
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "Present a great story for data science projects",
    "section": "References",
    "text": "References\n\n\n1. Efromovich S. Nonparametric curve estimation: Methods, theory, and applications [Internet]. Springer New York; 2008. (Springer series in statistics). Available from: https://books.google.com/books?id=mdoLBwAAQBAJ\n\n\n2. Bro R, Smilde AK. Principal component analysis. Analytical methods. 2014;6(9):2812–31. \n\n\n3. Wang M. Generalized estimating equations in longitudinal data analysis: A review and recent developments. Advances in Statistics. 2014;2014. \n\n\n4. Daffertshofer A, Lamoth CJ, Meijer OG, Beek PJ. PCA in studying coordination and variability: A tutorial. Clinical biomechanics. 2004;19(4):415–28."
  },
  {
    "objectID": "people/barbosa-renan/index.html#education",
    "href": "people/barbosa-renan/index.html#education",
    "title": "Renan Monteiro Barbosa",
    "section": "Education",
    "text": "Education\nB.S. Mechanical Engineering | Univevrsity of West Florida"
  },
  {
    "objectID": "people/basnet-shree/index.html#education",
    "href": "people/basnet-shree/index.html#education",
    "title": "Shree Krishna Basnet",
    "section": "Education",
    "text": "Education\nB.S. Mechanical Engineering | Univevrsity of West Florida"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Literature Review",
    "section": "",
    "text": "These are the literature review done by all the students during this semester.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 2\n\n\n\nliterature review\n\nweek 2\n\n\n\nLiterature review for the Week 2 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nKristina Kusem\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 2\n\n\n\nliterature review\n\nweek 2\n\n\n\nLiterature review for the Week 2 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 2\n\n\n\nliterature review\n\nweek 2\n\n\n\nLiterature review for the Week 2 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nShree Krishna Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 3\n\n\n\nliterature review\n\nweek 3\n\n\n\nLiterature review for the Week 3 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nKristina Kusem\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 3\n\n\n\nliterature review\n\nweek 3\n\n\n\nLiterature review for the Week 3 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 3\n\n\n\nliterature review\n\nweek 3\n\n\n\nLiterature review for the Week 3 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nShree Krishna Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 4\n\n\n\nliterature review\n\nweek 4\n\n\n\nLiterature review for the Week 4 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 4\n\n\n\nliterature review\n\nweek 4\n\n\n\nLiterature review for the Week 4 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nKristina Kusem\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 4\n\n\n\nliterature review\n\nweek 4\n\n\n\nLiterature review for the Week 4 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nShree Krishna Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 5\n\n\n\nliterature review\n\nweek 5\n\n\n\nLiterature review for the Week 5 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nShree Krishna Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 5\n\n\n\nliterature review\n\nweek 5\n\n\n\nLiterature review for the Week 5 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nKristina Kusem\n\n\n\n\n\nNo matching items"
  }
]