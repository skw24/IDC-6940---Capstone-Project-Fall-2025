{
  "hash": "ee5982e5541f31978691889d78c74aee",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Reproducing Steve's Code - Week 7\"\ndescription: \"For Week 7 we are replicating Steve's findings\"\nauthor:\n  - name: Renan Monteiro Barbosa\n    url: https://github.com/renanmb\n    affiliation: Master of Data Science Program @ The University of West Florida (UWF)\n    # affiliation-url: https://ucsb-meds.github.io/\n# date: 10-24-2022\ncategories: [coding, week 7, renan]\n# citation:\n#   url: https://samanthacsik.github.io/posts/2022-10-24-my-blog-post/\nimage: images/spongebob-imagination.jpg\ndraft: false\nbibliography: references.bib\nlink-citations: true\n---\n\nFor the Week 7 we will be reproducing Steve's findings with the dataset @fedesorianoStrokePredictionDatasetKaggle. \n\nYou can download the Dataset from the following link: [Stroke Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset)\n\n\n## 1. Setup and Data Loading\n\nFirst we need to install all packages, system dependencies and solve conflicts to produce a new renv.lock file.\n\n### 1.1 Load Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Run this once to install all the necessary packages\n# install.packages(c(\"corrplot\", \"ggpubr\", \"caret\", \"mice\", \"ROSE\", \"ranger\", \"stacks\", \"tidymodels\"))\n# install.packages(\"themis\")\n# install.packages(\"xgboost\")\n# install.packages(\"gghighlight\")\n# install.packages(\"dplyr\")\n# install.packages(\"pscl\")\n# install.packages(\"parallelly\")\n# install.packages(\"cli\")\n# install.packages(\"car\")\n# install.packages(\"ResourceSelection\")\n```\n:::\n\n\nWe can use this to check installed packages:\n\n```{{r}}\nrenv::activate(\"website\")\n\"yardstick\" %in% rownames(installed.packages())\n```\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# For data manipulation and visualization\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(knitr)\nlibrary(ggpubr)\n\n# For data preprocessing and modeling\nlibrary(caret)\nlibrary(mice)\nlibrary(ROSE) # For SMOTE\nlibrary(ranger) # A fast implementation of random forests\n\n# For stacking/ensemble models\nlibrary(stacks)\nlibrary(tidymodels)\n\nlibrary(themis)\nlibrary(gghighlight)\n\nlibrary(dplyr)\nlibrary(pscl)\nlibrary(car)\nlibrary(ResourceSelection)\n\n# Set seed for reproducibility\nset.seed(123)\n```\n:::\n\n\n### 1.2 Load Data\n\nWill be using my original Dataset as well Steve's Dataset and compare for differences.\n\nRenan: kaggle_data1\nSteve: stroke1\n\n#### 1.2.1 Renan Dataset\n\nBelow will be loading the healthcare-dataset-stroke-data.csv and performing necessary changes to the dataset and loading into the DataFrame: kaggle_data1\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nfind_git_root <- function(start = getwd()) {\n  path <- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path <- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root <- find_git_root()\ndatasets_path <- file.path(repo_root, \"datasets\")\nkaggle_dataset_path <- file.path(datasets_path, \"kaggle-healthcare-dataset-stroke-data/healthcare-dataset-stroke-data.csv\")\nkaggle_data1 = read_csv(kaggle_dataset_path, show_col_types = FALSE)\n\n# unique(kaggle_data1$bmi)\nkaggle_data1 <- kaggle_data1 %>%\n  mutate(bmi = na_if(bmi, \"N/A\")) %>%   # Convert \"N/A\" string to NA\n  mutate(bmi = as.numeric(bmi))         # Convert from character to numeric\n\n# Remove the 'Other' gender row and the 'id' column\nkaggle_data1 <- kaggle_data1 %>%\n  filter(gender != \"Other\") %>%\n  select(-id) %>%\n  mutate_if(is.character, as.factor) # Convert character columns to factors for easier modeling\n```\n:::\n\n\n#### 1.2.1 Steve Dataset\n\nBelow will be loading the stroke.csv and performing necessary changes to the dataset and loading into the DataFrame: stroke1\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Reading the datafile in (the same one you got for us Renan)#\nsteve_dataset_path <- file.path(datasets_path, \"steve/stroke.csv\")\nstroke1 = read_csv(steve_dataset_path, show_col_types = FALSE)\n# stroke1 <- read.csv(\"D:\\\\stroke.csv\")\n```\n:::\n\n\nExploring Dataset so we can plan on how to proceed and possible changes.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Reveiewing the columns of the data and the dataset size#\nhead(stroke1)\nnrow(stroke1)\n#Some data to look at the data in each column#\nsummary(stroke1)\ncount_tables <- lapply(stroke1, table)\ncount_tables\n```\n:::\n\n\nPreparing the Dataset\n\nFor each Column...removing the unncessary or unusable variables:\n1. **Smoking Status** - remove unknown\n1. **bmi** - remove N/A\n3. **Work type** - remove children\n4. **age** create numerical variable with 2 places after the decimal\n5. **gender** -remove other\n\nIn each column..that has data points that are not usable, recoding those datapoints to become\"N/A\"\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nstroke1[stroke1 == \"N/A\"] <- NA\nstroke1[stroke1 == \"Unknown\"] <- NA\nstroke1[stroke1 == \"children\"] <- NA\nstroke1[stroke1 == \"other\"] <- NA\n```\n:::\n\n\nfor BMI changing the variable type to numeric and formatting the data point to 2 places ater the decimal\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nstroke1$bmi <- round(as.numeric(stroke1$bmi), 2)\n```\n:::\n\n\nFor Gender, changning Male to 1 and Female to 2, then reformatting gender as numeric\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nstroke1$gender[stroke1$gender == \"Male\"] <- 1\nstroke1$gender[stroke1$gender == \"Female\"] <- 2\nstroke1$gender <- as.numeric(stroke1$gender)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: NAs introduced by coercion\n```\n\n\n:::\n:::\n\n\nFor ever_married, changing yes to 1 and No to 2, the reformatting the variable ever_married to numeric\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nstroke1$ever_married[stroke1$ever_married == \"Yes\"] <- 1\nstroke1$ever_married[stroke1$ever_married == \"No\"] <- 2\nstroke1$ever_married <- as.numeric(stroke1$ever_married)\n```\n:::\n\n\nFor work type recoding Govt_job to 1, Private to 3, Self-employed to 3, and Never_worked to 4, then reformatting work_type to numeric\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nstroke1$work_type[stroke1$work_type == \"Govt_job\"] <- 1\nstroke1$work_type[stroke1$work_type == \"Private\"] <- 2\nstroke1$work_type[stroke1$work_type == \"Self-employed\"] <- 3\nstroke1$work_type[stroke1$work_type == \"Never_worked\"] <- 4\nstroke1$work_type <- as.numeric(stroke1$work_type)\n```\n:::\n\n\nFor Residence_type, recoding urban to 1, Rural to 2, and then reformatting Residence type to Numeric\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nstroke1$Residence_type[stroke1$Residence_type == \"Urban\"] <- 1\nstroke1$Residence_type[stroke1$Residence_type == \"Rural\"] <- 2\nstroke1$Residence_type <- as.numeric(stroke1$Residence_type)\n```\n:::\n\n\nfor avg_glucose_level, heart_disease, and hypertension, reformattint the 3 variables to numeric\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nstroke1$avg_glucose_level <- as.numeric(stroke1$avg_glucose_level)\nstroke1$heart_disease <- as.numeric(stroke1$heart_disease)\nstroke1$hypertension <- as.numeric(stroke1$hypertension)\n```\n:::\n\n\nFor age, reformatting age to numeric and putting 2 places after the decimnals\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nstroke1$age <- round(as.numeric(stroke1$age), 2)\n```\n:::\n\n\nFor stroke, reformatting the variable stroke  to numeric\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nstroke1$stroke <- as.numeric(stroke1$stroke)\n```\n:::\n\n\nFor smoking_status, recoding never smoked to 1, formerly smoked to 2, and smokes to 3. The reformat the variable to numeric\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nstroke1$smoking_status[stroke1$smoking_status == \"never smoked\"] <- 1\nstroke1$smoking_status[stroke1$smoking_status == \"formerly smoked\"] <- 2\nstroke1$smoking_status[stroke1$smoking_status == \"smokes\"] <- 3\nstroke1$smoking_status <- as.numeric(stroke1$smoking_status)\n```\n:::\n\n\ndeleted to column ID from the dataset since its not needed for the analysis\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nstroke1 <- stroke1[, !(names(stroke1) %in% \"id\")]\n```\n:::\n\n\nrenameed stroke dataset without id to stroke1_clean\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nstroke1_clean <- na.omit(stroke1)\n```\n:::\n\n\nconverted all columns to numeric and removed id\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nstr(stroke1_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [3,357 × 11] (S3: tbl_df/tbl/data.frame)\n $ gender           : num [1:3357] 1 1 2 2 1 1 2 2 2 2 ...\n $ age              : num [1:3357] 67 80 49 79 81 74 69 81 61 54 ...\n $ hypertension     : num [1:3357] 0 0 0 1 0 1 0 1 0 0 ...\n $ heart_disease    : num [1:3357] 1 1 0 0 0 1 0 0 1 0 ...\n $ ever_married     : num [1:3357] 1 1 1 1 1 1 2 1 1 1 ...\n $ work_type        : num [1:3357] 2 2 2 3 2 2 2 2 1 2 ...\n $ Residence_type   : num [1:3357] 1 2 1 2 1 2 1 2 2 1 ...\n $ avg_glucose_level: num [1:3357] 229 106 171 174 186 ...\n $ bmi              : num [1:3357] 36.6 32.5 34.4 24 29 27.4 22.8 29.7 36.8 27.3 ...\n $ smoking_status   : num [1:3357] 2 1 3 1 2 1 1 1 3 3 ...\n $ stroke           : num [1:3357] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:1753] 2 9 10 14 20 24 28 30 32 39 ...\n  ..- attr(*, \"names\")= chr [1:1753] \"2\" \"9\" \"10\" \"14\" ...\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nnrow(stroke1_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3357\n```\n\n\n:::\n:::\n\n\n\n## 2. Apply Logistic Regression\n\nApplying Logistic Regression to Steve Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nLR_stroke1 <- stroke1_clean\n#Do Logistic Regression on dataset#\nmodel <- glm(stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, data=LR_stroke1, family = binomial)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = stroke ~ gender + age + hypertension + heart_disease + \n    ever_married + work_type + Residence_type + avg_glucose_level + \n    bmi + smoking_status, family = binomial, data = LR_stroke1)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       -8.426854   0.873243  -9.650  < 2e-16 ***\ngender             0.080370   0.167274   0.480 0.630893    \nage                0.070967   0.006845  10.368  < 2e-16 ***\nhypertension       0.570797   0.182580   3.126 0.001770 ** \nheart_disease      0.417884   0.220311   1.897 0.057856 .  \never_married       0.174316   0.261832   0.666 0.505569    \nwork_type         -0.109615   0.126101  -0.869 0.384703    \nResidence_type     0.005932   0.162188   0.037 0.970822    \navg_glucose_level  0.004658   0.001375   3.388 0.000704 ***\nbmi                0.006275   0.012875   0.487 0.625954    \nsmoking_status     0.179921   0.106431   1.691 0.090932 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1403.5  on 3356  degrees of freedom\nResidual deviance: 1145.4  on 3346  degrees of freedom\nAIC: 1167.4\n\nNumber of Fisher Scoring iterations: 7\n```\n\n\n:::\n:::\n\n\nBecause, Rsquared and adjusted Rsquared is not appropriated for logistic regression model, to see how model fits and explains variance used alternative\n\n### 2.1 Evaluating model fit\n\nEvaluating model fit\n\nComment Oh crap _ McFadden = .18-- not a bad fit for logistic regression\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Because, Rsquared and adjusted Rsquared is not appropriated for logistic regression model, to see how model fits and explains variance used alternative#\n#looking at model fit#\npR2(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfitting null model for pseudo-r2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          llh       llhNull            G2      McFadden          r2ML \n-572.70320797 -701.73792863  258.06944131    0.18387879    0.07399442 \n         r2CU \n   0.21655630 \n```\n\n\n:::\n:::\n\n\n### 2.2 Apply Confusion Matrix\n\nDo a confusion matrix for the model by installing Parallelly, and cli, and using caret from the library\n\ncomment on confusion matrix =- poor results\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Predict probabilities from the logistic regression model\npredicted_prob <- predict(model, type = \"response\")\n\n# Convert probabilities to binary classes using a 0.5 cutoff\npredicted_class <- ifelse(predicted_prob > 0.5, 1, 0)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# library(caret)\npredicted_class <- factor(predicted_class, levels = c(0,1))\nForReal_Stroke <- factor(LR_stroke1$stroke, levels = c(0,1))\nconfusionMatrix(predicted_class, ForReal_Stroke)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 3177  178\n         1    0    2\n                                          \n               Accuracy : 0.947           \n                 95% CI : (0.9388, 0.9543)\n    No Information Rate : 0.9464          \n    P-Value [Acc > NIR] : 0.4587          \n                                          \n                  Kappa : 0.0208          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 1.00000         \n            Specificity : 0.01111         \n         Pos Pred Value : 0.94694         \n         Neg Pred Value : 1.00000         \n             Prevalence : 0.94638         \n         Detection Rate : 0.94638         \n   Detection Prevalence : 0.99940         \n      Balanced Accuracy : 0.50556         \n                                          \n       'Positive' Class : 0               \n                                          \n```\n\n\n:::\n:::\n\n\n### 2.3 Apply F1 Score and Precision Recall\n\nLook at dataset and logistic regression analysis close with F1 score and Precision Recall\n\ndo F1 Score and Precision Recall\n\nPrecision - out of all the true strokes the model predicted, how many really were strokes? Consider 1 = 100%\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nprecision <- sum((predicted_class == 1) & (ForReal_Stroke == 1)) / sum(predicted_class == 1)\n```\n:::\n\n\nRecall - out of all the actual strokes, how many did the model catch? = .01 or 1%\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nrecall <- sum((predicted_class == 1) & (ForReal_Stroke == 1)) / sum(ForReal_Stroke == 1)\n```\n:::\n\n\nf1_Score - How well does this model predict strokes? = .022 or 2.2% --very poorly \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nf1_score  <- 2 * precision * recall / (precision + recall)\n```\n:::\n\n\nprecision, recall, f1_score\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nprecision\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nrecall\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01111111\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nf1_score\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02197802\n```\n\n\n:::\n:::\n\n\n## 3. Testing logistic Regression Model Assumptions\n\nThere are several assumptions for Logistic Regression:\n1. The Dependent Variable is binary (i.e, 0 or 1)\n2. There is a linear relationship between th logit of the outcome and each predictor\n3. There are NO high leverage outliers in the predictors\n4. There is No high multicollinearity (ie strong correlations) between predictors\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nLR_stroke2 <- stroke1_clean\nmodel2 <- glm(stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, data=LR_stroke2, family = binomial)\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = stroke ~ gender + age + hypertension + heart_disease + \n    ever_married + work_type + Residence_type + avg_glucose_level + \n    bmi + smoking_status, family = binomial, data = LR_stroke2)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       -8.426854   0.873243  -9.650  < 2e-16 ***\ngender             0.080370   0.167274   0.480 0.630893    \nage                0.070967   0.006845  10.368  < 2e-16 ***\nhypertension       0.570797   0.182580   3.126 0.001770 ** \nheart_disease      0.417884   0.220311   1.897 0.057856 .  \never_married       0.174316   0.261832   0.666 0.505569    \nwork_type         -0.109615   0.126101  -0.869 0.384703    \nResidence_type     0.005932   0.162188   0.037 0.970822    \navg_glucose_level  0.004658   0.001375   3.388 0.000704 ***\nbmi                0.006275   0.012875   0.487 0.625954    \nsmoking_status     0.179921   0.106431   1.691 0.090932 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1403.5  on 3356  degrees of freedom\nResidual deviance: 1145.4  on 3346  degrees of freedom\nAIC: 1167.4\n\nNumber of Fisher Scoring iterations: 7\n```\n\n\n:::\n:::\n\n\n### 3.1 Testing Assumption 1\n\nTesting Assumption 1: The Dependent Variable is binary (0 or 1)\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nunique(LR_stroke2$stroke)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 0\n```\n\n\n:::\n:::\n\n\n### 3.2 Testing Assumption 2\n\nTesting Assumption 2: There is a linear relationship between the outcome variable and each predictor (use boxTidwell)\n\nFor boxTidwell, first adjust all predictors so all values are positive. If we obtain a P value greater than 0.05 it indicates a linear relationship between the predictor and the outcome.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nLR_stroke2$genderadj            <- LR_stroke2$gender            + abs(min(LR_stroke1$gender))            + 1\nLR_stroke2$ageadj               <- LR_stroke2$age               + abs(min(LR_stroke1$age))               + 1\nLR_stroke2$hypertensionadj      <- LR_stroke2$hypertension      + abs(min(LR_stroke1$hypertension))      + 1\nLR_stroke2$heart_diseaseadj     <- LR_stroke2$heart_disease     + abs(min(LR_stroke1$hypertension))      + 1\nLR_stroke2$ever_marriedadj      <- LR_stroke2$ever_married      + abs(min(LR_stroke1$ever_married))      + 1\nLR_stroke2$work_typeadj         <- LR_stroke2$work_type         + abs(min(LR_stroke1$work_type))         + 1\nLR_stroke2$Residence_typeadj    <- LR_stroke2$Residence_type    + abs(min(LR_stroke1$Residence_type))    + 1\nLR_stroke2$avg_glucose_leveladj <- LR_stroke2$avg_glucose_level + abs(min(LR_stroke1$avg_glucose_level)) + 1\nLR_stroke2$bmiadj               <- LR_stroke2$bmi               + abs(min(LR_stroke1$bmi))               + 1\nLR_stroke2$smoking_statusadj    <- LR_stroke2$smoking_status    + abs(min(LR_stroke1$smoking_status))    + 1\n```\n:::\n\n\nError in linearHypothesis.lm(mod.2, H) : there are aliased coefficients in the model.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# boxTidwell(stroke ~ genderadj + ageadj + hypertensionadj + heart_diseaseadj + ever_marriedadj + work_typeadj + Residence_typeadj + avg_glucose_leveladj + bmiadj + smoking_statusadj, data=LR_stroke2)\n```\n:::\n\n\n#### 3.2.1 Issues Testing Assumption 2\n\nTrying to Drop Aliased Predictors\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# First, create the linear model object\nlm_model <- lm(stroke ~ genderadj + ageadj + hypertensionadj + heart_diseaseadj + ever_marriedadj + work_typeadj + Residence_typeadj + avg_glucose_leveladj + bmiadj + smoking_statusadj, data=LR_stroke2)\n\n# Then, run the alias() function\nalias(lm_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel :\nstroke ~ genderadj + ageadj + hypertensionadj + heart_diseaseadj + \n    ever_marriedadj + work_typeadj + Residence_typeadj + avg_glucose_leveladj + \n    bmiadj + smoking_statusadj\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# Model : stroke ~ genderadj + ageadj + hypertensionadj + heart_diseaseadj + ever_marriedadj + work_typeadj + Residence_typeadj + avg_glucose_leveladj + bmiadj + smoking_statusadj\n```\n:::\n\n\nYou can also check for perfect correlation and see if any correlations are ±1.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncor(LR_stroke2[, c(\"genderadj\",\"ageadj\",\"hypertensionadj\",\"heart_diseaseadj\",\n                   \"ever_marriedadj\",\"work_typeadj\",\"Residence_typeadj\",\n                   \"avg_glucose_leveladj\",\"bmiadj\",\"smoking_statusadj\")])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                       genderadj      ageadj hypertensionadj heart_diseaseadj\ngenderadj             1.00000000 -0.05599748    -0.040011423     -0.104191111\nageadj               -0.05599748  1.00000000     0.263123514      0.260353361\nhypertensionadj      -0.04001142  0.26312351     1.000000000      0.110020853\nheart_diseaseadj     -0.10419111  0.26035336     0.110020853      1.000000000\never_marriedadj       0.02728003 -0.48775310    -0.107114849     -0.069804764\nwork_typeadj          0.01495643  0.14214267     0.048358096      0.029618537\nResidence_typeadj    -0.01143732 -0.01761422     0.003112702     -0.010250014\navg_glucose_leveladj -0.07148597  0.23864619     0.168909926      0.143333385\nbmiadj               -0.01991382  0.04222590     0.127363138     -0.003962827\nsmoking_statusadj    -0.07723370  0.03463196    -0.005416806      0.060198195\n                     ever_marriedadj work_typeadj Residence_typeadj\ngenderadj                 0.02728003  0.014956427      -0.011437323\nageadj                   -0.48775310  0.142142673      -0.017614219\nhypertensionadj          -0.10711485  0.048358096       0.003112702\nheart_diseaseadj         -0.06980476  0.029618537      -0.010250014\never_marriedadj           1.00000000 -0.020663455       0.011239966\nwork_typeadj             -0.02066345  1.000000000      -0.007197831\nResidence_typeadj         0.01123997 -0.007197831       1.000000000\navg_glucose_leveladj     -0.11858810  0.034327248       0.008010375\nbmiadj                   -0.12527547 -0.017017707       0.009836168\nsmoking_statusadj        -0.05723324 -0.015271022      -0.039896247\n                     avg_glucose_leveladj       bmiadj smoking_statusadj\ngenderadj                    -0.071485971 -0.019913816      -0.077233702\nageadj                        0.238646187  0.042225902       0.034631959\nhypertensionadj               0.168909926  0.127363138      -0.005416806\nheart_diseaseadj              0.143333385 -0.003962827       0.060198195\never_marriedadj              -0.118588103 -0.125275472      -0.057233241\nwork_typeadj                  0.034327248 -0.017017707      -0.015271022\nResidence_typeadj             0.008010375  0.009836168      -0.039896247\navg_glucose_leveladj          1.000000000  0.155139559       0.005095349\nbmiadj                        0.155139559  1.000000000       0.029041449\nsmoking_statusadj             0.005095349  0.029041449       1.000000000\n```\n\n\n:::\n:::\n\n\ncheck constant columns:\n\nIf any variable only has one unique value → it’s constant → alias.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsapply(LR_stroke2, function(x) length(unique(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              gender                  age         hypertension \n                   2                   70                    2 \n       heart_disease         ever_married            work_type \n                   2                    2                    4 \n      Residence_type    avg_glucose_level                  bmi \n                   2                 2861                  364 \n      smoking_status               stroke            genderadj \n                   3                    2                    2 \n              ageadj      hypertensionadj     heart_diseaseadj \n                  70                    2                    2 \n     ever_marriedadj         work_typeadj    Residence_typeadj \n                   2                    4                    2 \navg_glucose_leveladj               bmiadj    smoking_statusadj \n                2861                  364                    3 \n```\n\n\n:::\n:::\n\n\nError in linearHypothesis.lm(mod.2, H) : there are aliased coefficients in the model.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# boxTidwell(stroke ~ genderadj + ageadj + hypertensionadj + heart_diseaseadj + ever_marriedadj + work_typeadj + Residence_typeadj + avg_glucose_leveladj + bmiadj + smoking_statusadj, data=LR_stroke2)\n```\n:::\n\n\n\n\n\n### 3.3 Testing Assumption 3\n\nTesting Assumption 3: assess influential outliers using car package and influencePlot\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncar::influencePlot(model2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        StudRes          Hat       CookD\n83    2.6917353 0.0039267816 0.012237368\n84    1.5018344 0.0343771041 0.006641860\n87    3.0732709 0.0012042796 0.011476828\n131   3.1608870 0.0006013465 0.007697762\n152   3.1135601 0.0005613972 0.006237531\n2583 -0.8509399 0.0399302730 0.001668526\n```\n\n\n:::\n:::\n\n\n### 3.4 Testing Assumption 4\n\nTesting Assumption 4 : Multicollinearity using ggplot and augment\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Testing Assumption 4 : Multicollinearity using ggplot and augment#\naug <- augment(model2)\nggplot(aug,aes(.fitted, .std.resid)) + geom_point() + geom_hline(yintercept=0)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n### 3.5 Conclusion of Testing Assumptions\n\nConclusion: Now that all 4 assumptions are met, logistic regression is a valid model to analyze the model\n\n## 4 Analysis of the Model\n\nPart 4: Analysis of the Model\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmodel2 <- glm(stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, data=LR_stroke2, family = binomial)\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = stroke ~ gender + age + hypertension + heart_disease + \n    ever_married + work_type + Residence_type + avg_glucose_level + \n    bmi + smoking_status, family = binomial, data = LR_stroke2)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       -8.426854   0.873243  -9.650  < 2e-16 ***\ngender             0.080370   0.167274   0.480 0.630893    \nage                0.070967   0.006845  10.368  < 2e-16 ***\nhypertension       0.570797   0.182580   3.126 0.001770 ** \nheart_disease      0.417884   0.220311   1.897 0.057856 .  \never_married       0.174316   0.261832   0.666 0.505569    \nwork_type         -0.109615   0.126101  -0.869 0.384703    \nResidence_type     0.005932   0.162188   0.037 0.970822    \navg_glucose_level  0.004658   0.001375   3.388 0.000704 ***\nbmi                0.006275   0.012875   0.487 0.625954    \nsmoking_status     0.179921   0.106431   1.691 0.090932 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1403.5  on 3356  degrees of freedom\nResidual deviance: 1145.4  on 3346  degrees of freedom\nAIC: 1167.4\n\nNumber of Fisher Scoring iterations: 7\n```\n\n\n:::\n:::\n\n\nConclusion: age, hypertension, heartdisease, and avg_glucose_level are statistically significant predictors on whether one has a stroke or not.\n\nall the P values of these 4 predictors is .05 or less (note included heart_disease because it approaches statistical significance at .057)\n\nSince this a logistic regression, we cant use R squared and adjusted R squared to see how well the model predicted stroke. So we substitute McFadden's P value.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# install.packages(\"pscl\")\n# library(pscl)\npR2(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfitting null model for pseudo-r2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          llh       llhNull            G2      McFadden          r2ML \n-572.70320797 -701.73792863  258.06944131    0.18387879    0.07399442 \n         r2CU \n   0.21655630 \n```\n\n\n:::\n:::\n\n\n\nComment  McFadden = .18-- not a bad fit for logistic regression\n\nCreate a confusion matrix (Type1 vs Type2 error in statistics)\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# install.packages(\"parallelly\")\n# install.packages(\"cli\")\n# library(caret)\n\n# Predict probabilities from the logistic regression model\npredicted_prob1 <- predict(model2, type = \"response\")\n\n# Convert probabilities to binary classes using a 0.5 cutoff\npredicted_class1 <- ifelse(predicted_prob1 > 0.5, 1, 0)\n\npredicted_class1 <- factor(predicted_class1, levels = c(0,1))\nForReal_Stroke1 <- factor(LR_stroke2$stroke, levels = c(0,1))\nconfusionMatrix(predicted_class1, ForReal_Stroke1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 3177  178\n         1    0    2\n                                          \n               Accuracy : 0.947           \n                 95% CI : (0.9388, 0.9543)\n    No Information Rate : 0.9464          \n    P-Value [Acc > NIR] : 0.4587          \n                                          \n                  Kappa : 0.0208          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 1.00000         \n            Specificity : 0.01111         \n         Pos Pred Value : 0.94694         \n         Neg Pred Value : 1.00000         \n             Prevalence : 0.94638         \n         Detection Rate : 0.94638         \n   Detection Prevalence : 0.99940         \n      Balanced Accuracy : 0.50556         \n                                          \n       'Positive' Class : 0               \n                                          \n```\n\n\n:::\n:::\n\n\nAnalysis of the Confusion Matrix: Crud...poor results\n\n\nFurther Analysis of the Confusion Matrix with F1 score and Precision Recall\n\n### 4.1 F1 Score and Precision Recall\n\nPrecision - out of all the true strokes the model predicted, how many really were strokes? =  1 = 100%\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nprecision1 <- sum((predicted_class1 == 1) & (ForReal_Stroke1 == 1)) / sum(predicted_class1 == 1)\n```\n:::\n\n\nRecall - out of all the actual strokes, how many did the model catch?\n\nResults = .01 or 1%---\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nrecall1 <- sum((predicted_class1 == 1) & (ForReal_Stroke1 == 1)) / sum(ForReal_Stroke1 == 1)\n```\n:::\n\n\nf1_Score - How well does this model predict strokes?\n\nResults = .022 or 2.2% --very poorly\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nf1_score1  <- 2 * precision1 * recall1 / (precision1 + recall1)\n```\n:::\n\n\nprecision1, recall1, f1_score1\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nprecision1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nrecall1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01111111\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nf1_score1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02197802\n```\n\n\n:::\n:::\n\n\n## Conclusion\n\n- Ideas for improving precision, recall and f1_score\n- Address imbalance by upsample (add stroke cases), downsample (remove non strokecases) and or SMOTE (Synthetic data)\n- Change the classification threshold\n- Compare with Alternative Models such as random forrests or XGBoost\n\n\n\n\n\n\n### References\n\n::: {#refs}\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}